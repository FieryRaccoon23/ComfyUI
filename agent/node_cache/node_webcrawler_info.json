{
  "basic-scheduler": "The BasicScheduler node is designed to compute a sequence of sigma values for diffusion models based on the provided scheduler, model, and denoising parameters. It dynamically adjusts the total number of steps based on the denoise factor to fine-tune the diffusion process.",
  "canny": "The Canny node is designed for edge detection in images, utilizing the Canny algorithm to identify and highlight the edges. This process involves applying a series of filters to the input image to detect areas of high gradient, which correspond to edges, thereby enhancing the image’s structural details.",
  "checkpoint-loader-simple": "The CheckpointLoaderSimple node is designed for loading model checkpoints without the need for specifying a configuration. It simplifies the process of checkpoint loading by requiring only the checkpoint name, making it more accessible for users who may not be familiar with the configuration details.",
  "checkpoint-save": "The CheckpointSave node is designed for saving the state of various model components, including models, CLIP, and VAE, into a checkpoint file. This functionality is crucial for preserving the training progress or configuration of models for later use or sharing.",
  "clip-loader": "The CLIPLoader node is designed for loading CLIP models, supporting different types such as stable diffusion and stable cascade. It abstracts the complexities of loading and configuring CLIP models for use in various applications, providing a streamlined way to access these models with specific configurations.",
  "clip-merge-simple": "This node specializes in merging two CLIP models based on a specified ratio, effectively blending their characteristics. It selectively applies patches from one model to another, excluding specific components like position IDs and logit scale, to create a hybrid model that combines features from both source models.",
  "clip-save": "The CLIPSave node is designed for saving CLIP models along with additional information such as prompts and extra PNG metadata. It encapsulates the functionality to serialize and store the model’s state, facilitating the preservation and sharing of model configurations and their associated creative prompts.",
  "clip-set-last-layer": "This node is designed to modify the behavior of a CLIP model by setting a specific layer as the last one to be executed. It allows for the customization of the depth of processing within the CLIP model, potentially affecting the model’s output by limiting the amount of information processed.",
  "clip-text-encode": "The CLIPTextEncode node is designed to encode textual inputs using a CLIP model, transforming text into a form that can be utilized for conditioning in generative tasks. It abstracts the complexity of text tokenization and encoding, providing a streamlined interface for generating text-based conditioning vectors.",
  "clip-text-encode-flux": "This node, named CLIPTextEncodeFlux, primarily functions to encode text and generate data for conditional control.",
  "clip-text-encode-sdxl": "This node is designed to encode text inputs using the CLIP model specifically tailored for the SDXL architecture. It focuses on converting textual descriptions into a format that can be effectively utilized for generating or manipulating images, leveraging the capabilities of the CLIP model to understand and process text in the context of visual content.",
  "clip-text-encode-sdxl-refiner": "This node specializes in refining the encoding of text inputs using CLIP models, enhancing the conditioning for generative tasks by incorporating aesthetic scores and dimensions.",
  "clip-vision-encode": "The CLIPVisionEncode node is designed to encode images using a CLIP vision model, transforming visual input into a format suitable for further processing or analysis. This node abstracts the complexity of image encoding, offering a streamlined interface for converting images into encoded representations.",
  "clip-vision-loader": "The CLIPVisionLoader node is designed for loading CLIP Vision models from specified paths. It abstracts the complexities of locating and initializing CLIP Vision models, making them readily available for further processing or inference tasks.",
  "clip_text_encode_hunyuan_dit": "This node can be viewed as a \"language translator\" that converts user input text (whether English or other languages) into \"machine language\" that AI models can understand, enabling the model to generate corresponding content based on these conditions.",
  "conditioning-average": "The ConditioningAverage node is designed to blend two sets of conditioning data, applying a weighted average based on a specified strength. This process allows for the dynamic adjustment of conditioning influence, facilitating the fine-tuning of generated content or features.",
  "conditioning-combine": "This node combines two conditioning inputs into a single output, effectively merging their information.",
  "conditioning-concat": "The ConditioningConcat node is designed to concatenate conditioning vectors, specifically merging the ‘conditioning_from’ vector into the ‘conditioning_to’ vector. This operation is fundamental in scenarios where the conditioning information from two sources needs to be combined into a single, unified representation.",
  "conditioning-set-area": "This node is designed to modify the conditioning information by setting specific areas within the conditioning context. It allows for the precise spatial manipulation of conditioning elements, enabling targeted adjustments and enhancements based on specified dimensions and strength.",
  "conditioning-set-area-percentage": "The ConditioningSetAreaPercentage node specializes in adjusting the area of influence for conditioning elements based on percentage values. It allows for the specification of the area’s dimensions and position as percentages of the total image size, alongside a strength parameter to modulate the intensity of the conditioning effect.",
  "conditioning-set-area-strength": "This node is designed to modify the strength attribute of a given conditioning set, allowing for the adjustment of the influence or intensity of the conditioning on the generation process.",
  "conditioning-set-mask": "This node is designed to modify the conditioning of a generative model by applying a mask with a specified strength to certain areas. It allows for targeted adjustments within the conditioning, enabling more precise control over the generation process.",
  "conditioning-settimestep-range": "This node is designed to adjust the temporal aspect of conditioning by setting a specific range of timesteps. It allows for the precise control over the start and end points of the conditioning process, enabling more targeted and efficient generation.",
  "conditioning-zero-out": "This node zeroes out specific elements within the conditioning data structure, effectively neutralizing their influence in subsequent processing steps. It’s designed for advanced conditioning operations where direct manipulation of the conditioning’s internal representation is required.",
  "controlnet-apply": "This node applies a ControlNet to a given image and conditioning, adjusting the image’s attributes based on the control network’s parameters and specified strength, such as Depth, OpenPose, Canny, HED, etc.",
  "controlnet-apply-advanced": "This node applies advanced control net transformations to conditioning data based on an image and a control net model. It allows for fine-tuned adjustments of the control net’s influence over the generated content, enabling more precise and varied modifications to the conditioning.",
  "controlnet-loader": "The ControlNetLoader node is designed to load a ControlNet model from a specified path. It plays a crucial role in initializing ControlNet models, which are essential for applying control mechanisms over generated content or modifying existing content based on control signals.",
  "crop-mask": "The CropMask node is designed for cropping a specified area from a given mask. It allows users to define the region of interest by specifying coordinates and dimensions, effectively extracting a portion of the mask for further processing or analysis.",
  "deprecated-checkpoint-loader": "The CheckpointLoader node is designed for advanced loading operations, specifically to load model checkpoints along with their configurations. It facilitates the retrieval of model components necessary for initializing and running generative models, including configurations and checkpoints from specified directories.",
  "deprecated-diffusers-loader": "The DiffusersLoader node is designed for loading models from the diffusers library, specifically handling the loading of UNet, CLIP, and VAE models based on provided model paths. It facilitates the integration of these models into the ComfyUI framework, enabling advanced functionalities such as text-to-image generation, image manipulation, and more.",
  "diff-controlnet-loader": "The DiffControlNetLoader node is designed for loading differential control networks, which are specialized models that can modify the behavior of another model based on control net specifications. This node allows for the dynamic adjustment of model behaviors by applying differential control nets, facilitating the creation of customized model outputs.",
  "dual-clip-loader": "The DualCLIPLoader node is designed for loading two CLIP models simultaneously, facilitating operations that require the integration or comparison of features from both models.",
  "empty-hunyuan-latent-video": "The `EmptyHunyuanLatentVideo` node is similar to the [EmptyLatent Image](https://comfyui-wiki.com/en/comfyui-nodes/latent/empty-latent-image) node. You can think of it as providing a blank canvas for video generation, where the width, height and length define the canvas properties, and batch size determines how many canvases to create. This node creates fresh empty canvases ready for subsequent video generation tasks.",
  "empty-image": "The EmptyImage node is designed to generate blank images of specified dimensions and color. It allows for the creation of uniform color images that can serve as backgrounds or placeholders in various image processing tasks.",
  "empty-latent-image": "The EmptyLatentImage node is designed to generate a blank latent space representation with specified dimensions and batch size. This node serves as a foundational step in generating or manipulating images in latent space, providing a starting point for further image synthesis or modification processes.",
  "exponential-scheduler": "The ExponentialScheduler node is designed to generate a sequence of sigma values following an exponential schedule for diffusion sampling processes. It provides a customizable approach to control the noise levels applied at each step of the diffusion process, allowing for fine-tuning of the sampling behavior.",
  "feather-mask": "The FeatherMask node applies a feathering effect to the edges of a given mask, smoothly transitioning the mask’s edges by adjusting their opacity based on specified distances from each edge. This creates a softer, more blended edge effect.",
  "flip-sigmas": "The FlipSigmas node is designed to manipulate the sequence of sigma values used in diffusion models by reversing their order and ensuring the first value is non-zero if originally zero. This operation is crucial for adapting the noise levels in reverse order, facilitating the generation process in models that operate by gradually reducing noise from data.",
  "flux-guidance": "For relatively short prompts and requirements, setting the guidance to 4 may be a good choice. However, if your prompt is longer or you want more creative content, setting the guidance between 1.0 and 1.5 might be a better option.",
  "gligen-loader": "The GLIGENLoader node is designed for loading GLIGEN models, which are specialized generative models. It facilitates the process of retrieving and initializing these models from specified paths, making them ready for further generative tasks.",
  "gligen-text-box-apply": "The GLIGENTextBoxApply node is designed to integrate text-based conditioning into a generative model’s input, specifically by applying text box parameters and encoding them using a CLIP model. This process enriches the conditioning with spatial and textual information, facilitating more precise and context-aware generation.",
  "grow-mask": "The GrowMask node is designed to modify the size of a given mask, either expanding or contracting it, while optionally applying a tapered effect to the corners. This functionality is crucial for dynamically adjusting mask boundaries in image processing tasks, allowing for more flexible and precise control over the area of interest.",
  "hypernetwork-loader": "The HypernetworkLoader node is designed to enhance or modify the capabilities of a given model by applying a hypernetwork. It loads a specified hypernetwork and applies it to the model, potentially altering its behavior or performance based on the strength parameter. This process allows for dynamic adjustments to the model’s architecture or parameters, enabling more flexible and adaptive AI systems.",
  "image-batch": "The ImageBatch node is designed for combining two images into a single batch. If the dimensions of the images do not match, it automatically rescales the second image to match the first one’s dimensions before combining them.",
  "image-blend": "The ImageBlend node is designed to blend two images together based on a specified blending mode and blend factor. It supports various blending modes such as normal, multiply, screen, overlay, soft light, and difference, allowing for versatile image manipulation and compositing techniques. This node is essential for creating composite images by adjusting the visual interaction between two image layers.",
  "image-blur": "The ImageBlur node applies a Gaussian blur to an image, allowing for the softening of edges and reduction of detail and noise. It provides control over the intensity and spread of the blur through parameters.",
  "image-color-to-mask": "The ImageColorToMask node is designed to convert a specified color in an image to a mask. It processes an image and a target color, generating a mask where the specified color is highlighted, facilitating operations like color-based segmentation or object isolation.",
  "image-composite-masked": "The ImageCompositeMasked node is designed for compositing images, allowing for the overlay of a source image onto a destination image at specified coordinates, with optional resizing and masking.",
  "image-crop": "The ImageCrop node is designed for cropping images to a specified width and height starting from a given x and y coordinate. This functionality is essential for focusing on specific regions of an image or for adjusting the image size to meet certain requirements.",
  "image-from-batch": "The ImageFromBatch node is designed for extracting a specific segment of images from a batch based on the provided index and length. It allows for more granular control over the batched images, enabling operations on individual or subsets of images within a larger batch.",
  "image-invert": "The ImageInvert node is designed to invert the colors of an image, effectively transforming each pixel’s color value to its complementary color on the color wheel. This operation is useful for creating negative images or for visual effects that require color inversion.",
  "image-only-checkpoint-loader": "This node will detect models located in the `ComfyUI/models/checkpoints` folder, and it will also read models from additional paths configured in the [extra_model_paths.yaml] file. Sometimes, you may need to **refresh the ComfyUI interface** to allow it to read the model files from the corresponding folder.",
  "image-pad-for-outpaint": "This node is designed for preparing images for the outpainting process by adding padding around them. It adjusts the image dimensions to ensure compatibility with outpainting algorithms, facilitating the generation of extended image areas beyond the original boundaries.",
  "image-quantize": "The ImageQuantize node is designed to reduce the number of colors in an image to a specified number, optionally applying dithering techniques to maintain visual quality. This process is useful for creating palette-based images or reducing the color complexity for certain applications.",
  "image-scale": "The ImageScale node is designed for resizing images to specific dimensions, offering a selection of upscale methods and the ability to crop the resized image. It abstracts the complexity of image upscaling and cropping, providing a straightforward interface for modifying image dimensions according to user-defined parameters.",
  "image-scale-by": "The ImageScaleBy node is designed for upscaling images by a specified scale factor using various interpolation methods. It allows for the adjustment of the image size in a flexible manner, catering to different upscaling needs.",
  "image-scale-to-total-pixels": "The ImageScaleToTotalPixels node is designed for resizing images to a specified total number of pixels while maintaining the aspect ratio. It provides various methods for upscaling the image to achieve the desired pixel count.",
  "image-sharpen": "The ImageSharpen node enhances the clarity of an image by accentuating its edges and details. It applies a sharpening filter to the image, which can be adjusted in intensity and radius, thereby making the image appear more defined and crisp.",
  "image-to-mask": "The ImageToMask node is designed to convert an image into a mask based on a specified color channel. It allows for the extraction of mask layers corresponding to the red, green, blue, or alpha channels of an image, facilitating operations that require channel-specific masking or processing.",
  "image-upscale-with-model": "This node is designed for upscaling images using a specified upscale model. It efficiently manages the upscaling process by adjusting the image to the appropriate device, optimizing memory usage, and applying the upscale model in a tiled manner to prevent potential out-of-memory errors.",
  "inpaint-model-conditioning": "The InpaintModelConditioning node is designed to facilitate the conditioning process for inpainting models, enabling the integration and manipulation of various conditioning inputs to tailor the inpainting output. It encompasses a broad range of functionalities, from loading specific model checkpoints and applying style or control net models, to encoding and combining conditioning elements, thereby serving as a comprehensive tool for customizing inpainting tasks.",
  "invert-mask": "The InvertMask node is designed to invert the values of a given mask, effectively flipping the masked and unmasked areas. This operation is fundamental in image processing tasks where the focus of interest needs to be switched between the foreground and the background.",
  "join-image-with-alpha": "This node is designed for compositing operations, specifically to join an image with its corresponding alpha mask to produce a single output image. It effectively combines visual content with transparency information, enabling the creation of images where certain areas are transparent or semi-transparent.",
  "k-sampler": "The KSampler works like this: it modifies the provided original latent image information based on a specific model and both positive and negative conditions. First, it adds noise to the original image data according to the set **seed** and **denoise strength** , then inputs the preset **Model** combined with **positive** and **negative** guidance conditions to generate the image.",
  "k-sampler-advanced": "The KSamplerAdvanced node is designed to enhance the sampling process by providing advanced configurations and techniques. It aims to offer more sophisticated options for generating samples from a model, improving upon the basic KSampler functionalities.",
  "k-sampler-select": "The KSamplerSelect node is designed to select a specific sampler based on the provided sampler name. It abstracts the complexity of sampler selection, allowing users to easily switch between different sampling strategies for their tasks.",
  "karras-scheduler": "The KarrasScheduler node is designed to generate a sequence of noise levels (sigmas) based on the Karras et al. (2022) noise schedule. This scheduler is useful for controlling the diffusion process in generative models, allowing for fine-tuned adjustments to the noise levels applied at each step of the generation process.",
  "latent-add": "The LatentAdd node is designed for the addition of two latent representations. It facilitates the combination of features or characteristics encoded in these representations by performing element-wise addition.",
  "latent-batch": "The LatentBatch node is designed to merge two sets of latent samples into a single batch, potentially resizing one set to match the dimensions of the other before concatenation. This operation facilitates the combination of different latent representations for further processing or generation tasks.",
  "latent-batch-seed-behavior": "The LatentBatchSeedBehavior node is designed to modify the seed behavior of a batch of latent samples. It allows for either randomizing or fixing the seed across the batch, thereby influencing the generation process by either introducing variability or maintaining consistency in the generated outputs.",
  "latent-composite": "The LatentComposite node is designed to blend or merge two latent representations into a single output. This process is essential for creating composite images or features by combining the characteristics of the input latents in a controlled manner.",
  "latent-composite-masked": "The LatentCompositeMasked node is designed for blending two latent representations together at specified coordinates, optionally using a mask for more controlled compositing. This node enables the creation of complex latent images by overlaying parts of one image onto another, with the ability to resize the source image for a perfect fit.",
  "latent-crop": "The LatentCrop node is designed to perform cropping operations on latent representations of images. It allows for the specification of the crop dimensions and position, enabling targeted modifications of the latent space.",
  "latent-flip": "The LatentFlip node is designed to manipulate latent representations by flipping them either vertically or horizontally. This operation allows for the transformation of the latent space, potentially uncovering new variations or perspectives within the data.",
  "latent-from-batch": "This node is designed to extract a specific subset of latent samples from a given batch based on the specified batch index and length. It allows for selective processing of latent samples, facilitating operations on smaller segments of the batch for efficiency or targeted manipulation.",
  "latent-interpolate": "The LatentInterpolate node is designed to perform interpolation between two sets of latent samples based on a specified ratio, blending the characteristics of both sets to produce a new, intermediate set of latent samples.",
  "latent-multiply": "The LatentMultiply node is designed to scale the latent representation of samples by a specified multiplier. This operation allows for the adjustment of the intensity or magnitude of features within the latent space, enabling fine-tuning of generated content or the exploration of variations within a given latent direction.",
  "latent-rotate": "The LatentRotate node is designed to rotate latent representations of images by specified angles. It abstracts the complexity of manipulating latent space to achieve rotation effects, enabling users to easily transform images in a generative model’s latent space.",
  "latent-subtract": "The LatentSubtract node is designed for subtracting one latent representation from another. This operation can be used to manipulate or modify the characteristics of generative models’ outputs by effectively removing features or attributes represented in one latent space from another.",
  "latent-upscale": "The LatentUpscale node is designed for upscaling latent representations of images. It allows for the adjustment of the output image’s dimensions and the method of upscaling, providing flexibility in enhancing the resolution of latent images.",
  "latent-upscale-by": "The LatentUpscaleBy node is designed for upscaling latent representations of images. It allows for the adjustment of the scale factor and the method of upscaling, providing flexibility in enhancing the resolution of latent samples.",
  "load-image": "The LoadImage node is designed to load and preprocess images from a specified path. It handles image formats with multiple frames, applies necessary transformations such as rotation based on EXIF data, normalizes pixel values, and optionally generates a mask for images with an alpha channel. This node is essential for preparing images for further processing or analysis within a pipeline.",
  "load-image-mask": "The LoadImageMask node is designed to load images and their associated masks from a specified path, processing them to ensure compatibility with further image manipulation or analysis tasks. It focuses on handling various image formats and conditions, such as presence of an alpha channel for masks, and prepares the images and masks for downstream processing by converting them to a standardized format.",
  "lora-loader": "This node automatically detects models located in the LoRA folder (including subfolders) with the corresponding model path being `ComfyUI\\models\\loras`. The LoRA Loader node is primarily used to load LoRA models. You can think of LoRA models as filters that can give your images specific styles, content, and details: Apply specific artistic styles (like ink painting) Add characteristics of certain characters (like game characters) Add specific details to the image All of these can be achieved through LoRA.",
  "lora-loader-model-only": "This node specializes in loading a LoRA model without requiring a CLIP model, focusing on enhancing or modifying a given model based on LoRA parameters. It allows for the dynamic adjustment of the model’s strength through LoRA parameters, facilitating fine-tuned control over the model’s behavior.",
  "mask-composite": "This node specializes in combining two mask inputs through a variety of operations such as addition, subtraction, and logical operations, to produce a new, modified mask. It abstractly handles the manipulation of mask data to achieve complex masking effects, serving as a crucial component in mask-based image editing and processing workflows.",
  "mask-to-image": "The `MaskToImage` node is designed to convert a mask into an image format. This transformation allows for the visualization and further processing of masks as images, facilitating a bridge between mask-based operations and image-based applications.",
  "model-merge-add": "The ModelMergeAdd node is designed for merging two models by adding key patches from one model to another. This process involves cloning the first model and then applying patches from the second model, allowing for the combination of features or behaviors from both models.",
  "model-merge-blocks": "ModelMergeBlocks is designed for advanced model merging operations, allowing for the integration of two models with customizable blending ratios for different parts of the models. This node facilitates the creation of hybrid models by selectively merging components from two source models based on specified parameters.",
  "model-merge-simple": "The ModelMergeSimple node is designed for merging two models by blending their parameters based on a specified ratio. This node facilitates the creation of hybrid models that combine the strengths or characteristics of both input models.",
  "model-merge-subtract": "This node is designed for advanced model merging operations, specifically to subtract the parameters of one model from another based on a specified multiplier. It enables the customization of model behaviors by adjusting the influence of one model’s parameters over another, facilitating the creation of new, hybrid models.",
  "model-sampling-continuous-edm": "This node is designed to enhance a model’s sampling capabilities by integrating continuous EDM (Energy-based Diffusion Models) sampling techniques. It allows for the dynamic adjustment of the noise levels within the model’s sampling process, offering a more refined control over the generation quality and diversity.",
  "model-sampling-discrete": "This node is designed to modify the sampling behavior of a model by applying a discrete sampling strategy. It allows for the selection of different sampling methods, such as epsilon, v_prediction, lcm, or x0, and optionally adjusts the model’s noise reduction strategy based on the zero-shot noise ratio (zsnr) setting.",
  "note": "Used to add text annotations in the workflow",
  "polyexponential-scheduler": "The PolyexponentialScheduler node is designed to generate a sequence of noise levels (sigmas) based on a polyexponential noise schedule. This schedule is a polynomial function in the logarithm of sigma, allowing for a flexible and customizable progression of noise levels throughout the diffusion process.",
  "porter-duff-image-composite": "The PorterDuffImageComposite node is designed to perform image compositing using the Porter-Duff compositing operators. It allows for the combination of source and destination images according to various blending modes, enabling the creation of complex visual effects by manipulating image transparency and overlaying images in creative ways.",
  "preview-image": "The PreviewImage node is designed for creating temporary preview images. It automatically generates a unique temporary file name for each image, compresses the image to a specified level, and saves it to a temporary directory. This functionality is particularly useful for generating previews of images during processing without affecting the original files.",
  "primitive": "The primitive node can recognize the type of input connected to it and provide input data accordingly. When this node is connected to different input types, it will change to different input states. It can be used to use a unified parameter among multiple different nodes, such as using the same seed in multiple Ksampler.",
  "quadruple-clip-loader": "The Quadruple CLIP Loader, QuadrupleCLIPLoader, is one of the core nodes of ComfyUI, first added to support the HiDream I1 version model. If you find this node missing, try updating ComfyUI to the latest version to ensure node support.",
  "rebatch-images": "The RebatchImages node is designed to reorganize a batch of images into a new batch configuration, adjusting the batch size as specified. This process is essential for managing and optimizing the processing of image data in batch operations, ensuring that images are grouped according to the desired batch size for efficient handling.",
  "rebatch-latents": "The RebatchLatents node is designed to reorganize a batch of latent representations into a new batch configuration, based on a specified batch size. It ensures that the latent samples are grouped appropriately, handling variations in dimensions and sizes, to facilitate further processing or model inference.",
  "repeat-image-batch": "The RepeatImageBatch node is designed to replicate a given image a specified number of times, creating a batch of identical images. This functionality is useful for operations that require multiple instances of the same image, such as batch processing or data augmentation.",
  "repeat-latent-batch": "The RepeatLatentBatch node is designed to replicate a given batch of latent representations a specified number of times, potentially including additional data like noise masks and batch indices. This functionality is crucial for operations that require multiple instances of the same latent data, such as data augmentation or specific generative tasks.",
  "reroute": "Mainly used to organize the logic of overly long connection lines in the ComfyUI workflow.",
  "rescale-cfg": "The RescaleCFG node is designed to adjust the conditioning and unconditioning scales of a model’s output based on a specified multiplier, aiming to achieve a more balanced and controlled generation process. It operates by rescaling the model’s output to modify the influence of conditioned and unconditioned components, thereby potentially enhancing the model’s performance or output quality.",
  "sampler-custom": "The SamplerCustom node is designed to provide a flexible and customizable sampling mechanism for various applications. It enables users to select and configure different sampling strategies tailored to their specific needs, enhancing the adaptability and efficiency of the sampling process.",
  "sampler-dpmpp-2m-sde": "This node is designed to generate a sampler for the DPMPP_2M_SDE model, allowing for the creation of samples based on specified solver types, noise levels, and computational device preferences. It abstracts the complexities of sampler configuration, providing a streamlined interface for generating samples with customized settings.",
  "sampler-dpmpp-sde": "This node is designed to generate a sampler for the DPM++ SDE (Stochastic Differential Equation) model. It adapts to both CPU and GPU execution environments, optimizing the sampler’s implementation based on the available hardware.",
  "save-animated-png": "The SaveAnimatedPNG node is designed for creating and saving animated PNG images from a sequence of frames. It handles the assembly of individual image frames into a cohesive animation, allowing for customization of frame duration, looping, and metadata inclusion.",
  "save-animated-webp": "This node is designed for saving a sequence of images as an animated WEBP file. It handles the aggregation of individual frames into a cohesive animation, applying specified metadata, and optimizing the output based on quality and compression settings.",
  "save-image": "The `Save Image` node is mainly used to save images to the **output** folder in ComfyUI. If you only want to preview the image during the intermediate process rather than saving it, you can use the `Preview Image` node. Default save location: `ComfyUI/output/`",
  "sd-4xupscale-conditioning": "This node specializes in enhancing the resolution of images through a 4x upscale process, incorporating conditioning elements to refine the output. It leverages diffusion techniques to upscale images while allowing for the adjustment of scale ratio and noise augmentation to fine-tune the enhancement process.",
  "sd-turbo-scheduler": "SDTurboScheduler is designed to generate a sequence of sigma values for image sampling, adjusting the sequence based on the denoise level and the number of steps specified. It leverages a specific model’s sampling capabilities to produce these sigma values, which are crucial for controlling the denoising process during image generation.",
  "set-latent-noise-mask": "This node is designed to apply a noise mask to a set of latent samples. It modifies the input samples by integrating a specified mask, thereby altering their noise characteristics.",
  "solid-mask": "The SolidMask node generates a uniform mask with a specified value across its entire area. It’s designed to create masks of specific dimensions and intensity, useful in various image processing and masking tasks.",
  "split-image-with-alpha": "The SplitImageWithAlpha node is designed to separate the color and alpha components of an image. It processes an input image tensor, extracting the RGB channels as the color component and the alpha channel as the transparency component, facilitating operations that require manipulation of these distinct image aspects.",
  "split-sigmas": "The SplitSigmas node is designed for dividing a sequence of sigma values into two parts based on a specified step. This functionality is crucial for operations that require different handling or processing of the initial and subsequent parts of the sigma sequence, enabling more flexible and targeted manipulation of these values.",
  "stablezero123-conditioning": "This node is designed to process and condition data for use in StableZero123 models, focusing on preparing the input in a specific format that is compatible and optimized for these models.",
  "stablezero123-conditioning-batched": "This node is designed to process conditioning information in a batched manner specifically tailored for the StableZero123 model. It focuses on efficiently handling multiple sets of conditioning data simultaneously, optimizing the workflow for scenarios where batch processing is crucial.",
  "style-model-apply": "This node applies a style model to a given conditioning, enhancing or altering its style based on the output of a CLIP vision model. It integrates the style model’s conditioning into the existing conditioning, allowing for a seamless blend of styles in the generation process.",
  "style-model-loader": "The StyleModelLoader node is designed to load a style model from a specified path. It focuses on retrieving and initializing style models that can be used to apply specific artistic styles to images, thereby enabling the customization of visual outputs based on the loaded style model.",
  "svd-img2vid-conditioning": "This node is designed for generating conditioning data for video generation tasks, specifically tailored for use with SVD_img2vid models. It takes various inputs including initial images, video parameters, and a VAE model to produce conditioning data that can be used to guide the generation of video frames.",
  "terminal-log": "Terminal Log (Manager) node is primarily used to display the running information of ComfyUI in the terminal within the ComfyUI interface. To use it, you need to set the `mode` to **logging** mode. This will allow it to record corresponding log information during the image generation task. If the `mode` is set to **stop** mode, it will not record log information. When you access and use ComfyUI via remote connections or local area network connections, Terminal Log (Manager) node becomes particularly useful. It allows you to directly view error messages from the CMD within the ComfyUI interface, making it easier to understand the current status of ComfyUI’s operation.",
  "unclip-checkpoint-loader": "The unCLIPCheckpointLoader node is designed for loading checkpoints specifically tailored for unCLIP models. It facilitates the retrieval and initialization of models, CLIP vision modules, and VAEs from a specified checkpoint, streamlining the setup process for further operations or analyses.",
  "unclip-conditioning": "This node is designed to integrate CLIP vision outputs into the conditioning process, adjusting the influence of these outputs based on specified strength and noise augmentation parameters. It enriches the conditioning with visual context, enhancing the generation process.",
  "unet-loader": "The UNETLoader node is designed for loading U-Net models by name, facilitating the use of pre-trained U-Net architectures within the system.",
  "upscale-model-loader": "The UpscaleModelLoader node is designed for loading upscale models from a specified directory. It facilitates the retrieval and preparation of upscale models for image upscaling tasks, ensuring that the models are correctly loaded and configured for evaluation.",
  "vae-decode": "The VAEDecode node is designed for decoding latent representations into images using a specified Variational Autoencoder (VAE). It serves the purpose of generating images from compressed data representations, facilitating the reconstruction of images from their latent space encodings.",
  "vae-encode": "This node is designed for encoding images into a latent space representation using a specified VAE model. It abstracts the complexity of the encoding process, providing a straightforward way to transform images into their latent representations.",
  "vae-encode-for-inpaint": "This node is designed for encoding images into a latent representation suitable for inpainting tasks, incorporating additional preprocessing steps to adjust the input image and mask for optimal encoding by the VAE model.",
  "vae-loader": "The VAELoader node is designed for loading Variational Autoencoder (VAE) models, specifically tailored to handle both standard and approximate VAEs. It supports loading VAEs by name, including specialized handling for ‘taesd’ and ‘taesdxl’ models, and dynamically adjusts based on the VAE’s specific configuration.",
  "vae-save": "The VAESave node is designed for saving VAE models along with their metadata, including prompts and additional PNG information, to a specified output directory. It encapsulates the functionality to serialize the model state and associated information into a file, facilitating the preservation and sharing of trained models.",
  "video-linear-cfg-guidance": "The VideoLinearCFGGuidance node applies a linear conditioning guidance scale to a video model, adjusting the influence of conditioned and unconditioned components over a specified range. This enables dynamic control over the generation process, allowing for fine-tuning of the model’s output based on the desired level of conditioning.",
  "vp-scheduler": "The VPScheduler node is designed to generate a sequence of noise levels (sigmas) based on the Variance Preserving (VP) scheduling method. This sequence is crucial for guiding the denoising process in diffusion models, allowing for controlled generation of images or other data types.",
  "wan-fun-control-to-video": "Prepare the conditioning information needed for video generation, using the Wan 2.1 Fun Control model."
}