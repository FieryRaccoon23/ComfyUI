{
  "APG": "APG",
  "AddNoise": "AddNoise",
  "AddTextPrefix": "Processed texts",
  "AddTextSuffix": "Processed texts",
  "AdjustBrightness": "Processed images",
  "AdjustContrast": "Processed images",
  "AlignYourStepsScheduler": "AlignYourStepsScheduler",
  "AudioAdjustVolume": "AudioAdjustVolume",
  "AudioConcat": "Concatenates the audio1 to audio2 in the specified direction.",
  "AudioEncoderEncode": "AudioEncoderEncode",
  "AudioEncoderLoader": "AudioEncoderLoader",
  "AudioMerge": "Combine two audio tracks by overlaying their waveforms.",
  "BasicGuider": "BasicGuider",
  "BasicScheduler": "The BasicScheduler node is designed to compute a sequence of sigma values for diffusion models based on the provided scheduler, model, and denoising parameters. It dynamically adjusts the total number of steps based on the denoise factor to fine-tune the diffusion process.",
  "BetaSamplingScheduler": "BetaSamplingScheduler",
  "ByteDanceFirstLastFrameNode": "Generate video using prompt and first and last frames.",
  "ByteDanceImageEditNode": "Edit images using ByteDance models via api based on prompt",
  "ByteDanceImageNode": "Generate images using ByteDance models via api based on prompt",
  "ByteDanceImageReferenceNode": "Generate video using prompt and reference images.",
  "ByteDanceImageToVideoNode": "Generate video using ByteDance models via api based on image and prompt",
  "ByteDanceSeedreamNode": "Unified text-to-image generation and precise single-sentence editing at up to 4K resolution.",
  "ByteDanceTextToVideoNode": "Generate video using ByteDance models via api based on prompt",
  "CFGGuider": "CFGGuider",
  "CFGNorm": "CFGNorm",
  "CFGZeroStar": "CFGZeroStar",
  "CLIPAttentionMultiply": "CLIPAttentionMultiply",
  "CLIPLoader": "[Recipes]\n\nstable_diffusion: clip-l\nstable_cascade: clip-g\nsd3: t5 xxl/ clip-g / clip-l\nstable_audio: t5 base\nmochi: t5 xxl\ncosmos: old t5 xxl\nlumina2: gemma 2 2B\nwan: umt5 xxl\n hidream: llama-3.1 (Recommend) or t5\nomnigen2: qwen vl 2.5 3B",
  "CLIPMergeAdd": "CLIPMergeAdd",
  "CLIPMergeSimple": "CLIPMergeSimple",
  "CLIPMergeSubtract": "CLIPMergeSubtract",
  "CLIPSave": "CLIPSave",
  "CLIPSetLastLayer": "CLIPSetLastLayer",
  "CLIPTextEncode": "Encodes a text prompt using a CLIP model into an embedding that can be used to guide the diffusion model towards generating specific images.\nA conditioning containing the embedded text used to guide the diffusion model.",
  "CLIPTextEncodeControlnet": "CLIPTextEncodeControlnet",
  "CLIPTextEncodeFlux": "CLIPTextEncodeFlux",
  "CLIPTextEncodeHiDream": "CLIPTextEncodeHiDream",
  "CLIPTextEncodeHunyuanDiT": "CLIPTextEncodeHunyuanDiT",
  "CLIPTextEncodeLumina2": "Encodes a system prompt and a user prompt using a CLIP model into an embedding that can be used to guide the diffusion model towards generating specific images.\nA conditioning containing the embedded text used to guide the diffusion model.",
  "CLIPTextEncodePixArtAlpha": "Encodes text and sets the resolution conditioning for PixArt Alpha. Does not apply to PixArt Sigma.",
  "CLIPTextEncodeSD3": "CLIPTextEncodeSD3",
  "CLIPTextEncodeSDXL": "CLIPTextEncodeSDXL",
  "CLIPTextEncodeSDXLRefiner": "CLIPTextEncodeSDXLRefiner",
  "CLIPVisionEncode": "CLIPVisionEncode",
  "CLIPVisionLoader": "CLIPVisionLoader",
  "Canny": "The Canny node is designed for edge detection in images, utilizing the Canny algorithm to identify and highlight the edges. This process involves applying a series of filters to the input image to detect areas of high gradient, which correspond to edges, thereby enhancing the image\u2019s structural details.",
  "CaseConverter": "CaseConverter",
  "CenterCropImages": "Processed images",
  "CheckpointLoader": "CheckpointLoader",
  "CheckpointLoaderSimple": "Loads a diffusion model checkpoint, diffusion models are used to denoise latents.\nThe model used for denoising latents.\nThe CLIP model used for encoding text prompts.\nThe VAE model used for encoding and decoding images to and from latent space.The CheckpointLoaderSimple node is designed for loading model checkpoints without the need for specifying a configuration. It simplifies the process of checkpoint loading by requiring only the checkpoint name, making it more accessible for users who may not be familiar with the configuration details.",
  "CheckpointSave": "The CheckpointSave node is designed for saving the state of various model components, including models, CLIP, and VAE, into a checkpoint file. This functionality is crucial for preserving the training progress or configuration of models for later use or sharing.",
  "ChromaRadianceOptions": "Allows setting advanced options for the Chroma Radiance model.",
  "ClipLoader": "The CLIPLoader node is designed for loading CLIP models, supporting different types such as stable diffusion and stable cascade. It abstracts the complexities of loading and configuring CLIP models for use in various applications, providing a streamlined way to access these models with specific configurations.",
  "ClipMergeSimple": "This node specializes in merging two CLIP models based on a specified ratio, effectively blending their characteristics. It selectively applies patches from one model to another, excluding specific components like position IDs and logit scale, to create a hybrid model that combines features from both source models.",
  "ClipSave": "The CLIPSave node is designed for saving CLIP models along with additional information such as prompts and extra PNG metadata. It encapsulates the functionality to serialize and store the model\u2019s state, facilitating the preservation and sharing of model configurations and their associated creative prompts.",
  "ClipSetLastLayer": "This node is designed to modify the behavior of a CLIP model by setting a specific layer as the last one to be executed. It allows for the customization of the depth of processing within the CLIP model, potentially affecting the model\u2019s output by limiting the amount of information processed.",
  "ClipTextEncode": "The CLIPTextEncode node is designed to encode textual inputs using a CLIP model, transforming text into a form that can be utilized for conditioning in generative tasks. It abstracts the complexity of text tokenization and encoding, providing a streamlined interface for generating text-based conditioning vectors.",
  "ClipTextEncodeFlux": "This node, named CLIPTextEncodeFlux, primarily functions to encode text and generate data for conditional control.",
  "ClipTextEncodeHunyuanDit": "This node can be viewed as a \"language translator\" that converts user input text (whether English or other languages) into \"machine language\" that AI models can understand, enabling the model to generate corresponding content based on these conditions.",
  "ClipTextEncodeSdxl": "This node is designed to encode text inputs using the CLIP model specifically tailored for the SDXL architecture. It focuses on converting textual descriptions into a format that can be effectively utilized for generating or manipulating images, leveraging the capabilities of the CLIP model to understand and process text in the context of visual content.",
  "ClipTextEncodeSdxlRefiner": "This node specializes in refining the encoding of text inputs using CLIP models, enhancing the conditioning for generative tasks by incorporating aesthetic scores and dimensions.",
  "ClipVisionEncode": "The CLIPVisionEncode node is designed to encode images using a CLIP vision model, transforming visual input into a format suitable for further processing or analysis. This node abstracts the complexity of image encoding, offering a streamlined interface for converting images into encoded representations.",
  "ClipVisionLoader": "The CLIPVisionLoader node is designed for loading CLIP Vision models from specified paths. It abstracts the complexities of locating and initializing CLIP Vision models, making them readily available for further processing or inference tasks.",
  "CombineHooks2": "CombineHooks2",
  "CombineHooks4": "CombineHooks4",
  "CombineHooks8": "CombineHooks8",
  "ConditioningAverage": "The ConditioningAverage node is designed to blend two sets of conditioning data, applying a weighted average based on a specified strength. This process allows for the dynamic adjustment of conditioning influence, facilitating the fine-tuning of generated content or features.",
  "ConditioningCombine": "This node combines two conditioning inputs into a single output, effectively merging their information.",
  "ConditioningConcat": "The ConditioningConcat node is designed to concatenate conditioning vectors, specifically merging the \u2018conditioning_from\u2019 vector into the \u2018conditioning_to\u2019 vector. This operation is fundamental in scenarios where the conditioning information from two sources needs to be combined into a single, unified representation.",
  "ConditioningSetArea": "This node is designed to modify the conditioning information by setting specific areas within the conditioning context. It allows for the precise spatial manipulation of conditioning elements, enabling targeted adjustments and enhancements based on specified dimensions and strength.",
  "ConditioningSetAreaPercentage": "The ConditioningSetAreaPercentage node specializes in adjusting the area of influence for conditioning elements based on percentage values. It allows for the specification of the area\u2019s dimensions and position as percentages of the total image size, alongside a strength parameter to modulate the intensity of the conditioning effect.",
  "ConditioningSetAreaPercentageVideo": "ConditioningSetAreaPercentageVideo",
  "ConditioningSetAreaStrength": "This node is designed to modify the strength attribute of a given conditioning set, allowing for the adjustment of the influence or intensity of the conditioning on the generation process.",
  "ConditioningSetDefaultCombine": "ConditioningSetDefaultCombine",
  "ConditioningSetMask": "This node is designed to modify the conditioning of a generative model by applying a mask with a specified strength to certain areas. It allows for targeted adjustments within the conditioning, enabling more precise control over the generation process.",
  "ConditioningSetProperties": "ConditioningSetProperties",
  "ConditioningSetPropertiesAndCombine": "ConditioningSetPropertiesAndCombine",
  "ConditioningSetTimestepRange": "ConditioningSetTimestepRange",
  "ConditioningSettimestepRange": "This node is designed to adjust the temporal aspect of conditioning by setting a specific range of timesteps. It allows for the precise control over the start and end points of the conditioning process, enabling more targeted and efficient generation.",
  "ConditioningStableAudio": "ConditioningStableAudio",
  "ConditioningTimestepsRange": "ConditioningTimestepsRange",
  "ConditioningZeroOut": "This node zeroes out specific elements within the conditioning data structure, effectively neutralizing their influence in subsequent processing steps. It\u2019s designed for advanced conditioning operations where direct manipulation of the conditioning\u2019s internal representation is required.",
  "ContextWindowsManual": "Manually set context windows.\nThe model with context windows applied during sampling.",
  "ControlNetApply": "ControlNetApply",
  "ControlNetApplyAdvanced": "ControlNetApplyAdvanced",
  "ControlNetApplySD3": "ControlNetApplySD3",
  "ControlNetInpaintingAliMamaApply": "ControlNetInpaintingAliMamaApply",
  "ControlNetLoader": "ControlNetLoader",
  "ControlnetApply": "This node applies a ControlNet to a given image and conditioning, adjusting the image\u2019s attributes based on the control network\u2019s parameters and specified strength, such as Depth, OpenPose, Canny, HED, etc.",
  "ControlnetApplyAdvanced": "This node applies advanced control net transformations to conditioning data based on an image and a control net model. It allows for fine-tuned adjustments of the control net\u2019s influence over the generated content, enabling more precise and varied modifications to the conditioning.",
  "ControlnetLoader": "The ControlNetLoader node is designed to load a ControlNet model from a specified path. It plays a crucial role in initializing ControlNet models, which are essential for applying control mechanisms over generated content or modifying existing content based on control signals.",
  "CosmosImageToVideoLatent": "CosmosImageToVideoLatent",
  "CosmosPredict2ImageToVideoLatent": "CosmosPredict2ImageToVideoLatent",
  "CreateHookKeyframe": "CreateHookKeyframe",
  "CreateHookKeyframesFromFloats": "CreateHookKeyframesFromFloats",
  "CreateHookKeyframesInterpolated": "CreateHookKeyframesInterpolated",
  "CreateHookLora": "CreateHookLora",
  "CreateHookLoraModelOnly": "CreateHookLoraModelOnly",
  "CreateHookModelAsLora": "CreateHookModelAsLora",
  "CreateHookModelAsLoraModelOnly": "CreateHookModelAsLoraModelOnly",
  "CreateVideo": "Create a video from images.",
  "CropMask": "The CropMask node is designed for cropping a specified area from a given mask. It allows users to define the region of interest by specifying coordinates and dimensions, effectively extracting a portion of the mask for further processing or analysis.",
  "DeprecatedCheckpointLoader": "The CheckpointLoader node is designed for advanced loading operations, specifically to load model checkpoints along with their configurations. It facilitates the retrieval of model components necessary for initializing and running generative models, including configurations and checkpoints from specified directories.",
  "DeprecatedDiffusersLoader": "The DiffusersLoader node is designed for loading models from the diffusers library, specifically handling the loading of UNet, CLIP, and VAE models based on provided model paths. It facilitates the integration of these models into the ComfyUI framework, enabling advanced functionalities such as text-to-image generation, image manipulation, and more.",
  "DiffControlNetLoader": "DiffControlNetLoader",
  "DiffControlnetLoader": "The DiffControlNetLoader node is designed for loading differential control networks, which are specialized models that can modify the behavior of another model based on control net specifications. This node allows for the dynamic adjustment of model behaviors by applying differential control nets, facilitating the creation of customized model outputs.",
  "DifferentialDiffusion": "DifferentialDiffusion",
  "DiffusersLoader": "DiffusersLoader",
  "DisableNoise": "DisableNoise",
  "DualCFGGuider": "DualCFGGuider",
  "DualCLIPLoader": "[Recipes]\n\nsdxl: clip-l, clip-g\nsd3: clip-l, clip-g / clip-l, t5 / clip-g, t5\nflux: clip-l, t5\nhidream: at least one of t5 or llama, recommended t5 and llama\nhunyuan_image: qwen2.5vl 7b and byt5 small",
  "DualClipLoader": "The DualCLIPLoader node is designed for loading two CLIP models simultaneously, facilitating operations that require the integration or comparison of features from both models.",
  "EasyCache": "Native EasyCache implementation.\nThe model with EasyCache.",
  "EmptyAceStepLatentAudio": "EmptyAceStepLatentAudio",
  "EmptyAudio": "EmptyAudio",
  "EmptyChromaRadianceLatentImage": "EmptyChromaRadianceLatentImage",
  "EmptyCosmosLatentVideo": "EmptyCosmosLatentVideo",
  "EmptyFlux2LatentImage": "EmptyFlux2LatentImage",
  "EmptyHunyuanImageLatent": "EmptyHunyuanImageLatent",
  "EmptyHunyuanLatentVideo": "The `EmptyHunyuanLatentVideo` node is similar to the [EmptyLatent Image](https://comfyui-wiki.com/en/comfyui-nodes/latent/empty-latent-image) node. You can think of it as providing a blank canvas for video generation, where the width, height and length define the canvas properties, and batch size determines how many canvases to create. This node creates fresh empty canvases ready for subsequent video generation tasks.",
  "EmptyHunyuanVideo15Latent": "EmptyHunyuanVideo15Latent",
  "EmptyImage": "The EmptyImage node is designed to generate blank images of specified dimensions and color. It allows for the creation of uniform color images that can serve as backgrounds or placeholders in various image processing tasks.",
  "EmptyLTXVLatentVideo": "EmptyLTXVLatentVideo",
  "EmptyLatentAudio": "EmptyLatentAudio",
  "EmptyLatentHunyuan3Dv2": "EmptyLatentHunyuan3Dv2",
  "EmptyLatentImage": "Create a new batch of empty latent images to be denoised via sampling.\nThe empty latent image batch.The EmptyLatentImage node is designed to generate a blank latent space representation with specified dimensions and batch size. This node serves as a foundational step in generating or manipulating images in latent space, providing a starting point for further image synthesis or modification processes.",
  "EmptyMochiLatentVideo": "EmptyMochiLatentVideo",
  "EmptySD3LatentImage": "EmptySD3LatentImage",
  "Epsilon Scaling": "Epsilon Scaling",
  "ExponentialScheduler": "The ExponentialScheduler node is designed to generate a sequence of sigma values following an exponential schedule for diffusion sampling processes. It provides a customizable approach to control the noise levels applied at each step of the diffusion process, allowing for fine-tuning of the sampling behavior.",
  "ExtendIntermediateSigmas": "ExtendIntermediateSigmas",
  "FeatherMask": "The FeatherMask node applies a feathering effect to the edges of a given mask, smoothly transitioning the mask\u2019s edges by adjusting their opacity based on specified distances from each edge. This creates a softer, more blended edge effect.",
  "FlipSigmas": "The FlipSigmas node is designed to manipulate the sequence of sigma values used in diffusion models by reversing their order and ensuring the first value is non-zero if originally zero. This operation is crucial for adapting the noise levels in reverse order, facilitating the generation process in models that operate by gradually reducing noise from data.",
  "Flux2ProImageNode": "Generates images synchronously based on prompt and resolution.",
  "Flux2Scheduler": "Flux2Scheduler",
  "FluxDisableGuidance": "This node completely disables the guidance embed on Flux and Flux like models",
  "FluxGuidance": "For relatively short prompts and requirements, setting the guidance to 4 may be a good choice. However, if your prompt is longer or you want more creative content, setting the guidance between 1.0 and 1.5 might be a better option.",
  "FluxKontextImageScale": "This node resizes the image to one that is more optimal for flux kontext.",
  "FluxKontextMaxImageNode": "Edits images using Flux.1 Kontext [max] via api based on prompt and aspect ratio.",
  "FluxKontextMultiReferenceLatentMethod": "FluxKontextMultiReferenceLatentMethod",
  "FluxKontextProImageNode": "Edits images using Flux.1 Kontext [pro] via api based on prompt and aspect ratio.",
  "FluxProExpandNode": "Outpaints image based on prompt.",
  "FluxProFillNode": "Inpaints image based on mask and prompt.",
  "FluxProUltraImageNode": "Generates images using Flux Pro 1.1 Ultra via api based on prompt and resolution.",
  "FreSca": "Applies frequency-dependent scaling to the guidance",
  "FreeU": "FreeU",
  "FreeU_V2": "FreeU_V2",
  "GITSScheduler": "GITSScheduler",
  "GLIGENLoader": "GLIGENLoader",
  "GLIGENTextBoxApply": "GLIGENTextBoxApply",
  "GeminiImage2Node": "Generate or edit images synchronously via Google Vertex API.",
  "GeminiImageNode": "Edit images synchronously via Google API.",
  "GeminiInputFiles": "Loads and prepares input files to include as inputs for Gemini LLM nodes. The files will be read by the Gemini model when generating a response. The contents of the text file count toward the token limit. \ud83d\udec8 TIP: Can be chained together with other Gemini Input File nodes.",
  "GeminiNode": "Generate text responses with Google's Gemini AI model. You can provide multiple types of inputs (text, images, audio, video) as context for generating more relevant and meaningful responses.",
  "GetImageSize": "Returns width and height of the image, and passes it through unchanged.",
  "GetVideoComponents": "Extracts all components from a video: frames, audio, and framerate.",
  "GligenLoader": "The GLIGENLoader node is designed for loading GLIGEN models, which are specialized generative models. It facilitates the process of retrieving and initializing these models from specified paths, making them ready for further generative tasks.",
  "GligenTextBoxApply": "The GLIGENTextBoxApply node is designed to integrate text-based conditioning into a generative model\u2019s input, specifically by applying text box parameters and encoding them using a CLIP model. This process enriches the conditioning with spatial and textual information, facilitating more precise and context-aware generation.",
  "GrowMask": "The GrowMask node is designed to modify the size of a given mask, either expanding or contracting it, while optionally applying a tapered effect to the corners. This functionality is crucial for dynamically adjusting mask boundaries in image processing tasks, allowing for more flexible and precise control over the area of interest.",
  "Hunyuan3Dv2Conditioning": "Hunyuan3Dv2Conditioning",
  "Hunyuan3Dv2ConditioningMultiView": "Hunyuan3Dv2ConditioningMultiView",
  "HunyuanImageToVideo": "HunyuanImageToVideo",
  "HunyuanRefinerLatent": "HunyuanRefinerLatent",
  "HunyuanVideo15ImageToVideo": "HunyuanVideo15ImageToVideo",
  "HunyuanVideo15LatentUpscaleWithModel": "HunyuanVideo15LatentUpscaleWithModel",
  "HunyuanVideo15SuperResolution": "HunyuanVideo15SuperResolution",
  "HyperTile": "HyperTile",
  "HypernetworkLoader": "The HypernetworkLoader node is designed to enhance or modify the capabilities of a given model by applying a hypernetwork. It loads a specified hypernetwork and applies it to the model, potentially altering its behavior or performance based on the strength parameter. This process allows for dynamic adjustments to the model\u2019s architecture or parameters, enabling more flexible and adaptive AI systems.",
  "IdeogramV1": "Generates images using the Ideogram V1 model.",
  "IdeogramV2": "Generates images using the Ideogram V2 model.",
  "IdeogramV3": "Generates images using the Ideogram V3 model. Supports both regular image generation from text prompts and image editing with mask.",
  "ImageAddNoise": "ImageAddNoise",
  "ImageBatch": "The ImageBatch node is designed for combining two images into a single batch. If the dimensions of the images do not match, it automatically rescales the second image to match the first one\u2019s dimensions before combining them.",
  "ImageBlend": "The ImageBlend node is designed to blend two images together based on a specified blending mode and blend factor. It supports various blending modes such as normal, multiply, screen, overlay, soft light, and difference, allowing for versatile image manipulation and compositing techniques. This node is essential for creating composite images by adjusting the visual interaction between two image layers.",
  "ImageBlur": "The ImageBlur node applies a Gaussian blur to an image, allowing for the softening of edges and reduction of detail and noise. It provides control over the intensity and spread of the blur through parameters.",
  "ImageColorToMask": "The ImageColorToMask node is designed to convert a specified color in an image to a mask. It processes an image and a target color, generating a mask where the specified color is highlighted, facilitating operations like color-based segmentation or object isolation.",
  "ImageCompositeMasked": "The ImageCompositeMasked node is designed for compositing images, allowing for the overlay of a source image onto a destination image at specified coordinates, with optional resizing and masking.",
  "ImageCrop": "The ImageCrop node is designed for cropping images to a specified width and height starting from a given x and y coordinate. This functionality is essential for focusing on specific regions of an image or for adjusting the image size to meet certain requirements.",
  "ImageDeduplication": "Processed images",
  "ImageFlip": "ImageFlip",
  "ImageFromBatch": "The ImageFromBatch node is designed for extracting a specific segment of images from a batch based on the provided index and length. It allows for more granular control over the batched images, enabling operations on individual or subsets of images within a larger batch.",
  "ImageGrid": "Processed images",
  "ImageInvert": "The ImageInvert node is designed to invert the colors of an image, effectively transforming each pixel\u2019s color value to its complementary color on the color wheel. This operation is useful for creating negative images or for visual effects that require color inversion.",
  "ImageOnlyCheckpointLoader": "This node will detect models located in the `ComfyUI/models/checkpoints` folder, and it will also read models from additional paths configured in the [extra_model_paths.yaml] file. Sometimes, you may need to **refresh the ComfyUI interface** to allow it to read the model files from the corresponding folder.",
  "ImageOnlyCheckpointSave": "ImageOnlyCheckpointSave",
  "ImagePadForOutpaint": "This node is designed for preparing images for the outpainting process by adding padding around them. It adjusts the image dimensions to ensure compatibility with outpainting algorithms, facilitating the generation of extended image areas beyond the original boundaries.",
  "ImageQuantize": "The ImageQuantize node is designed to reduce the number of colors in an image to a specified number, optionally applying dithering techniques to maintain visual quality. This process is useful for creating palette-based images or reducing the color complexity for certain applications.",
  "ImageRGBToYUV": "ImageRGBToYUV",
  "ImageRotate": "ImageRotate",
  "ImageScale": "The ImageScale node is designed for resizing images to specific dimensions, offering a selection of upscale methods and the ability to crop the resized image. It abstracts the complexity of image upscaling and cropping, providing a straightforward interface for modifying image dimensions according to user-defined parameters.",
  "ImageScaleBy": "The ImageScaleBy node is designed for upscaling images by a specified scale factor using various interpolation methods. It allows for the adjustment of the image size in a flexible manner, catering to different upscaling needs.",
  "ImageScaleToMaxDimension": "ImageScaleToMaxDimension",
  "ImageScaleToTotalPixels": "The ImageScaleToTotalPixels node is designed for resizing images to a specified total number of pixels while maintaining the aspect ratio. It provides various methods for upscaling the image to achieve the desired pixel count.",
  "ImageSharpen": "The ImageSharpen node enhances the clarity of an image by accentuating its edges and details. It applies a sharpening filter to the image, which can be adjusted in intensity and radius, thereby making the image appear more defined and crisp.",
  "ImageStitch": "Stitches image2 to image1 in the specified direction.\nIf image2 is not provided, returns image1 unchanged.\nOptional spacing can be added between images.",
  "ImageToMask": "The ImageToMask node is designed to convert an image into a mask based on a specified color channel. It allows for the extraction of mask layers corresponding to the red, green, blue, or alpha channels of an image, facilitating operations that require channel-specific masking or processing.",
  "ImageUpscaleWithModel": "This node is designed for upscaling images using a specified upscale model. It efficiently manages the upscaling process by adjusting the image to the appropriate device, optimizing memory usage, and applying the upscale model in a tiled manner to prevent potential out-of-memory errors.",
  "ImageYUVToRGB": "ImageYUVToRGB",
  "InpaintModelConditioning": "The InpaintModelConditioning node is designed to facilitate the conditioning process for inpainting models, enabling the integration and manipulation of various conditioning inputs to tailor the inpainting output. It encompasses a broad range of functionalities, from loading specific model checkpoints and applying style or control net models, to encoding and combining conditioning elements, thereby serving as a comprehensive tool for customizing inpainting tasks.",
  "InstructPixToPixConditioning": "InstructPixToPixConditioning",
  "InvertMask": "The InvertMask node is designed to invert the values of a given mask, effectively flipping the masked and unmasked areas. This operation is fundamental in image processing tasks where the focus of interest needs to be switched between the foreground and the background.",
  "JoinImageWithAlpha": "This node is designed for compositing operations, specifically to join an image with its corresponding alpha mask to produce a single output image. It effectively combines visual content with transparency information, enabling the creation of images where certain areas are transparent or semi-transparent.",
  "KSampler": "Uses the provided model, positive and negative conditioning to denoise the latent image.\nThe denoised latent.The KSampler works like this: it modifies the provided original latent image information based on a specific model and both positive and negative conditions. First, it adds noise to the original image data according to the set **seed** and **denoise strength** , then inputs the preset **Model** combined with **positive** and **negative** guidance conditions to generate the image.",
  "KSamplerAdvanced": "The KSamplerAdvanced node is designed to enhance the sampling process by providing advanced configurations and techniques. It aims to offer more sophisticated options for generating samples from a model, improving upon the basic KSampler functionalities.",
  "KSamplerSelect": "The KSamplerSelect node is designed to select a specific sampler based on the provided sampler name. It abstracts the complexity of sampler selection, allowing users to easily switch between different sampling strategies for their tasks.",
  "KarrasScheduler": "The KarrasScheduler node is designed to generate a sequence of noise levels (sigmas) based on the Karras et al. (2022) noise schedule. This scheduler is useful for controlling the diffusion process in generative models, allowing for fine-tuned adjustments to the noise levels applied at each step of the generation process.",
  "KlingCameraControlI2VNode": "Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image.",
  "KlingCameraControlT2VNode": "Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text.",
  "KlingCameraControls": "Allows specifying configuration options for Kling Camera Controls and motion control effects.",
  "KlingDualCharacterVideoEffectNode": "Achieve different special effects when generating a video based on the effect_scene. First image will be positioned on left side, second on right side of the composite.",
  "KlingImage2VideoNode": "Kling Image to Video Node",
  "KlingImageGenerationNode": "Kling Image Generation Node. Generate an image from a text prompt with an optional reference image.",
  "KlingLipSyncAudioToVideoNode": "Kling Lip Sync Audio to Video Node. Syncs mouth movements in a video file to the audio content of an audio file. When using, ensure that the audio contains clearly distinguishable vocals and that the video contains a distinct face. The audio file should not be larger than 5MB. The video file should not be larger than 100MB, should have height/width between 720px and 1920px, and should be between 2s and 10s in length.",
  "KlingLipSyncTextToVideoNode": "Kling Lip Sync Text to Video Node. Syncs mouth movements in a video file to a text prompt. The video file should not be larger than 100MB, should have height/width between 720px and 1920px, and should be between 2s and 10s in length.",
  "KlingSingleImageVideoEffectNode": "Achieve different special effects when generating a video based on the effect_scene.",
  "KlingStartEndFrameNode": "Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last.",
  "KlingTextToVideoNode": "Kling Text to Video Node",
  "KlingVideoExtendNode": "Kling Video Extend Node. Extend videos made by other Kling nodes. The video_id is created by using other Kling Nodes.",
  "KlingVirtualTryOnNode": "Kling Virtual Try On Node. Input a human image and a cloth image to try on the cloth on the human. You can merge multiple clothing item pictures into one image with a white background.",
  "LTXVAddGuide": "LTXVAddGuide",
  "LTXVConditioning": "LTXVConditioning",
  "LTXVCropGuides": "LTXVCropGuides",
  "LTXVImgToVideo": "LTXVImgToVideo",
  "LTXVPreprocess": "LTXVPreprocess",
  "LTXVScheduler": "LTXVScheduler",
  "LaplaceScheduler": "LaplaceScheduler",
  "LatentAdd": "The LatentAdd node is designed for the addition of two latent representations. It facilitates the combination of features or characteristics encoded in these representations by performing element-wise addition.",
  "LatentApplyOperation": "LatentApplyOperation",
  "LatentApplyOperationCFG": "LatentApplyOperationCFG",
  "LatentBatch": "The LatentBatch node is designed to merge two sets of latent samples into a single batch, potentially resizing one set to match the dimensions of the other before concatenation. This operation facilitates the combination of different latent representations for further processing or generation tasks.",
  "LatentBatchSeedBehavior": "The LatentBatchSeedBehavior node is designed to modify the seed behavior of a batch of latent samples. It allows for either randomizing or fixing the seed across the batch, thereby influencing the generation process by either introducing variability or maintaining consistency in the generated outputs.",
  "LatentBlend": "LatentBlend",
  "LatentComposite": "The LatentComposite node is designed to blend or merge two latent representations into a single output. This process is essential for creating composite images or features by combining the characteristics of the input latents in a controlled manner.",
  "LatentCompositeMasked": "The LatentCompositeMasked node is designed for blending two latent representations together at specified coordinates, optionally using a mask for more controlled compositing. This node enables the creation of complex latent images by overlaying parts of one image onto another, with the ability to resize the source image for a perfect fit.",
  "LatentConcat": "LatentConcat",
  "LatentCrop": "The LatentCrop node is designed to perform cropping operations on latent representations of images. It allows for the specification of the crop dimensions and position, enabling targeted modifications of the latent space.",
  "LatentCut": "LatentCut",
  "LatentFlip": "The LatentFlip node is designed to manipulate latent representations by flipping them either vertically or horizontally. This operation allows for the transformation of the latent space, potentially uncovering new variations or perspectives within the data.",
  "LatentFromBatch": "This node is designed to extract a specific subset of latent samples from a given batch based on the specified batch index and length. It allows for selective processing of latent samples, facilitating operations on smaller segments of the batch for efficiency or targeted manipulation.",
  "LatentInterpolate": "The LatentInterpolate node is designed to perform interpolation between two sets of latent samples based on a specified ratio, blending the characteristics of both sets to produce a new, intermediate set of latent samples.",
  "LatentMultiply": "The LatentMultiply node is designed to scale the latent representation of samples by a specified multiplier. This operation allows for the adjustment of the intensity or magnitude of features within the latent space, enabling fine-tuning of generated content or the exploration of variations within a given latent direction.",
  "LatentOperationSharpen": "LatentOperationSharpen",
  "LatentOperationTonemapReinhard": "LatentOperationTonemapReinhard",
  "LatentRotate": "The LatentRotate node is designed to rotate latent representations of images by specified angles. It abstracts the complexity of manipulating latent space to achieve rotation effects, enabling users to easily transform images in a generative model\u2019s latent space.",
  "LatentSubtract": "The LatentSubtract node is designed for subtracting one latent representation from another. This operation can be used to manipulate or modify the characteristics of generative models\u2019 outputs by effectively removing features or attributes represented in one latent space from another.",
  "LatentUpscale": "The LatentUpscale node is designed for upscaling latent representations of images. It allows for the adjustment of the output image\u2019s dimensions and the method of upscaling, providing flexibility in enhancing the resolution of latent images.",
  "LatentUpscaleBy": "The LatentUpscaleBy node is designed for upscaling latent representations of images. It allows for the adjustment of the scale factor and the method of upscaling, providing flexibility in enhancing the resolution of latent samples.",
  "LatentUpscaleModelLoader": "LatentUpscaleModelLoader",
  "LazyCache": "A homebrew version of EasyCache - even 'easier' version of EasyCache to implement. Overall works worse than EasyCache, but better in some rare cases AND universal compatibility with everything in ComfyUI.\nThe model with LazyCache.",
  "Load3D": "Load3D",
  "LoadAudio": "LoadAudio",
  "LoadImage": "The LoadImage node is designed to load and preprocess images from a specified path. It handles image formats with multiple frames, applies necessary transformations such as rotation based on EXIF data, normalizes pixel values, and optionally generates a mask for images with an alpha channel. This node is essential for preparing images for further processing or analysis within a pipeline.",
  "LoadImageDataSetFromFolder": "List of loaded images",
  "LoadImageMask": "The LoadImageMask node is designed to load images and their associated masks from a specified path, processing them to ensure compatibility with further image manipulation or analysis tasks. It focuses on handling various image formats and conditions, such as presence of an alpha channel for masks, and prepares the images and masks for downstream processing by converting them to a standardized format.",
  "LoadImageOutput": "Load an image from the output folder. When the refresh button is clicked, the node will update the image list and automatically select the first image, allowing for easy iteration.",
  "LoadImageTextDataSetFromFolder": "List of loaded images\nList of text captions",
  "LoadLatent": "LoadLatent",
  "LoadTrainingDataset": "List of latent dicts\nList of conditioning lists",
  "LoadVideo": "LoadVideo",
  "LoraLoader": "LoRAs are used to modify diffusion and CLIP models, altering the way in which latents are denoised such as applying styles. Multiple LoRA nodes can be linked together.\nThe modified diffusion model.\nThe modified CLIP model.This node automatically detects models located in the LoRA folder (including subfolders) with the corresponding model path being `ComfyUI\\models\\loras`. The LoRA Loader node is primarily used to load LoRA models. You can think of LoRA models as filters that can give your images specific styles, content, and details: Apply specific artistic styles (like ink painting) Add characteristics of certain characters (like game characters) Add specific details to the image All of these can be achieved through LoRA.",
  "LoraLoaderModelOnly": "LoRAs are used to modify diffusion and CLIP models, altering the way in which latents are denoised such as applying styles. Multiple LoRA nodes can be linked together.\nThe modified diffusion model.\nThe modified CLIP model.This node specializes in loading a LoRA model without requiring a CLIP model, focusing on enhancing or modifying a given model based on LoRA parameters. It allows for the dynamic adjustment of the model\u2019s strength through LoRA parameters, facilitating fine-tuned control over the model\u2019s behavior.",
  "LoraModelLoader": "The modified diffusion model.",
  "LoraSave": "LoraSave",
  "LossGraphNode": "LossGraphNode",
  "LotusConditioning": "LotusConditioning",
  "LtxvApiImageToVideo": "Professional-quality videos with customizable duration and resolution based on start image.",
  "LtxvApiTextToVideo": "Professional-quality videos with customizable duration and resolution.",
  "LumaConceptsNode": "Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.",
  "LumaImageModifyNode": "Modifies images synchronously based on prompt and aspect ratio.",
  "LumaImageNode": "Generates images synchronously based on prompt and aspect ratio.",
  "LumaImageToVideoNode": "Generates videos synchronously based on prompt, input images, and output_size.",
  "LumaReferenceNode": "Holds an image and weight for use with Luma Generate Image node.",
  "LumaVideoNode": "Generates videos synchronously based on prompt and output_size.",
  "Mahiro": "Modify the guidance to scale more on the 'direction' of the positive prompt rather than the difference between the negative prompt.",
  "MakeTrainingDataset": "List of latent dicts\nList of conditioning lists",
  "MaskComposite": "This node specializes in combining two mask inputs through a variety of operations such as addition, subtraction, and logical operations, to produce a new, modified mask. It abstractly handles the manipulation of mask data to achieve complex masking effects, serving as a crucial component in mask-based image editing and processing workflows.",
  "MaskPreview": "Saves the input images to your ComfyUI output directory.",
  "MaskToImage": "The `MaskToImage` node is designed to convert a mask into an image format. This transformation allows for the visualization and further processing of masks as images, facilitating a bridge between mask-based operations and image-based applications.",
  "MergeImageLists": "Processed images",
  "MergeTextLists": "Processed texts",
  "MinimaxHailuoVideoNode": "Generates videos from prompt, with optional start frame using the new MiniMax Hailuo-02 model.",
  "MinimaxImageToVideoNode": "Generates videos synchronously based on an image and prompt, and optional parameters.",
  "MinimaxTextToVideoNode": "Generates videos synchronously based on a prompt, and optional parameters.",
  "ModelComputeDtype": "ModelComputeDtype",
  "ModelMergeAdd": "The ModelMergeAdd node is designed for merging two models by adding key patches from one model to another. This process involves cloning the first model and then applying patches from the second model, allowing for the combination of features or behaviors from both models.",
  "ModelMergeAuraflow": "ModelMergeAuraflow",
  "ModelMergeBlocks": "ModelMergeBlocks is designed for advanced model merging operations, allowing for the integration of two models with customizable blending ratios for different parts of the models. This node facilitates the creation of hybrid models by selectively merging components from two source models based on specified parameters.",
  "ModelMergeCosmos14B": "ModelMergeCosmos14B",
  "ModelMergeCosmos7B": "ModelMergeCosmos7B",
  "ModelMergeCosmosPredict2_14B": "ModelMergeCosmosPredict2_14B",
  "ModelMergeCosmosPredict2_2B": "ModelMergeCosmosPredict2_2B",
  "ModelMergeFlux1": "ModelMergeFlux1",
  "ModelMergeLTXV": "ModelMergeLTXV",
  "ModelMergeMochiPreview": "ModelMergeMochiPreview",
  "ModelMergeQwenImage": "ModelMergeQwenImage",
  "ModelMergeSD1": "ModelMergeSD1",
  "ModelMergeSD2": "ModelMergeSD2",
  "ModelMergeSD35_Large": "ModelMergeSD35_Large",
  "ModelMergeSD3_2B": "ModelMergeSD3_2B",
  "ModelMergeSDXL": "ModelMergeSDXL",
  "ModelMergeSimple": "The ModelMergeSimple node is designed for merging two models by blending their parameters based on a specified ratio. This node facilitates the creation of hybrid models that combine the strengths or characteristics of both input models.",
  "ModelMergeSubtract": "This node is designed for advanced model merging operations, specifically to subtract the parameters of one model from another based on a specified multiplier. It enables the customization of model behaviors by adjusting the influence of one model\u2019s parameters over another, facilitating the creation of new, hybrid models.",
  "ModelMergeWAN2_1": "1.3B model has 30 blocks, 14B model has 40 blocks. Image to video model has the extra img_emb.",
  "ModelPatchLoader": "ModelPatchLoader",
  "ModelSamplingAuraFlow": "ModelSamplingAuraFlow",
  "ModelSamplingContinuousEDM": "ModelSamplingContinuousEDM",
  "ModelSamplingContinuousEdm": "This node is designed to enhance a model\u2019s sampling capabilities by integrating continuous EDM (Energy-based Diffusion Models) sampling techniques. It allows for the dynamic adjustment of the noise levels within the model\u2019s sampling process, offering a more refined control over the generation quality and diversity.",
  "ModelSamplingContinuousV": "ModelSamplingContinuousV",
  "ModelSamplingDiscrete": "This node is designed to modify the sampling behavior of a model by applying a discrete sampling strategy. It allows for the selection of different sampling methods, such as epsilon, v_prediction, lcm, or x0, and optionally adjusts the model\u2019s noise reduction strategy based on the zero-shot noise ratio (zsnr) setting.",
  "ModelSamplingFlux": "ModelSamplingFlux",
  "ModelSamplingLTXV": "ModelSamplingLTXV",
  "ModelSamplingSD3": "ModelSamplingSD3",
  "ModelSamplingStableCascade": "ModelSamplingStableCascade",
  "ModelSave": "ModelSave",
  "MoonvalleyImg2VideoNode": "Moonvalley Marey Image to Video Node",
  "MoonvalleyTxt2VideoNode": "MoonvalleyTxt2VideoNode",
  "MoonvalleyVideo2VideoNode": "MoonvalleyVideo2VideoNode",
  "Morphology": "Morphology",
  "NormalizeImages": "Processed images",
  "Note": "Used to add text annotations in the workflow",
  "OpenAIChatConfig": "Allows specifying advanced configuration options for the OpenAI Chat Nodes.",
  "OpenAIChatNode": "Generate text responses from an OpenAI model.",
  "OpenAIDalle2": "Generates images synchronously via OpenAI's DALL\u00b7E 2 endpoint.",
  "OpenAIDalle3": "Generates images synchronously via OpenAI's DALL\u00b7E 3 endpoint.",
  "OpenAIGPTImage1": "Generates images synchronously via OpenAI's GPT Image 1 endpoint.",
  "OpenAIInputFiles": "Loads and prepares input files (text, pdf, etc.) to include as inputs for the OpenAI Chat Node. The files will be read by the OpenAI model when generating a response. \ud83d\udec8 TIP: Can be chained together with other OpenAI Input File nodes.",
  "OpenAIVideoSora2": "OpenAI video and audio generation.",
  "OptimalStepsScheduler": "OptimalStepsScheduler",
  "PairConditioningCombine": "PairConditioningCombine",
  "PairConditioningSetDefaultCombine": "PairConditioningSetDefaultCombine",
  "PairConditioningSetProperties": "PairConditioningSetProperties",
  "PairConditioningSetPropertiesAndCombine": "PairConditioningSetPropertiesAndCombine",
  "PatchModelAddDownscale": "PatchModelAddDownscale",
  "PerpNeg": "PerpNeg",
  "PerpNegGuider": "PerpNegGuider",
  "PerturbedAttentionGuidance": "PerturbedAttentionGuidance",
  "PhotoMakerEncode": "PhotoMakerEncode",
  "PhotoMakerLoader": "PhotoMakerLoader",
  "PikaImageToVideoNode2_2": "Sends an image and prompt to the Pika API v2.2 to generate a video.",
  "PikaScenesV2_2": "Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them.",
  "PikaStartEndFrameNode2_2": "Generate a video by combining your first and last frame. Upload two images to define the start and end points, and let the AI create a smooth transition between them.",
  "PikaTextToVideoNode2_2": "Sends a text prompt to the Pika API v2.2 to generate a video.",
  "Pikadditions": "Add any object or image into your video. Upload a video and specify what you'd like to add to create a seamlessly integrated result.",
  "Pikaffects": "Generate a video with a specific Pikaffect. Supported Pikaffects: Cake-ify, Crumble, Crush, Decapitate, Deflate, Dissolve, Explode, Eye-pop, Inflate, Levitate, Melt, Peel, Poke, Squish, Ta-da, Tear",
  "Pikaswaps": "Swap out any object or region of your video with a new image or object. Define areas to replace either with a mask or coordinates.",
  "PixverseImageToVideoNode": "Generates videos based on prompt and output_size.",
  "PixverseTemplateNode": "PixverseTemplateNode",
  "PixverseTextToVideoNode": "Generates videos based on prompt and output_size.",
  "PixverseTransitionVideoNode": "Generates videos based on prompt and output_size.",
  "PolyexponentialScheduler": "The PolyexponentialScheduler node is designed to generate a sequence of noise levels (sigmas) based on a polyexponential noise schedule. This schedule is a polynomial function in the logarithm of sigma, allowing for a flexible and customizable progression of noise levels throughout the diffusion process.",
  "PorterDuffImageComposite": "The PorterDuffImageComposite node is designed to perform image compositing using the Porter-Duff compositing operators. It allows for the combination of source and destination images according to various blending modes, enabling the creation of complex visual effects by manipulating image transparency and overlaying images in creative ways.",
  "Preview3D": "Preview3D",
  "PreviewAny": "PreviewAny",
  "PreviewAudio": "PreviewAudio",
  "PreviewImage": "Saves the input images to your ComfyUI output directory.The PreviewImage node is designed for creating temporary preview images. It automatically generates a unique temporary file name for each image, compresses the image to a specified level, and saves it to a temporary directory. This functionality is particularly useful for generating previews of images during processing without affecting the original files.",
  "Primitive": "The primitive node can recognize the type of input connected to it and provide input data accordingly. When this node is connected to different input types, it will change to different input states. It can be used to use a unified parameter among multiple different nodes, such as using the same seed in multiple Ksampler.",
  "PrimitiveBoolean": "PrimitiveBoolean",
  "PrimitiveFloat": "PrimitiveFloat",
  "PrimitiveInt": "PrimitiveInt",
  "PrimitiveString": "PrimitiveString",
  "PrimitiveStringMultiline": "PrimitiveStringMultiline",
  "QuadrupleCLIPLoader": "[Recipes]\n\nhidream: long clip-l, long clip-g, t5xxl, llama_8b_3.1_instruct",
  "QuadrupleClipLoader": "The Quadruple CLIP Loader, QuadrupleCLIPLoader, is one of the core nodes of ComfyUI, first added to support the HiDream I1 version model. If you find this node missing, try updating ComfyUI to the latest version to ensure node support.",
  "QwenImageDiffsynthControlnet": "QwenImageDiffsynthControlnet",
  "RandomCropImages": "Processed images",
  "RandomNoise": "RandomNoise",
  "RebatchImages": "The RebatchImages node is designed to reorganize a batch of images into a new batch configuration, adjusting the batch size as specified. This process is essential for managing and optimizing the processing of image data in batch operations, ensuring that images are grouped according to the desired batch size for efficient handling.",
  "RebatchLatents": "The RebatchLatents node is designed to reorganize a batch of latent representations into a new batch configuration, based on a specified batch size. It ensures that the latent samples are grouped appropriately, handling variations in dimensions and sizes, to facilitate further processing or model inference.",
  "RecordAudio": "RecordAudio",
  "RecraftColorRGB": "Create Recraft Color by choosing specific RGB values.",
  "RecraftControls": "Create Recraft Controls for customizing Recraft generation.",
  "RecraftCreativeUpscaleNode": "Upscale image synchronously.\nEnhances a given raster image using \u2018creative upscale\u2019 tool, boosting resolution with a focus on refining small details and faces.",
  "RecraftCrispUpscaleNode": "Upscale image synchronously.\nEnhances a given raster image using \u2018crisp upscale\u2019 tool, increasing image resolution, making the image sharper and cleaner.",
  "RecraftImageInpaintingNode": "Modify image based on prompt and mask.",
  "RecraftImageToImageNode": "Modify image based on prompt and strength.",
  "RecraftRemoveBackgroundNode": "Remove background from image, and return processed image and mask.",
  "RecraftReplaceBackgroundNode": "Replace background on image, based on provided prompt.",
  "RecraftStyleV3DigitalIllustration": "Select realistic_image style and optional substyle.",
  "RecraftStyleV3InfiniteStyleLibrary": "Select style based on preexisting UUID from Recraft's Infinite Style Library.",
  "RecraftStyleV3LogoRaster": "Select realistic_image style and optional substyle.",
  "RecraftStyleV3RealisticImage": "Select realistic_image style and optional substyle.",
  "RecraftTextToImageNode": "Generates images synchronously based on prompt and resolution.",
  "RecraftTextToVectorNode": "Generates SVG synchronously based on prompt and resolution.",
  "RecraftVectorizeImageNode": "Generates SVG synchronously from an input image.",
  "ReferenceLatent": "This node sets the guiding latent for an edit model. If the model supports it you can chain multiple to set multiple reference images.",
  "RegexExtract": "RegexExtract",
  "RegexMatch": "RegexMatch",
  "RegexReplace": "Find and replace text using regex patterns.",
  "RenormCFG": "RenormCFG",
  "RepeatImageBatch": "The RepeatImageBatch node is designed to replicate a given image a specified number of times, creating a batch of identical images. This functionality is useful for operations that require multiple instances of the same image, such as batch processing or data augmentation.",
  "RepeatLatentBatch": "The RepeatLatentBatch node is designed to replicate a given batch of latent representations a specified number of times, potentially including additional data like noise masks and batch indices. This functionality is crucial for operations that require multiple instances of the same latent data, such as data augmentation or specific generative tasks.",
  "ReplaceText": "Processed texts",
  "Reroute": "Mainly used to organize the logic of overly long connection lines in the ComfyUI workflow.",
  "RescaleCFG": "RescaleCFG",
  "RescaleCfg": "The RescaleCFG node is designed to adjust the conditioning and unconditioning scales of a model\u2019s output based on a specified multiplier, aiming to achieve a more balanced and controlled generation process. It operates by rescaling the model\u2019s output to modify the influence of conditioned and unconditioned components, thereby potentially enhancing the model\u2019s performance or output quality.",
  "ResizeAndPadImage": "ResizeAndPadImage",
  "ResizeImagesByLongerEdge": "Processed images",
  "ResizeImagesByShorterEdge": "Processed images",
  "Rodin3D_Detail": "Generate 3D Assets using Rodin API",
  "Rodin3D_Gen2": "Generate 3D Assets using Rodin API",
  "Rodin3D_Regular": "Generate 3D Assets using Rodin API",
  "Rodin3D_Sketch": "Generate 3D Assets using Rodin API",
  "Rodin3D_Smooth": "Generate 3D Assets using Rodin API",
  "RunwayFirstLastFrameNode": "Upload first and last keyframes, draft a prompt, and generate a video. More complex transitions, such as cases where the Last frame is completely different from the First frame, may benefit from the longer 10s duration. This would give the generation more time to smoothly transition between the two inputs. Before diving in, review these best practices to ensure that your input selections will set your generation up for success: https://help.runwayml.com/hc/en-us/articles/34170748696595-Creating-with-Keyframes-on-Gen-3.",
  "RunwayImageToVideoNodeGen3a": "Generate a video from a single starting frame using Gen3a Turbo model. Before diving in, review these best practices to ensure that your input selections will set your generation up for success: https://help.runwayml.com/hc/en-us/articles/33927968552339-Creating-with-Act-One-on-Gen-3-Alpha-and-Turbo.",
  "RunwayImageToVideoNodeGen4": "Generate a video from a single starting frame using Gen4 Turbo model. Before diving in, review these best practices to ensure that your input selections will set your generation up for success: https://help.runwayml.com/hc/en-us/articles/37327109429011-Creating-with-Gen-4-Video.",
  "RunwayTextToImageNode": "Generate an image from a text prompt using Runway's Gen 4 model. You can also include reference image to guide the generation.",
  "SDTurboScheduler": "SDTurboScheduler",
  "SD_4XUpscale_Conditioning": "SD_4XUpscale_Conditioning",
  "SV3D_Conditioning": "SV3D_Conditioning",
  "SVD_img2vid_Conditioning": "SVD_img2vid_Conditioning",
  "SamplerCustom": "The SamplerCustom node is designed to provide a flexible and customizable sampling mechanism for various applications. It enables users to select and configure different sampling strategies tailored to their specific needs, enhancing the adaptability and efficiency of the sampling process.",
  "SamplerCustomAdvanced": "SamplerCustomAdvanced",
  "SamplerDPMAdaptative": "SamplerDPMAdaptative",
  "SamplerDPMPP_2M_SDE": "SamplerDPMPP_2M_SDE",
  "SamplerDPMPP_2S_Ancestral": "SamplerDPMPP_2S_Ancestral",
  "SamplerDPMPP_3M_SDE": "SamplerDPMPP_3M_SDE",
  "SamplerDPMPP_SDE": "SamplerDPMPP_SDE",
  "SamplerDpmpp2mSde": "This node is designed to generate a sampler for the DPMPP_2M_SDE model, allowing for the creation of samples based on specified solver types, noise levels, and computational device preferences. It abstracts the complexities of sampler configuration, providing a streamlined interface for generating samples with customized settings.",
  "SamplerDpmppSde": "This node is designed to generate a sampler for the DPM++ SDE (Stochastic Differential Equation) model. It adapts to both CPU and GPU execution environments, optimizing the sampler\u2019s implementation based on the available hardware.",
  "SamplerER_SDE": "SamplerER_SDE",
  "SamplerEulerAncestral": "SamplerEulerAncestral",
  "SamplerEulerAncestralCFGPP": "SamplerEulerAncestralCFGPP",
  "SamplerEulerCFGpp": "SamplerEulerCFGpp",
  "SamplerLCMUpscale": "SamplerLCMUpscale",
  "SamplerLMS": "SamplerLMS",
  "SamplerSASolver": "SamplerSASolver",
  "SamplingPercentToSigma": "SamplingPercentToSigma",
  "SaveAnimatedPNG": "SaveAnimatedPNG",
  "SaveAnimatedPng": "The SaveAnimatedPNG node is designed for creating and saving animated PNG images from a sequence of frames. It handles the assembly of individual image frames into a cohesive animation, allowing for customization of frame duration, looping, and metadata inclusion.",
  "SaveAnimatedWEBP": "SaveAnimatedWEBP",
  "SaveAnimatedWebp": "This node is designed for saving a sequence of images as an animated WEBP file. It handles the aggregation of individual frames into a cohesive animation, applying specified metadata, and optimizing the output based on quality and compression settings.",
  "SaveAudio": "SaveAudio",
  "SaveAudioMP3": "SaveAudioMP3",
  "SaveAudioOpus": "SaveAudioOpus",
  "SaveGLB": "SaveGLB",
  "SaveImage": "Saves the input images to your ComfyUI output directory.The `Save Image` node is mainly used to save images to the **output** folder in ComfyUI. If you only want to preview the image during the intermediate process rather than saving it, you can use the `Preview Image` node. Default save location: `ComfyUI/output/`",
  "SaveImageDataSetToFolder": "SaveImageDataSetToFolder",
  "SaveImageTextDataSetToFolder": "SaveImageTextDataSetToFolder",
  "SaveImageWebsocket": "SaveImageWebsocket",
  "SaveLatent": "SaveLatent",
  "SaveLoRA": "SaveLoRA",
  "SaveSVGNode": "Save SVG files on disk.",
  "SaveTrainingDataset": "SaveTrainingDataset",
  "SaveVideo": "Saves the input images to your ComfyUI output directory.",
  "SaveWEBM": "SaveWEBM",
  "ScaleROPE": "Scale and shift the ROPE of the model.",
  "Sd4xupscaleConditioning": "This node specializes in enhancing the resolution of images through a 4x upscale process, incorporating conditioning elements to refine the output. It leverages diffusion techniques to upscale images while allowing for the adjustment of scale ratio and noise augmentation to fine-tune the enhancement process.",
  "SdTurboScheduler": "SDTurboScheduler is designed to generate a sequence of sigma values for image sampling, adjusting the sequence based on the denoise level and the number of steps specified. It leverages a specific model\u2019s sampling capabilities to produce these sigma values, which are crucial for controlling the denoising process during image generation.",
  "SelfAttentionGuidance": "SelfAttentionGuidance",
  "SetClipHooks": "SetClipHooks",
  "SetFirstSigma": "SetFirstSigma",
  "SetHookKeyframes": "SetHookKeyframes",
  "SetLatentNoiseMask": "This node is designed to apply a noise mask to a set of latent samples. It modifies the input samples by integrating a specified mask, thereby altering their noise characteristics.",
  "SetUnionControlNetType": "SetUnionControlNetType",
  "ShuffleDataset": "Processed images",
  "ShuffleImageTextDataset": "Shuffled images\nShuffled texts",
  "SkipLayerGuidanceDiT": "Generic version of SkipLayerGuidance node that can be used on every DiT model.",
  "SkipLayerGuidanceDiTSimple": "Simple version of the SkipLayerGuidanceDiT node that only modifies the uncond pass.",
  "SkipLayerGuidanceSD3": "Generic version of SkipLayerGuidance node that can be used on every DiT model.",
  "SolidMask": "The SolidMask node generates a uniform mask with a specified value across its entire area. It\u2019s designed to create masks of specific dimensions and intensity, useful in various image processing and masking tasks.",
  "SplitAudioChannels": "Separates the audio into left and right channels.",
  "SplitImageWithAlpha": "The SplitImageWithAlpha node is designed to separate the color and alpha components of an image. It processes an input image tensor, extracting the RGB channels as the color component and the alpha channel as the transparency component, facilitating operations that require manipulation of these distinct image aspects.",
  "SplitSigmas": "The SplitSigmas node is designed for dividing a sequence of sigma values into two parts based on a specified step. This functionality is crucial for operations that require different handling or processing of the initial and subsequent parts of the sigma sequence, enabling more flexible and targeted manipulation of these values.",
  "SplitSigmasDenoise": "SplitSigmasDenoise",
  "StabilityAudioInpaint": "Transforms part of existing audio sample using text instructions.",
  "StabilityAudioToAudio": "Transforms existing audio samples into new high-quality compositions using text instructions.",
  "StabilityStableImageSD_3_5Node": "Generates images synchronously based on prompt and resolution.",
  "StabilityStableImageUltraNode": "Generates images synchronously based on prompt and resolution.",
  "StabilityTextToAudio": "Generates high-quality music and sound effects from text descriptions.",
  "StabilityUpscaleConservativeNode": "Upscale image with minimal alterations to 4K resolution.",
  "StabilityUpscaleCreativeNode": "Upscale image with minimal alterations to 4K resolution.",
  "StabilityUpscaleFastNode": "Quickly upscales an image via Stability API call to 4x its original size; intended for upscaling low-quality/compressed images.",
  "StableCascade_EmptyLatentImage": "StableCascade_EmptyLatentImage",
  "StableCascade_StageB_Conditioning": "StableCascade_StageB_Conditioning",
  "StableCascade_StageC_VAEEncode": "StableCascade_StageC_VAEEncode",
  "StableCascade_SuperResolutionControlnet": "StableCascade_SuperResolutionControlnet",
  "StableZero123_Conditioning": "StableZero123_Conditioning",
  "StableZero123_Conditioning_Batched": "StableZero123_Conditioning_Batched",
  "Stablezero123Conditioning": "This node is designed to process and condition data for use in StableZero123 models, focusing on preparing the input in a specific format that is compatible and optimized for these models.",
  "Stablezero123ConditioningBatched": "This node is designed to process conditioning information in a batched manner specifically tailored for the StableZero123 model. It focuses on efficiently handling multiple sets of conditioning data simultaneously, optimizing the workflow for scenarios where batch processing is crucial.",
  "StringCompare": "StringCompare",
  "StringConcatenate": "StringConcatenate",
  "StringContains": "StringContains",
  "StringLength": "StringLength",
  "StringReplace": "StringReplace",
  "StringSubstring": "StringSubstring",
  "StringTrim": "StringTrim",
  "StripWhitespace": "Processed texts",
  "StyleModelApply": "This node applies a style model to a given conditioning, enhancing or altering its style based on the output of a CLIP vision model. It integrates the style model\u2019s conditioning into the existing conditioning, allowing for a seamless blend of styles in the generation process.",
  "StyleModelLoader": "The StyleModelLoader node is designed to load a style model from a specified path. It focuses on retrieving and initializing style models that can be used to apply specific artistic styles to images, thereby enabling the customization of visual outputs based on the loaded style model.",
  "SvdImg2vidConditioning": "This node is designed for generating conditioning data for video generation tasks, specifically tailored for use with SVD_img2vid models. It takes various inputs including initial images, video parameters, and a VAE model to produce conditioning data that can be used to guide the generation of video frames.",
  "T5TokenizerOptions": "T5TokenizerOptions",
  "TCFG": "TCFG \u2013 Tangential Damping CFG (2503.18137)\n\nRefine the uncond (negative) to align with the cond (positive) for improving quality.",
  "TemporalScoreRescaling": "[Post-CFG Function]\nTSR - Temporal Score Rescaling (2510.01184)\n\nRescaling the model's score or noise to steer the sampling diversity.",
  "TerminalLog": "Terminal Log (Manager) node is primarily used to display the running information of ComfyUI in the terminal within the ComfyUI interface. To use it, you need to set the `mode` to **logging** mode. This will allow it to record corresponding log information during the image generation task. If the `mode` is set to **stop** mode, it will not record log information. When you access and use ComfyUI via remote connections or local area network connections, Terminal Log (Manager) node becomes particularly useful. It allows you to directly view error messages from the CMD within the ComfyUI interface, making it easier to understand the current status of ComfyUI\u2019s operation.",
  "TextEncodeAceStepAudio": "TextEncodeAceStepAudio",
  "TextEncodeHunyuanVideo_ImageToVideo": "TextEncodeHunyuanVideo_ImageToVideo",
  "TextEncodeQwenImageEdit": "TextEncodeQwenImageEdit",
  "TextEncodeQwenImageEditPlus": "TextEncodeQwenImageEditPlus",
  "TextToLowercase": "Processed texts",
  "TextToUppercase": "Processed texts",
  "ThresholdMask": "ThresholdMask",
  "TomePatchModel": "TomePatchModel",
  "TopazImageEnhance": "Industry-standard upscaling and image enhancement.",
  "TopazVideoEnhance": "Breathe new life into video with powerful upscaling and recovery technology.",
  "TorchCompileModel": "TorchCompileModel",
  "TrainLoraNode": "Model with LoRA applied\nLoRA weights\nLoss history\nTotal training steps",
  "TrimAudioDuration": "Trim audio tensor into chosen time range.",
  "TrimVideoLatent": "TrimVideoLatent",
  "TripleCLIPLoader": "[Recipes]\n\nsd3: clip-l, clip-g, t5",
  "TripoConversionNode": "TripoConversionNode",
  "TripoImageToModelNode": "TripoImageToModelNode",
  "TripoMultiviewToModelNode": "TripoMultiviewToModelNode",
  "TripoRefineNode": "Refine a draft model created by v1.4 Tripo models only.",
  "TripoRetargetNode": "TripoRetargetNode",
  "TripoRigNode": "TripoRigNode",
  "TripoTextToModelNode": "TripoTextToModelNode",
  "TripoTextureNode": "TripoTextureNode",
  "TruncateText": "Processed texts",
  "UNETLoader": "UNETLoader",
  "UNetCrossAttentionMultiply": "UNetCrossAttentionMultiply",
  "UNetSelfAttentionMultiply": "UNetSelfAttentionMultiply",
  "UNetTemporalAttentionMultiply": "UNetTemporalAttentionMultiply",
  "USOStyleReference": "USOStyleReference",
  "UnclipCheckpointLoader": "The unCLIPCheckpointLoader node is designed for loading checkpoints specifically tailored for unCLIP models. It facilitates the retrieval and initialization of models, CLIP vision modules, and VAEs from a specified checkpoint, streamlining the setup process for further operations or analyses.",
  "UnclipConditioning": "This node is designed to integrate CLIP vision outputs into the conditioning process, adjusting the influence of these outputs based on specified strength and noise augmentation parameters. It enriches the conditioning with visual context, enhancing the generation process.",
  "UnetLoader": "The UNETLoader node is designed for loading U-Net models by name, facilitating the use of pre-trained U-Net architectures within the system.",
  "UpscaleModelLoader": "The UpscaleModelLoader node is designed for loading upscale models from a specified directory. It facilitates the retrieval and preparation of upscale models for image upscaling tasks, ensuring that the models are correctly loaded and configured for evaluation.",
  "VAEDecode": "Decodes latent images back into pixel space images.\nThe decoded image.",
  "VAEDecodeAudio": "VAEDecodeAudio",
  "VAEDecodeHunyuan3D": "VAEDecodeHunyuan3D",
  "VAEDecodeTiled": "VAEDecodeTiled",
  "VAEEncode": "VAEEncode",
  "VAEEncodeAudio": "VAEEncodeAudio",
  "VAEEncodeForInpaint": "VAEEncodeForInpaint",
  "VAEEncodeTiled": "VAEEncodeTiled",
  "VAELoader": "VAELoader",
  "VAESave": "VAESave",
  "VPScheduler": "VPScheduler",
  "VaeDecode": "The VAEDecode node is designed for decoding latent representations into images using a specified Variational Autoencoder (VAE). It serves the purpose of generating images from compressed data representations, facilitating the reconstruction of images from their latent space encodings.",
  "VaeEncode": "This node is designed for encoding images into a latent space representation using a specified VAE model. It abstracts the complexity of the encoding process, providing a straightforward way to transform images into their latent representations.",
  "VaeEncodeForInpaint": "This node is designed for encoding images into a latent representation suitable for inpainting tasks, incorporating additional preprocessing steps to adjust the input image and mask for optimal encoding by the VAE model.",
  "VaeLoader": "The VAELoader node is designed for loading Variational Autoencoder (VAE) models, specifically tailored to handle both standard and approximate VAEs. It supports loading VAEs by name, including specialized handling for \u2018taesd\u2019 and \u2018taesdxl\u2019 models, and dynamically adjusts based on the VAE\u2019s specific configuration.",
  "VaeSave": "The VAESave node is designed for saving VAE models along with their metadata, including prompts and additional PNG information, to a specified output directory. It encapsulates the functionality to serialize the model state and associated information into a file, facilitating the preservation and sharing of trained models.",
  "Veo3FirstLastFrameNode": "Generate video using prompt and first and last frames.",
  "Veo3VideoGenerationNode": "Generates videos from text prompts using Google's Veo 3 API",
  "VeoVideoGenerationNode": "Generates videos from text prompts using Google's Veo 2 API",
  "VideoLinearCFGGuidance": "VideoLinearCFGGuidance",
  "VideoLinearCfgGuidance": "The VideoLinearCFGGuidance node applies a linear conditioning guidance scale to a video model, adjusting the influence of conditioned and unconditioned components over a specified range. This enables dynamic control over the generation process, allowing for fine-tuning of the model\u2019s output based on the desired level of conditioning.",
  "VideoTriangleCFGGuidance": "VideoTriangleCFGGuidance",
  "ViduImageToVideoNode": "Generate video from image and optional prompt",
  "ViduReferenceVideoNode": "Generate video from multiple images and prompt",
  "ViduStartEndToVideoNode": "Generate a video from start and end frames and a prompt",
  "ViduTextToVideoNode": "Generate video from text prompt",
  "VoxelToMesh": "VoxelToMesh",
  "VoxelToMeshBasic": "VoxelToMeshBasic",
  "VpScheduler": "The VPScheduler node is designed to generate a sequence of noise levels (sigmas) based on the Variance Preserving (VP) scheduling method. This sequence is crucial for guiding the denoising process in diffusion models, allowing for controlled generation of images or other data types.",
  "Wan22FunControlToVideo": "Wan22FunControlToVideo",
  "Wan22ImageToVideoLatent": "Wan22ImageToVideoLatent",
  "WanAnimateToVideo": "WanAnimateToVideo",
  "WanCameraEmbedding": "WanCameraEmbedding",
  "WanCameraImageToVideo": "WanCameraImageToVideo",
  "WanContextWindowsManual": "Manually set context windows for WAN-like models (dim=2).\nThe model with context windows applied during sampling.",
  "WanFirstLastFrameToVideo": "WanFirstLastFrameToVideo",
  "WanFunControlToVideo": "Prepare the conditioning information needed for video generation, using the Wan 2.1 Fun Control model.",
  "WanFunInpaintToVideo": "WanFunInpaintToVideo",
  "WanHuMoImageToVideo": "WanHuMoImageToVideo",
  "WanImageToImageApi": "Generates an image from one or two input images and a text prompt. The output image is currently fixed at 1.6 MP; its aspect ratio matches the input image(s).",
  "WanImageToVideo": "WanImageToVideo",
  "WanImageToVideoApi": "Generates video based on the first frame and text prompt.",
  "WanPhantomSubjectToVideo": "WanPhantomSubjectToVideo",
  "WanSoundImageToVideo": "WanSoundImageToVideo",
  "WanSoundImageToVideoExtend": "WanSoundImageToVideoExtend",
  "WanTextToImageApi": "Generates image based on text prompt.",
  "WanTextToVideoApi": "Generates video based on text prompt.",
  "WanTrackToVideo": "WanTrackToVideo",
  "WanVaceToVideo": "WanVaceToVideo",
  "WebcamCapture": "WebcamCapture",
  "unCLIPCheckpointLoader": "unCLIPCheckpointLoader",
  "unCLIPConditioning": "unCLIPConditioning",
  "wanBlockSwap": "NOP"
}