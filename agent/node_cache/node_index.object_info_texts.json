{
  "APG": "Adaptive Projected Guidance\n\n[required inputs]\nmodel: MODEL\neta: FLOAT | Controls the scale of the parallel guidance vector. Default CFG behavior at a setting of 1. | default=1.0 | min=-10.0 | max=10.0 | step=0.01\nnorm_threshold: FLOAT | Normalize guidance vector to this value, normalization disable at a setting of 0. | default=5.0 | min=0.0 | max=50.0 | step=0.1\nmomentum: FLOAT | Controls a running average of guidance during diffusion, disabled at a setting of 0. | default=0.0 | min=-5.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "AddNoise": "[required inputs]\nmodel: MODEL\nnoise: NOISE\nsigmas: SIGMAS\nlatent_image: LATENT\n\n[outputs]\nLATENT: LATENT",
  "AddTextPrefix": "Add Text Prefix\n\n[required inputs]\ntexts: STRING | Text to process.\nprefix: STRING | Prefix to add. | default=\n\n[outputs]\ntexts: STRING | Processed texts",
  "AddTextSuffix": "Add Text Suffix\n\n[required inputs]\ntexts: STRING | Text to process.\nsuffix: STRING | Suffix to add. | default=\n\n[outputs]\ntexts: STRING | Processed texts",
  "AdjustBrightness": "Adjust Brightness\n\n[required inputs]\nimages: IMAGE | Image to process.\nfactor: FLOAT | Brightness factor. 1.0 = no change, <1.0 = darker, >1.0 = brighter. | default=1.0 | min=0.0 | max=2.0\n\n[outputs]\nimages: IMAGE | Processed images",
  "AdjustContrast": "Adjust Contrast\n\n[required inputs]\nimages: IMAGE | Image to process.\nfactor: FLOAT | Contrast factor. 1.0 = no change, <1.0 = less contrast, >1.0 = more contrast. | default=1.0 | min=0.0 | max=2.0\n\n[outputs]\nimages: IMAGE | Processed images",
  "AlignYourStepsScheduler": "[required inputs]\nmodel_type: COMBO | options=SD1, SDXL, SVD\nsteps: INT | default=10 | min=1 | max=10000\ndenoise: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nSIGMAS: SIGMAS",
  "AudioAdjustVolume": "Audio Adjust Volume\n\n[required inputs]\naudio: AUDIO\nvolume: INT | Volume adjustment in decibels (dB). 0 = no change, +6 = double, -6 = half, etc | default=1.0 | min=-100 | max=100\n\n[outputs]\nAUDIO: AUDIO",
  "AudioConcat": "Audio Concat\nConcatenates the audio1 to audio2 in the specified direction.\n\n[required inputs]\naudio1: AUDIO\naudio2: AUDIO\ndirection: COMBO | Whether to append audio2 after or before audio1. | default=after | options=after, before\n\n[outputs]\nAUDIO: AUDIO",
  "AudioEncoderEncode": "[required inputs]\naudio_encoder: AUDIO_ENCODER\naudio: AUDIO\n\n[outputs]\nAUDIO_ENCODER_OUTPUT: AUDIO_ENCODER_OUTPUT",
  "AudioEncoderLoader": "[required inputs]\naudio_encoder_name: COMBO\n\n[outputs]\nAUDIO_ENCODER: AUDIO_ENCODER",
  "AudioMerge": "Audio Merge\nCombine two audio tracks by overlaying their waveforms.\n\n[required inputs]\naudio1: AUDIO\naudio2: AUDIO\nmerge_method: COMBO | The method used to combine the audio waveforms. | options=add, mean, subtract, multiply\n\n[outputs]\nAUDIO: AUDIO",
  "BasicGuider": "[required inputs]\nmodel: MODEL\nconditioning: CONDITIONING\n\n[outputs]\nGUIDER: GUIDER",
  "BasicScheduler": "[required inputs]\nmodel: MODEL\nscheduler: COMBO | options=simple, sgm_uniform, karras, exponential, ddim_uniform, beta, normal, linear_quadratic, kl_optimal\nsteps: INT | default=20 | min=1 | max=10000\ndenoise: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nSIGMAS: SIGMAS",
  "BetaSamplingScheduler": "[required inputs]\nmodel: MODEL\nsteps: INT | default=20 | min=1 | max=10000\nalpha: FLOAT | default=0.6 | min=0.0 | max=50.0 | step=0.01 | round=False\nbeta: FLOAT | default=0.6 | min=0.0 | max=50.0 | step=0.01 | round=False\n\n[outputs]\nSIGMAS: SIGMAS",
  "ByteDanceFirstLastFrameNode": "ByteDance First-Last-Frame to Video\nGenerate video using prompt and first and last frames.\n\n[required inputs]\nmodel: COMBO | Model name | default=seedance-1-0-lite-i2v-250428 | options=seedance-1-0-pro-250528, seedance-1-0-lite-i2v-250428\nprompt: STRING | The text prompt used to generate the video.\nfirst_frame: IMAGE | First frame to be used for the video.\nlast_frame: IMAGE | Last frame to be used for the video.\nresolution: COMBO | The resolution of the output video. | options=480p, 720p, 1080p\naspect_ratio: COMBO | The aspect ratio of the output video. | options=adaptive, 16:9, 4:3, 1:1, 3:4, 9:16, 21:9\nduration: INT | The duration of the output video in seconds. | default=5 | min=3 | max=12 | step=1\n\n[optional inputs]\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\ncamera_fixed: BOOLEAN | Specifies whether to fix the camera. The platform appends an instruction to fix the camera to your prompt, but does not guarantee the actual effect. | default=False\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the video. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "ByteDanceImageEditNode": "ByteDance Image Edit\nEdit images using ByteDance models via api based on prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=seededit-3-0-i2i-250628 | options=seededit-3-0-i2i-250628\nimage: IMAGE | The base image to edit\nprompt: STRING | Instruction to edit image | default=\n\n[optional inputs]\nseed: INT | Seed to use for generation | default=0 | min=0 | max=2147483647 | step=1\nguidance_scale: FLOAT | Higher value makes the image follow the prompt more closely | default=5.5 | min=1.0 | max=10.0 | step=0.01\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the image | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "ByteDanceImageNode": "ByteDance Image\nGenerate images using ByteDance models via api based on prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=seedream-3-0-t2i-250415 | options=seedream-3-0-t2i-250415\nprompt: STRING | The text prompt used to generate the image\nsize_preset: COMBO | Pick a recommended size. Select Custom to use the width and height below | options=1024x1024 (1:1), 864x1152 (3:4), 1152x864 (4:3), 1280x720 (16:9), 720x1280 (9:16), 832x1248 (2:3), 1248x832 (3:2), 1512x648 (21:9), 2048x2048 (1:1), Custom\nwidth: INT | Custom width for image. Value is working only if `size_preset` is set to `Custom` | default=1024 | min=512 | max=2048 | step=64\nheight: INT | Custom height for image. Value is working only if `size_preset` is set to `Custom` | default=1024 | min=512 | max=2048 | step=64\n\n[optional inputs]\nseed: INT | Seed to use for generation | default=0 | min=0 | max=2147483647 | step=1\nguidance_scale: FLOAT | Higher value makes the image follow the prompt more closely | default=2.5 | min=1.0 | max=10.0 | step=0.01\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the image | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "ByteDanceImageReferenceNode": "ByteDance Reference Images to Video\nGenerate video using prompt and reference images.\n\n[required inputs]\nmodel: COMBO | Model name | default=seedance-1-0-lite-i2v-250428 | options=seedance-1-0-lite-i2v-250428\nprompt: STRING | The text prompt used to generate the video.\nimages: IMAGE | One to four images.\nresolution: COMBO | The resolution of the output video. | options=480p, 720p\naspect_ratio: COMBO | The aspect ratio of the output video. | options=adaptive, 16:9, 4:3, 1:1, 3:4, 9:16, 21:9\nduration: INT | The duration of the output video in seconds. | default=5 | min=3 | max=12 | step=1\n\n[optional inputs]\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the video. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "ByteDanceImageToVideoNode": "ByteDance Image to Video\nGenerate video using ByteDance models via api based on image and prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=seedance-1-0-pro-250528 | options=seedance-1-0-pro-250528, seedance-1-0-lite-i2v-250428\nprompt: STRING | The text prompt used to generate the video.\nimage: IMAGE | First frame to be used for the video.\nresolution: COMBO | The resolution of the output video. | options=480p, 720p, 1080p\naspect_ratio: COMBO | The aspect ratio of the output video. | options=adaptive, 16:9, 4:3, 1:1, 3:4, 9:16, 21:9\nduration: INT | The duration of the output video in seconds. | default=5 | min=3 | max=12 | step=1\n\n[optional inputs]\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\ncamera_fixed: BOOLEAN | Specifies whether to fix the camera. The platform appends an instruction to fix the camera to your prompt, but does not guarantee the actual effect. | default=False\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the video. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "ByteDanceSeedreamNode": "ByteDance Seedream 4\nUnified text-to-image generation and precise single-sentence editing at up to 4K resolution.\n\n[required inputs]\nmodel: COMBO | Model name | options=seedream-4-0-250828\nprompt: STRING | Text prompt for creating or editing an image. | default=\nsize_preset: COMBO | Pick a recommended size. Select Custom to use the width and height below. | options=2048x2048 (1:1), 2304x1728 (4:3), 1728x2304 (3:4), 2560x1440 (16:9), 1440x2560 (9:16), 2496x1664 (3:2), 1664x2496 (2:3), 3024x1296 (21:9), 4096x4096 (1:1), Custom\n\n[optional inputs]\nimage: IMAGE | Input image(s) for image-to-image generation. List of 1-10 images for single or multi-reference generation.\nwidth: INT | Custom width for image. Value is working only if `size_preset` is set to `Custom` | default=2048 | min=1024 | max=4096 | step=64\nheight: INT | Custom height for image. Value is working only if `size_preset` is set to `Custom` | default=2048 | min=1024 | max=4096 | step=64\nsequential_image_generation: COMBO | Group image generation mode. 'disabled' generates a single image. 'auto' lets the model decide whether to generate multiple related images (e.g., story scenes, character variations). | options=disabled, auto\nmax_images: INT | Maximum number of images to generate when sequential_image_generation='auto'. Total images (input + generated) cannot exceed 15. | default=1 | min=1 | max=15 | step=1\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the image. | default=True\nfail_on_partial: BOOLEAN | If enabled, abort execution if any requested images are missing or return an error. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "ByteDanceTextToVideoNode": "ByteDance Text to Video\nGenerate video using ByteDance models via api based on prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=seedance-1-0-pro-250528 | options=seedance-1-0-pro-250528, seedance-1-0-lite-t2v-250428\nprompt: STRING | The text prompt used to generate the video.\nresolution: COMBO | The resolution of the output video. | options=480p, 720p, 1080p\naspect_ratio: COMBO | The aspect ratio of the output video. | options=16:9, 4:3, 1:1, 3:4, 9:16, 21:9\nduration: INT | The duration of the output video in seconds. | default=5 | min=3 | max=12 | step=1\n\n[optional inputs]\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\ncamera_fixed: BOOLEAN | Specifies whether to fix the camera. The platform appends an instruction to fix the camera to your prompt, but does not guarantee the actual effect. | default=False\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the video. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "CFGGuider": "[required inputs]\nmodel: MODEL\npositive: CONDITIONING\nnegative: CONDITIONING\ncfg: FLOAT | default=8.0 | min=0.0 | max=100.0 | step=0.1 | round=0.01\n\n[outputs]\nGUIDER: GUIDER",
  "CFGNorm": "[required inputs]\nmodel: MODEL\nstrength: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\npatched_model: MODEL",
  "CFGZeroStar": "[required inputs]\nmodel: MODEL\n\n[outputs]\npatched_model: MODEL",
  "CLIPAttentionMultiply": "[required inputs]\nclip: CLIP\nq: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nk: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nv: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nout: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nCLIP: CLIP",
  "CLIPLoader": "Load CLIP\n[Recipes]\n\nstable_diffusion: clip-l\nstable_cascade: clip-g\nsd3: t5 xxl/ clip-g / clip-l\nstable_audio: t5 base\nmochi: t5 xxl\ncosmos: old t5 xxl\nlumina2: gemma 2 2B\nwan: umt5 xxl\n hidream: llama-3.1 (Recommend) or t5\nomnigen2: qwen vl 2.5 3B\n\n[required inputs]\nclip_name: COMBO\ntype: COMBO | options=stable_diffusion, stable_cascade, sd3, stable_audio, mochi, ltxv, pixart, cosmos, lumina2, wan, hidream, chroma, ace, omnigen2, qwen_image, hunyuan_image, flux2\n\n[optional inputs]\ndevice: COMBO | options=default, cpu\n\n[outputs]\nCLIP: CLIP",
  "CLIPMergeAdd": "[required inputs]\nclip1: CLIP\nclip2: CLIP\n\n[outputs]\nCLIP: CLIP",
  "CLIPMergeSimple": "[required inputs]\nclip1: CLIP\nclip2: CLIP\nratio: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nCLIP: CLIP",
  "CLIPMergeSubtract": "[required inputs]\nclip1: CLIP\nclip2: CLIP\nmultiplier: FLOAT | default=1.0 | min=-10.0 | max=10.0 | step=0.01\n\n[outputs]\nCLIP: CLIP",
  "CLIPSave": "[required inputs]\nclip: CLIP\nfilename_prefix: STRING | default=clip/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "CLIPSetLastLayer": "CLIP Set Last Layer\n\n[required inputs]\nclip: CLIP\nstop_at_clip_layer: INT | default=-1 | min=-24 | max=-1 | step=1\n\n[outputs]\nCLIP: CLIP",
  "CLIPTextEncode": "CLIP Text Encode (Prompt)\nEncodes a text prompt using a CLIP model into an embedding that can be used to guide the diffusion model towards generating specific images.\n\n[required inputs]\ntext: STRING | The text to be encoded.\nclip: CLIP | The CLIP model used for encoding the text.\n\n[outputs]\nCONDITIONING: CONDITIONING | A conditioning containing the embedded text used to guide the diffusion model.",
  "CLIPTextEncodeControlnet": "[required inputs]\nclip: CLIP\nconditioning: CONDITIONING\ntext: STRING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPTextEncodeFlux": "[required inputs]\nclip: CLIP\nclip_l: STRING\nt5xxl: STRING\nguidance: FLOAT | default=3.5 | min=0.0 | max=100.0 | step=0.1\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPTextEncodeHiDream": "[required inputs]\nclip: CLIP\nclip_l: STRING\nclip_g: STRING\nt5xxl: STRING\nllama: STRING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPTextEncodeHunyuanDiT": "[required inputs]\nclip: CLIP\nbert: STRING\nmt5xl: STRING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPTextEncodeLumina2": "CLIP Text Encode for Lumina2\nEncodes a system prompt and a user prompt using a CLIP model into an embedding that can be used to guide the diffusion model towards generating specific images.\n\n[required inputs]\nsystem_prompt: COMBO | Lumina2 provide two types of system prompts:Superior: You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts. Alignment: You are an assistant designed to generate high-quality images with the highest degree of image-text alignment based on textual prompts. | options=superior, alignment\nuser_prompt: STRING | The text to be encoded.\nclip: CLIP | The CLIP model used for encoding the text.\n\n[outputs]\nCONDITIONING: CONDITIONING | A conditioning containing the embedded text used to guide the diffusion model.",
  "CLIPTextEncodePixArtAlpha": "Encodes text and sets the resolution conditioning for PixArt Alpha. Does not apply to PixArt Sigma.\n\n[required inputs]\nwidth: INT | default=1024 | min=0 | max=16384\nheight: INT | default=1024 | min=0 | max=16384\ntext: STRING\nclip: CLIP\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPTextEncodeSD3": "[required inputs]\nclip: CLIP\nclip_l: STRING\nclip_g: STRING\nt5xxl: STRING\nempty_padding: COMBO | options=none, empty_prompt\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPTextEncodeSDXL": "[required inputs]\nclip: CLIP\nwidth: INT | default=1024 | min=0 | max=16384\nheight: INT | default=1024 | min=0 | max=16384\ncrop_w: INT | default=0 | min=0 | max=16384\ncrop_h: INT | default=0 | min=0 | max=16384\ntarget_width: INT | default=1024 | min=0 | max=16384\ntarget_height: INT | default=1024 | min=0 | max=16384\ntext_g: STRING\ntext_l: STRING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPTextEncodeSDXLRefiner": "[required inputs]\nascore: FLOAT | default=6.0 | min=0.0 | max=1000.0 | step=0.01\nwidth: INT | default=1024 | min=0 | max=16384\nheight: INT | default=1024 | min=0 | max=16384\ntext: STRING\nclip: CLIP\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "CLIPVisionEncode": "CLIP Vision Encode\n\n[required inputs]\nclip_vision: CLIP_VISION\nimage: IMAGE\ncrop: COMBO | options=center, none\n\n[outputs]\nCLIP_VISION_OUTPUT: CLIP_VISION_OUTPUT",
  "CLIPVisionLoader": "Load CLIP Vision\n\n[required inputs]\nclip_name: COMBO\n\n[outputs]\nCLIP_VISION: CLIP_VISION",
  "Canny": "[required inputs]\nimage: IMAGE\nlow_threshold: FLOAT | default=0.4 | min=0.01 | max=0.99 | step=0.01\nhigh_threshold: FLOAT | default=0.8 | min=0.01 | max=0.99 | step=0.01\n\n[outputs]\nIMAGE: IMAGE",
  "CaseConverter": "Case Converter\n\n[required inputs]\nstring: STRING\nmode: COMBO | options=UPPERCASE, lowercase, Capitalize, Title Case\n\n[outputs]\nSTRING: STRING",
  "CenterCropImages": "Center Crop Images\n\n[required inputs]\nimages: IMAGE | Image to process.\nwidth: INT | Crop width. | default=512 | min=1 | max=8192\nheight: INT | Crop height. | default=512 | min=1 | max=8192\n\n[outputs]\nimages: IMAGE | Processed images",
  "CheckpointLoader": "Load Checkpoint With Config (DEPRECATED)\n\n[required inputs]\nconfig_name: COMBO | options=anything_v3.yaml, v1-inference.yaml, v1-inference_clip_skip_2.yaml, v1-inference_clip_skip_2_fp16.yaml, v1-inference_fp16.yaml, v1-inpainting-inference.yaml, v2-inference-v.yaml, v2-inference-v_fp32.yaml, v2-inference.yaml, v2-inference_fp32.yaml, v2-inpainting-inference.yaml\nckpt_name: COMBO\n\n[outputs]\nMODEL: MODEL\nCLIP: CLIP\nVAE: VAE",
  "CheckpointLoaderSimple": "Load Checkpoint\nLoads a diffusion model checkpoint, diffusion models are used to denoise latents.\n\n[required inputs]\nckpt_name: COMBO | The name of the checkpoint (model) to load.\n\n[outputs]\nMODEL: MODEL | The model used for denoising latents.\nCLIP: CLIP | The CLIP model used for encoding text prompts.\nVAE: VAE | The VAE model used for encoding and decoding images to and from latent space.",
  "CheckpointSave": "Save Checkpoint\n\n[required inputs]\nmodel: MODEL\nclip: CLIP\nvae: VAE\nfilename_prefix: STRING | default=checkpoints/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "ChromaRadianceOptions": "Allows setting advanced options for the Chroma Radiance model.\n\n[required inputs]\nmodel: MODEL\npreserve_wrapper: BOOLEAN | When enabled, will delegate to an existing model function wrapper if it exists. Generally should be left enabled. | default=True\nstart_sigma: FLOAT | First sigma that these options will be in effect. | default=1.0 | min=0.0 | max=1.0\nend_sigma: FLOAT | Last sigma that these options will be in effect. | default=0.0 | min=0.0 | max=1.0\nnerf_tile_size: INT | Allows overriding the default NeRF tile size. -1 means use the default (32). 0 means use non-tiling mode (may require a lot of VRAM). | default=-1 | min=-1\n\n[outputs]\nMODEL: MODEL",
  "ClipLoader": "",
  "ClipMergeSimple": "",
  "ClipSave": "",
  "ClipSetLastLayer": "",
  "ClipTextEncode": "",
  "ClipTextEncodeFlux": "",
  "ClipTextEncodeHunyuanDit": "",
  "ClipTextEncodeSdxl": "",
  "ClipTextEncodeSdxlRefiner": "",
  "ClipVisionEncode": "",
  "ClipVisionLoader": "",
  "CombineHooks2": "Combine Hooks [2]\n\n[optional inputs]\nhooks_A: HOOKS\nhooks_B: HOOKS\n\n[outputs]\nHOOKS: HOOKS",
  "CombineHooks4": "Combine Hooks [4]\n\n[optional inputs]\nhooks_A: HOOKS\nhooks_B: HOOKS\nhooks_C: HOOKS\nhooks_D: HOOKS\n\n[outputs]\nHOOKS: HOOKS",
  "CombineHooks8": "Combine Hooks [8]\n\n[optional inputs]\nhooks_A: HOOKS\nhooks_B: HOOKS\nhooks_C: HOOKS\nhooks_D: HOOKS\nhooks_E: HOOKS\nhooks_F: HOOKS\nhooks_G: HOOKS\nhooks_H: HOOKS\n\n[outputs]\nHOOKS: HOOKS",
  "ConditioningAverage": "[required inputs]\nconditioning_to: CONDITIONING\nconditioning_from: CONDITIONING\nconditioning_to_strength: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningCombine": "Conditioning (Combine)\n\n[required inputs]\nconditioning_1: CONDITIONING\nconditioning_2: CONDITIONING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningConcat": "Conditioning (Concat)\n\n[required inputs]\nconditioning_to: CONDITIONING\nconditioning_from: CONDITIONING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetArea": "Conditioning (Set Area)\n\n[required inputs]\nconditioning: CONDITIONING\nwidth: INT | default=64 | min=64 | max=16384 | step=8\nheight: INT | default=64 | min=64 | max=16384 | step=8\nx: INT | default=0 | min=0 | max=16384 | step=8\ny: INT | default=0 | min=0 | max=16384 | step=8\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetAreaPercentage": "Conditioning (Set Area with Percentage)\n\n[required inputs]\nconditioning: CONDITIONING\nwidth: FLOAT | default=1.0 | min=0 | max=1.0 | step=0.01\nheight: FLOAT | default=1.0 | min=0 | max=1.0 | step=0.01\nx: FLOAT | default=0 | min=0 | max=1.0 | step=0.01\ny: FLOAT | default=0 | min=0 | max=1.0 | step=0.01\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetAreaPercentageVideo": "[required inputs]\nconditioning: CONDITIONING\nwidth: FLOAT | default=1.0 | min=0 | max=1.0 | step=0.01\nheight: FLOAT | default=1.0 | min=0 | max=1.0 | step=0.01\ntemporal: FLOAT | default=1.0 | min=0 | max=1.0 | step=0.01\nx: FLOAT | default=0 | min=0 | max=1.0 | step=0.01\ny: FLOAT | default=0 | min=0 | max=1.0 | step=0.01\nz: FLOAT | default=0 | min=0 | max=1.0 | step=0.01\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetAreaStrength": "[required inputs]\nconditioning: CONDITIONING\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetDefaultCombine": "Cond Set Default Combine\n\n[required inputs]\ncond: CONDITIONING\ncond_DEFAULT: CONDITIONING\n\n[optional inputs]\nhooks: HOOKS\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetMask": "Conditioning (Set Mask)\n\n[required inputs]\nconditioning: CONDITIONING\nmask: MASK\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nset_cond_area: COMBO | options=default, mask bounds\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetProperties": "Cond Set Props\n\n[required inputs]\ncond_NEW: CONDITIONING\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nset_cond_area: COMBO | options=default, mask bounds\n\n[optional inputs]\nmask: MASK\nhooks: HOOKS\ntimesteps: TIMESTEPS_RANGE\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetPropertiesAndCombine": "Cond Set Props Combine\n\n[required inputs]\ncond: CONDITIONING\ncond_NEW: CONDITIONING\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nset_cond_area: COMBO | options=default, mask bounds\n\n[optional inputs]\nmask: MASK\nhooks: HOOKS\ntimesteps: TIMESTEPS_RANGE\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSetTimestepRange": "[required inputs]\nconditioning: CONDITIONING\nstart: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ConditioningSettimestepRange": "",
  "ConditioningStableAudio": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nseconds_start: FLOAT | default=0.0 | min=0.0 | max=1000.0 | step=0.1\nseconds_total: FLOAT | default=47.0 | min=0.0 | max=1000.0 | step=0.1\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "ConditioningTimestepsRange": "Timesteps Range\n\n[required inputs]\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\n\n[outputs]\nTIMESTEPS_RANGE: TIMESTEPS_RANGE\nBEFORE_RANGE: TIMESTEPS_RANGE\nAFTER_RANGE: TIMESTEPS_RANGE",
  "ConditioningZeroOut": "[required inputs]\nconditioning: CONDITIONING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ContextWindowsManual": "Context Windows (Manual)\nManually set context windows.\n\n[required inputs]\nmodel: MODEL | The model to apply context windows to during sampling.\ncontext_length: INT | The length of the context window. | default=16 | min=1\ncontext_overlap: INT | The overlap of the context window. | default=4 | min=0\ncontext_schedule: COMBO | The stride of the context window. | options=standard_static, standard_uniform, looped_uniform, batched\ncontext_stride: INT | The stride of the context window; only applicable to uniform schedules. | default=1 | min=1\nclosed_loop: BOOLEAN | Whether to close the context window loop; only applicable to looped schedules. | default=False\nfuse_method: COMBO | The method to use to fuse the context windows. | default=pyramid | options=pyramid, relative, flat, overlap-linear\ndim: INT | The dimension to apply the context windows to. | default=0 | min=0 | max=5\n\n[outputs]\nMODEL: MODEL | The model with context windows applied during sampling.",
  "ControlNetApply": "Apply ControlNet (OLD)\n\n[required inputs]\nconditioning: CONDITIONING\ncontrol_net: CONTROL_NET\nimage: IMAGE\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "ControlNetApplyAdvanced": "Apply ControlNet\n\n[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\ncontrol_net: CONTROL_NET\nimage: IMAGE\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\n\n[optional inputs]\nvae: VAE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "ControlNetApplySD3": "Apply Controlnet with VAE\n\n[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\ncontrol_net: CONTROL_NET\nvae: VAE\nimage: IMAGE\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "ControlNetInpaintingAliMamaApply": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\ncontrol_net: CONTROL_NET\nvae: VAE\nimage: IMAGE\nmask: MASK\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "ControlNetLoader": "Load ControlNet Model\n\n[required inputs]\ncontrol_net_name: COMBO\n\n[outputs]\nCONTROL_NET: CONTROL_NET",
  "ControlnetApply": "",
  "ControlnetApplyAdvanced": "",
  "ControlnetLoader": "",
  "CosmosImageToVideoLatent": "[required inputs]\nvae: VAE\nwidth: INT | default=1280 | min=16 | max=16384 | step=16\nheight: INT | default=704 | min=16 | max=16384 | step=16\nlength: INT | default=121 | min=1 | max=16384 | step=8\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nstart_image: IMAGE\nend_image: IMAGE\n\n[outputs]\nLATENT: LATENT",
  "CosmosPredict2ImageToVideoLatent": "[required inputs]\nvae: VAE\nwidth: INT | default=848 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=93 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nstart_image: IMAGE\nend_image: IMAGE\n\n[outputs]\nLATENT: LATENT",
  "CreateHookKeyframe": "Create Hook Keyframe\n\n[required inputs]\nstrength_mult: FLOAT | default=1.0 | min=-20.0 | max=20.0 | step=0.01\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\n\n[optional inputs]\nprev_hook_kf: HOOK_KEYFRAMES\n\n[outputs]\nHOOK_KF: HOOK_KEYFRAMES",
  "CreateHookKeyframesFromFloats": "Create Hook Keyframes From Floats\n\n[required inputs]\nfloats_strength: FLOATS | default=-1 | min=-1 | step=0.001\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\nprint_keyframes: BOOLEAN | default=False\n\n[optional inputs]\nprev_hook_kf: HOOK_KEYFRAMES\n\n[outputs]\nHOOK_KF: HOOK_KEYFRAMES",
  "CreateHookKeyframesInterpolated": "Create Hook Keyframes Interp.\n\n[required inputs]\nstrength_start: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.001\nstrength_end: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.001\ninterpolation: COMBO | options=linear, ease_in, ease_out, ease_in_out\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\nkeyframes_count: INT | default=5 | min=2 | max=100 | step=1\nprint_keyframes: BOOLEAN | default=False\n\n[optional inputs]\nprev_hook_kf: HOOK_KEYFRAMES\n\n[outputs]\nHOOK_KF: HOOK_KEYFRAMES",
  "CreateHookLora": "Create Hook LoRA\n\n[required inputs]\nlora_name: COMBO\nstrength_model: FLOAT | default=1.0 | min=-20.0 | max=20.0 | step=0.01\nstrength_clip: FLOAT | default=1.0 | min=-20.0 | max=20.0 | step=0.01\n\n[optional inputs]\nprev_hooks: HOOKS\n\n[outputs]\nHOOKS: HOOKS",
  "CreateHookLoraModelOnly": "Create Hook LoRA (MO)\n\n[required inputs]\nlora_name: COMBO\nstrength_model: FLOAT | default=1.0 | min=-20.0 | max=20.0 | step=0.01\n\n[optional inputs]\nprev_hooks: HOOKS\n\n[outputs]\nHOOKS: HOOKS",
  "CreateHookModelAsLora": "Create Hook Model as LoRA\n\n[required inputs]\nckpt_name: COMBO\nstrength_model: FLOAT | default=1.0 | min=-20.0 | max=20.0 | step=0.01\nstrength_clip: FLOAT | default=1.0 | min=-20.0 | max=20.0 | step=0.01\n\n[optional inputs]\nprev_hooks: HOOKS\n\n[outputs]\nHOOKS: HOOKS",
  "CreateHookModelAsLoraModelOnly": "Create Hook Model as LoRA (MO)\n\n[required inputs]\nckpt_name: COMBO\nstrength_model: FLOAT | default=1.0 | min=-20.0 | max=20.0 | step=0.01\n\n[optional inputs]\nprev_hooks: HOOKS\n\n[outputs]\nHOOKS: HOOKS",
  "CreateVideo": "Create Video\nCreate a video from images.\n\n[required inputs]\nimages: IMAGE | The images to create a video from.\nfps: FLOAT | default=30.0 | min=1.0 | max=120.0 | step=1.0\n\n[optional inputs]\naudio: AUDIO | The audio to add to the video.\n\n[outputs]\nVIDEO: VIDEO",
  "CropMask": "[required inputs]\nmask: MASK\nx: INT | default=0 | min=0 | max=16384 | step=1\ny: INT | default=0 | min=0 | max=16384 | step=1\nwidth: INT | default=512 | min=1 | max=16384 | step=1\nheight: INT | default=512 | min=1 | max=16384 | step=1\n\n[outputs]\nMASK: MASK",
  "DeprecatedCheckpointLoader": "",
  "DeprecatedDiffusersLoader": "",
  "DiffControlNetLoader": "Load ControlNet Model (diff)\n\n[required inputs]\nmodel: MODEL\ncontrol_net_name: COMBO\n\n[outputs]\nCONTROL_NET: CONTROL_NET",
  "DiffControlnetLoader": "",
  "DifferentialDiffusion": "Differential Diffusion\n\n[required inputs]\nmodel: MODEL\n\n[optional inputs]\nstrength: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "DiffusersLoader": "[required inputs]\nmodel_path: COMBO\n\n[outputs]\nMODEL: MODEL\nCLIP: CLIP\nVAE: VAE",
  "DisableNoise": "[outputs]\nNOISE: NOISE",
  "DualCFGGuider": "[required inputs]\nmodel: MODEL\ncond1: CONDITIONING\ncond2: CONDITIONING\nnegative: CONDITIONING\ncfg_conds: FLOAT | default=8.0 | min=0.0 | max=100.0 | step=0.1 | round=0.01\ncfg_cond2_negative: FLOAT | default=8.0 | min=0.0 | max=100.0 | step=0.1 | round=0.01\nstyle: COMBO | options=regular, nested\n\n[outputs]\nGUIDER: GUIDER",
  "DualCLIPLoader": "[Recipes]\n\nsdxl: clip-l, clip-g\nsd3: clip-l, clip-g / clip-l, t5 / clip-g, t5\nflux: clip-l, t5\nhidream: at least one of t5 or llama, recommended t5 and llama\nhunyuan_image: qwen2.5vl 7b and byt5 small\n\n[required inputs]\nclip_name1: COMBO\nclip_name2: COMBO\ntype: COMBO | options=sdxl, sd3, flux, hunyuan_video, hidream, hunyuan_image, hunyuan_video_15\n\n[optional inputs]\ndevice: COMBO | options=default, cpu\n\n[outputs]\nCLIP: CLIP",
  "DualClipLoader": "",
  "EasyCache": "Native EasyCache implementation.\n\n[required inputs]\nmodel: MODEL | The model to add EasyCache to.\nreuse_threshold: FLOAT | The threshold for reusing cached steps. | default=0.2 | min=0.0 | max=3.0 | step=0.01\nstart_percent: FLOAT | The relative sampling step to begin use of EasyCache. | default=0.15 | min=0.0 | max=1.0 | step=0.01\nend_percent: FLOAT | The relative sampling step to end use of EasyCache. | default=0.95 | min=0.0 | max=1.0 | step=0.01\nverbose: BOOLEAN | Whether to log verbose information. | default=False\n\n[outputs]\nMODEL: MODEL | The model with EasyCache.",
  "EmptyAceStepLatentAudio": "[required inputs]\nseconds: FLOAT | default=120.0 | min=1.0 | max=1000.0 | step=0.1\nbatch_size: INT | The number of latent images in the batch. | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyAudio": "Empty Audio\n\n[required inputs]\nduration: FLOAT | Duration of the empty audio clip in seconds | default=60.0 | min=0.0 | max=18446744073709551615 | step=0.01\nsample_rate: INT | Sample rate of the empty audio clip. | default=44100\nchannels: INT | Number of audio channels (1 for mono, 2 for stereo). | default=2 | min=1 | max=2\n\n[outputs]\nAUDIO: AUDIO",
  "EmptyChromaRadianceLatentImage": "[required inputs]\nwidth: INT | default=1024 | min=16 | max=16384 | step=16\nheight: INT | default=1024 | min=16 | max=16384 | step=16\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyCosmosLatentVideo": "[required inputs]\nwidth: INT | default=1280 | min=16 | max=16384 | step=16\nheight: INT | default=704 | min=16 | max=16384 | step=16\nlength: INT | default=121 | min=1 | max=16384 | step=8\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyFlux2LatentImage": "Empty Flux 2 Latent\n\n[required inputs]\nwidth: INT | default=1024 | min=16 | max=16384 | step=16\nheight: INT | default=1024 | min=16 | max=16384 | step=16\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyHunyuanImageLatent": "[required inputs]\nwidth: INT | default=2048 | min=64 | max=16384 | step=32\nheight: INT | default=2048 | min=64 | max=16384 | step=32\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyHunyuanLatentVideo": "Empty HunyuanVideo 1.0 Latent\n\n[required inputs]\nwidth: INT | default=848 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=25 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyHunyuanVideo15Latent": "Empty HunyuanVideo 1.5 Latent\n\n[required inputs]\nwidth: INT | default=848 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=25 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyImage": "[required inputs]\nwidth: INT | default=512 | min=1 | max=16384 | step=1\nheight: INT | default=512 | min=1 | max=16384 | step=1\nbatch_size: INT | default=1 | min=1 | max=4096\ncolor: INT | default=0 | min=0 | max=16777215 | step=1\n\n[outputs]\nIMAGE: IMAGE",
  "EmptyLTXVLatentVideo": "[required inputs]\nwidth: INT | default=768 | min=64 | max=16384 | step=32\nheight: INT | default=512 | min=64 | max=16384 | step=32\nlength: INT | default=97 | min=1 | max=16384 | step=8\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyLatentAudio": "Empty Latent Audio\n\n[required inputs]\nseconds: FLOAT | default=47.6 | min=1.0 | max=1000.0 | step=0.1\nbatch_size: INT | The number of latent images in the batch. | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyLatentHunyuan3Dv2": "[required inputs]\nresolution: INT | default=3072 | min=1 | max=8192\nbatch_size: INT | The number of latent images in the batch. | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptyLatentImage": "Empty Latent Image\nCreate a new batch of empty latent images to be denoised via sampling.\n\n[required inputs]\nwidth: INT | The width of the latent images in pixels. | default=512 | min=16 | max=16384 | step=8\nheight: INT | The height of the latent images in pixels. | default=512 | min=16 | max=16384 | step=8\nbatch_size: INT | The number of latent images in the batch. | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT | The empty latent image batch.",
  "EmptyMochiLatentVideo": "[required inputs]\nwidth: INT | default=848 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=25 | min=7 | max=16384 | step=6\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "EmptySD3LatentImage": "[required inputs]\nwidth: INT | default=1024 | min=16 | max=16384 | step=16\nheight: INT | default=1024 | min=16 | max=16384 | step=16\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "Epsilon Scaling": "[required inputs]\nmodel: MODEL\nscaling_factor: FLOAT | default=1.005 | min=0.5 | max=1.5 | step=0.001\n\n[outputs]\nMODEL: MODEL",
  "ExponentialScheduler": "[required inputs]\nsteps: INT | default=20 | min=1 | max=10000\nsigma_max: FLOAT | default=14.614642 | min=0.0 | max=5000.0 | step=0.01 | round=False\nsigma_min: FLOAT | default=0.0291675 | min=0.0 | max=5000.0 | step=0.01 | round=False\n\n[outputs]\nSIGMAS: SIGMAS",
  "ExtendIntermediateSigmas": "[required inputs]\nsigmas: SIGMAS\nsteps: INT | default=2 | min=1 | max=100\nstart_at_sigma: FLOAT | default=-1.0 | min=-1.0 | max=20000.0 | step=0.01 | round=False\nend_at_sigma: FLOAT | default=12.0 | min=0.0 | max=20000.0 | step=0.01 | round=False\nspacing: COMBO | options=linear, cosine, sine\n\n[outputs]\nSIGMAS: SIGMAS",
  "FeatherMask": "[required inputs]\nmask: MASK\nleft: INT | default=0 | min=0 | max=16384 | step=1\ntop: INT | default=0 | min=0 | max=16384 | step=1\nright: INT | default=0 | min=0 | max=16384 | step=1\nbottom: INT | default=0 | min=0 | max=16384 | step=1\n\n[outputs]\nMASK: MASK",
  "FlipSigmas": "[required inputs]\nsigmas: SIGMAS\n\n[outputs]\nSIGMAS: SIGMAS",
  "Flux2ProImageNode": "Flux.2 [pro] Image\nGenerates images synchronously based on prompt and resolution.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation or edit | default=\nwidth: INT | default=1024 | min=256 | max=2048 | step=32\nheight: INT | default=768 | min=256 | max=2048 | step=32\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615\nprompt_upsampling: BOOLEAN | Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result). | default=False\n\n[optional inputs]\nimages: IMAGE | Up to 4 images to be used as references.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "Flux2Scheduler": "[required inputs]\nsteps: INT | default=20 | min=1 | max=4096\nwidth: INT | default=1024 | min=16 | max=16384 | step=1\nheight: INT | default=1024 | min=16 | max=16384 | step=1\n\n[outputs]\nSIGMAS: SIGMAS",
  "FluxDisableGuidance": "This node completely disables the guidance embed on Flux and Flux like models\n\n[required inputs]\nconditioning: CONDITIONING\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "FluxGuidance": "[required inputs]\nconditioning: CONDITIONING\nguidance: FLOAT | default=3.5 | min=0.0 | max=100.0 | step=0.1\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "FluxKontextImageScale": "This node resizes the image to one that is more optimal for flux kontext.\n\n[required inputs]\nimage: IMAGE\n\n[outputs]\nIMAGE: IMAGE",
  "FluxKontextMaxImageNode": "Flux.1 Kontext [max] Image\nEdits images using Flux.1 Kontext [max] via api based on prompt and aspect ratio.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation - specify what and how to edit. | default=\naspect_ratio: STRING | Aspect ratio of image; must be between 1:4 and 4:1. | default=16:9\nguidance: FLOAT | Guidance strength for the image generation process | default=3.0 | min=0.1 | max=99.0 | step=0.1\nsteps: INT | Number of steps for the image generation process | default=50 | min=1 | max=150\nseed: INT | The random seed used for creating the noise. | default=1234 | min=0 | max=18446744073709551615\nprompt_upsampling: BOOLEAN | Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result). | default=False\n\n[optional inputs]\ninput_image: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "FluxKontextMultiReferenceLatentMethod": "[required inputs]\nconditioning: CONDITIONING\nreference_latents_method: COMBO | options=offset, index, uxo/uno\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "FluxKontextProImageNode": "Flux.1 Kontext [pro] Image\nEdits images using Flux.1 Kontext [pro] via api based on prompt and aspect ratio.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation - specify what and how to edit. | default=\naspect_ratio: STRING | Aspect ratio of image; must be between 1:4 and 4:1. | default=16:9\nguidance: FLOAT | Guidance strength for the image generation process | default=3.0 | min=0.1 | max=99.0 | step=0.1\nsteps: INT | Number of steps for the image generation process | default=50 | min=1 | max=150\nseed: INT | The random seed used for creating the noise. | default=1234 | min=0 | max=18446744073709551615\nprompt_upsampling: BOOLEAN | Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result). | default=False\n\n[optional inputs]\ninput_image: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "FluxProExpandNode": "Flux.1 Expand Image\nOutpaints image based on prompt.\n\n[required inputs]\nimage: IMAGE\nprompt: STRING | Prompt for the image generation | default=\nprompt_upsampling: BOOLEAN | Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result). | default=False\ntop: INT | Number of pixels to expand at the top of the image | default=0 | min=0 | max=2048\nbottom: INT | Number of pixels to expand at the bottom of the image | default=0 | min=0 | max=2048\nleft: INT | Number of pixels to expand at the left of the image | default=0 | min=0 | max=2048\nright: INT | Number of pixels to expand at the right of the image | default=0 | min=0 | max=2048\nguidance: FLOAT | Guidance strength for the image generation process | default=60 | min=1.5 | max=100\nsteps: INT | Number of steps for the image generation process | default=50 | min=15 | max=50\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "FluxProFillNode": "Flux.1 Fill Image\nInpaints image based on mask and prompt.\n\n[required inputs]\nimage: IMAGE\nmask: MASK\nprompt: STRING | Prompt for the image generation | default=\nprompt_upsampling: BOOLEAN | Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result). | default=False\nguidance: FLOAT | Guidance strength for the image generation process | default=60 | min=1.5 | max=100\nsteps: INT | Number of steps for the image generation process | default=50 | min=15 | max=50\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "FluxProUltraImageNode": "Flux 1.1 [pro] Ultra Image\nGenerates images using Flux Pro 1.1 Ultra via api based on prompt and resolution.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation | default=\nprompt_upsampling: BOOLEAN | Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result). | default=False\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615\naspect_ratio: STRING | Aspect ratio of image; must be between 1:4 and 4:1. | default=16:9\nraw: BOOLEAN | When True, generate less processed, more natural-looking images. | default=False\n\n[optional inputs]\nimage_prompt: IMAGE\nimage_prompt_strength: FLOAT | Blend between the prompt and the image prompt. | default=0.1 | min=0.0 | max=1.0 | step=0.01\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "FreSca": "Applies frequency-dependent scaling to the guidance\n\n[required inputs]\nmodel: MODEL\nscale_low: FLOAT | Scaling factor for low-frequency components | default=1.0 | min=0 | max=10 | step=0.01\nscale_high: FLOAT | Scaling factor for high-frequency components | default=1.25 | min=0 | max=10 | step=0.01\nfreq_cutoff: INT | Number of frequency indices around center to consider as low-frequency | default=20 | min=1 | max=10000 | step=1\n\n[outputs]\nMODEL: MODEL",
  "FreeU": "[required inputs]\nmodel: MODEL\nb1: FLOAT | default=1.1 | min=0.0 | max=10.0 | step=0.01\nb2: FLOAT | default=1.2 | min=0.0 | max=10.0 | step=0.01\ns1: FLOAT | default=0.9 | min=0.0 | max=10.0 | step=0.01\ns2: FLOAT | default=0.2 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "FreeU_V2": "[required inputs]\nmodel: MODEL\nb1: FLOAT | default=1.3 | min=0.0 | max=10.0 | step=0.01\nb2: FLOAT | default=1.4 | min=0.0 | max=10.0 | step=0.01\ns1: FLOAT | default=0.9 | min=0.0 | max=10.0 | step=0.01\ns2: FLOAT | default=0.2 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "GITSScheduler": "[required inputs]\ncoeff: FLOAT | default=1.2 | min=0.8 | max=1.5 | step=0.05\nsteps: INT | default=10 | min=2 | max=1000\ndenoise: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nSIGMAS: SIGMAS",
  "GLIGENLoader": "[required inputs]\ngligen_name: COMBO\n\n[outputs]\nGLIGEN: GLIGEN",
  "GLIGENTextBoxApply": "[required inputs]\nconditioning_to: CONDITIONING\nclip: CLIP\ngligen_textbox_model: GLIGEN\ntext: STRING\nwidth: INT | default=64 | min=8 | max=16384 | step=8\nheight: INT | default=64 | min=8 | max=16384 | step=8\nx: INT | default=0 | min=0 | max=16384 | step=8\ny: INT | default=0 | min=0 | max=16384 | step=8\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "GeminiImage2Node": "Nano Banana Pro (Google Gemini Image)\nGenerate or edit images synchronously via Google Vertex API.\n\n[required inputs]\nprompt: STRING | Text prompt describing the image to generate or the edits to apply. Include any constraints, styles, or details the model should follow. | default=\nmodel: COMBO | options=gemini-3-pro-image-preview\nseed: INT | When the seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests. Deterministic output isn't guaranteed. Also, changing the model or parameter settings, such as the temperature, can cause variations in the response even when you use the same seed value. By default, a random seed value is used. | default=42 | min=0 | max=18446744073709551615\naspect_ratio: COMBO | If set to 'auto', matches your input image's aspect ratio; if no image is provided, a 16:9 square is usually generated. | default=auto | options=auto, 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9, 21:9\nresolution: COMBO | Target output resolution. For 2K/4K the native Gemini upscaler is used. | options=1K, 2K, 4K\nresponse_modalities: COMBO | Choose 'IMAGE' for image-only output, or 'IMAGE+TEXT' to return both the generated image and a text response. | options=IMAGE+TEXT, IMAGE\n\n[optional inputs]\nimages: IMAGE | Optional reference image(s). To include multiple images, use the Batch Images node (up to 14).\nfiles: GEMINI_INPUT_FILES | Optional file(s) to use as context for the model. Accepts inputs from the Gemini Generate Content Input Files node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE\nSTRING: STRING",
  "GeminiImageNode": "Nano Banana (Google Gemini Image)\nEdit images synchronously via Google API.\n\n[required inputs]\nprompt: STRING | Text prompt for generation | default=\nmodel: COMBO | The Gemini model to use for generating responses. | default=gemini-2.5-flash-image | options=gemini-2.5-flash-image-preview, gemini-2.5-flash-image\nseed: INT | When seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests. Deterministic output isn't guaranteed. Also, changing the model or parameter settings, such as the temperature, can cause variations in the response even when you use the same seed value. By default, a random seed value is used. | default=42 | min=0 | max=18446744073709551615\n\n[optional inputs]\nimages: IMAGE | Optional image(s) to use as context for the model. To include multiple images, you can use the Batch Images node.\nfiles: GEMINI_INPUT_FILES | Optional file(s) to use as context for the model. Accepts inputs from the Gemini Generate Content Input Files node.\naspect_ratio: COMBO | Defaults to matching the output image size to that of your input image, or otherwise generates 1:1 squares. | default=auto | options=auto, 1:1, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9, 21:9\nresponse_modalities: COMBO | Choose 'IMAGE' for image-only output, or 'IMAGE+TEXT' to return both the generated image and a text response. | options=IMAGE+TEXT, IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE\nSTRING: STRING",
  "GeminiInputFiles": "Gemini Input Files\nLoads and prepares input files to include as inputs for Gemini LLM nodes. The files will be read by the Gemini model when generating a response. The contents of the text file count toward the token limit. \ud83d\udec8 TIP: Can be chained together with other Gemini Input File nodes.\n\n[required inputs]\nfile: COMBO | Input files to include as context for the model. Only accepts text (.txt) and PDF (.pdf) files for now.\n\n[optional inputs]\nGEMINI_INPUT_FILES: GEMINI_INPUT_FILES | An optional additional file(s) to batch together with the file loaded from this node. Allows chaining of input files so that a single message can include multiple input files.\n\n[outputs]\nGEMINI_INPUT_FILES: GEMINI_INPUT_FILES",
  "GeminiNode": "Google Gemini\nGenerate text responses with Google's Gemini AI model. You can provide multiple types of inputs (text, images, audio, video) as context for generating more relevant and meaningful responses.\n\n[required inputs]\nprompt: STRING | Text inputs to the model, used to generate a response. You can include detailed instructions, questions, or context for the model. | default=\nmodel: COMBO | The Gemini model to use for generating responses. | default=gemini-2.5-pro | options=gemini-2.5-pro-preview-05-06, gemini-2.5-flash-preview-04-17, gemini-2.5-pro, gemini-2.5-flash, gemini-3-pro-preview\nseed: INT | When seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests. Deterministic output isn't guaranteed. Also, changing the model or parameter settings, such as the temperature, can cause variations in the response even when you use the same seed value. By default, a random seed value is used. | default=42 | min=0 | max=18446744073709551615\n\n[optional inputs]\nimages: IMAGE | Optional image(s) to use as context for the model. To include multiple images, you can use the Batch Images node.\naudio: AUDIO | Optional audio to use as context for the model.\nvideo: VIDEO | Optional video to use as context for the model.\nfiles: GEMINI_INPUT_FILES | Optional file(s) to use as context for the model. Accepts inputs from the Gemini Generate Content Input Files node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nSTRING: STRING",
  "GetImageSize": "Get Image Size\nReturns width and height of the image, and passes it through unchanged.\n\n[required inputs]\nimage: IMAGE\n\n[hidden inputs]\nunique_id: UNIQUE_ID\n\n[outputs]\nwidth: INT\nheight: INT\nbatch_size: INT",
  "GetVideoComponents": "Get Video Components\nExtracts all components from a video: frames, audio, and framerate.\n\n[required inputs]\nvideo: VIDEO | The video to extract components from.\n\n[outputs]\nimages: IMAGE\naudio: AUDIO\nfps: FLOAT",
  "GligenLoader": "",
  "GligenTextBoxApply": "",
  "GrowMask": "[required inputs]\nmask: MASK\nexpand: INT | default=0 | min=-16384 | max=16384 | step=1\ntapered_corners: BOOLEAN | default=True\n\n[outputs]\nMASK: MASK",
  "Hunyuan3Dv2Conditioning": "[required inputs]\nclip_vision_output: CLIP_VISION_OUTPUT\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "Hunyuan3Dv2ConditioningMultiView": "[optional inputs]\nfront: CLIP_VISION_OUTPUT\nleft: CLIP_VISION_OUTPUT\nback: CLIP_VISION_OUTPUT\nright: CLIP_VISION_OUTPUT\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "HunyuanImageToVideo": "[required inputs]\npositive: CONDITIONING\nvae: VAE\nwidth: INT | default=848 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=53 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\nguidance_type: COMBO | options=v1 (concat), v2 (replace), custom\n\n[optional inputs]\nstart_image: IMAGE\n\n[outputs]\npositive: CONDITIONING\nlatent: LATENT",
  "HunyuanRefinerLatent": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT\nnoise_augmentation: FLOAT | default=0.1 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "HunyuanVideo15ImageToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=848 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=33 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nstart_image: IMAGE\nclip_vision_output: CLIP_VISION_OUTPUT\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "HunyuanVideo15LatentUpscaleWithModel": "Hunyuan Video 15 Latent Upscale With Model\n\n[required inputs]\nmodel: LATENT_UPSCALE_MODEL\nsamples: LATENT\nupscale_method: COMBO | default=bilinear | options=nearest-exact, bilinear, area, bicubic, bislerp\nwidth: INT | default=1280 | min=0 | max=16384 | step=8\nheight: INT | default=720 | min=0 | max=16384 | step=8\ncrop: COMBO | options=disabled, center\n\n[outputs]\nLATENT: LATENT",
  "HunyuanVideo15SuperResolution": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT\nnoise_augmentation: FLOAT | default=0.7 | min=0.0 | max=1.0 | step=0.01\n\n[optional inputs]\nvae: VAE\nstart_image: IMAGE\nclip_vision_output: CLIP_VISION_OUTPUT\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "HyperTile": "[required inputs]\nmodel: MODEL\ntile_size: INT | default=256 | min=1 | max=2048\nswap_size: INT | default=2 | min=1 | max=128\nmax_depth: INT | default=0 | min=0 | max=10\nscale_depth: BOOLEAN | default=False\n\n[outputs]\nMODEL: MODEL",
  "HypernetworkLoader": "[required inputs]\nmodel: MODEL\nhypernetwork_name: COMBO\nstrength: FLOAT | default=1.0 | min=-10.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "IdeogramV1": "Ideogram V1\nGenerates images using the Ideogram V1 model.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation | default=\nturbo: BOOLEAN | Whether to use turbo mode (faster generation, potentially lower quality) | default=False\n\n[optional inputs]\naspect_ratio: COMBO | The aspect ratio for image generation. | default=1:1 | options=1:1, 4:3, 3:4, 16:9, 9:16, 2:1, 1:2, 3:2, 2:3, 4:5, 5:4\nmagic_prompt_option: COMBO | Determine if MagicPrompt should be used in generation | default=AUTO | options=AUTO, ON, OFF\nseed: INT | default=0 | min=0 | max=2147483647 | step=1\nnegative_prompt: STRING | Description of what to exclude from the image | default=\nnum_images: INT | default=1 | min=1 | max=8 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "IdeogramV2": "Ideogram V2\nGenerates images using the Ideogram V2 model.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation | default=\nturbo: BOOLEAN | Whether to use turbo mode (faster generation, potentially lower quality) | default=False\n\n[optional inputs]\naspect_ratio: COMBO | The aspect ratio for image generation. Ignored if resolution is not set to AUTO. | default=1:1 | options=1:1, 4:3, 3:4, 16:9, 9:16, 2:1, 1:2, 3:2, 2:3, 4:5, 5:4\nresolution: COMBO | The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting. | default=Auto | options=Auto, 512 x 1536, 576 x 1408, 576 x 1472, 576 x 1536, 640 x 1024, 640 x 1344, 640 x 1408, 640 x 1472, 640 x 1536, 704 x 1152, 704 x 1216, 704 x 1280, 704 x 1344, 704 x 1408, 704 x 1472, 720 x 1280, 736 x 1312, 768 x 1024, 768 x 1088, 768 x 1152, 768 x 1216, 768 x 1232, 768 x 1280, 768 x 1344, 832 x 960, 832 x 1024, 832 x 1088, 832 x 1152, 832 x 1216, 832 x 1248, 864 x 1152, 896 x 960, 896 x 1024, 896 x 1088, 896 x 1120, 896 x 1152, 960 x 832, 960 x 896, 960 x 1024, 960 x 1088, 1024 x 640, 1024 x 768, 1024 x 832, 1024 x 896, 1024 x 960, 1024 x 1024, 1088 x 768, 1088 x 832, 1088 x 896...\nmagic_prompt_option: COMBO | Determine if MagicPrompt should be used in generation | default=AUTO | options=AUTO, ON, OFF\nseed: INT | default=0 | min=0 | max=2147483647 | step=1\nstyle_type: COMBO | Style type for generation (V2 only) | default=NONE | options=AUTO, GENERAL, REALISTIC, DESIGN, RENDER_3D, ANIME\nnegative_prompt: STRING | Description of what to exclude from the image | default=\nnum_images: INT | default=1 | min=1 | max=8 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "IdeogramV3": "Ideogram V3\nGenerates images using the Ideogram V3 model. Supports both regular image generation from text prompts and image editing with mask.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation or editing | default=\n\n[optional inputs]\nimage: IMAGE | Optional reference image for image editing.\nmask: MASK | Optional mask for inpainting (white areas will be replaced)\naspect_ratio: COMBO | The aspect ratio for image generation. Ignored if resolution is not set to Auto. | default=1:1 | options=1:3, 3:1, 1:2, 2:1, 9:16, 16:9, 10:16, 16:10, 2:3, 3:2, 3:4, 4:3, 4:5, 5:4, 1:1\nresolution: COMBO | The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting. | default=Auto | options=Auto, 512x1536, 576x1408, 576x1472, 576x1536, 640x1344, 640x1408, 640x1472, 640x1536, 704x1152, 704x1216, 704x1280, 704x1344, 704x1408, 704x1472, 736x1312, 768x1088, 768x1216, 768x1280, 768x1344, 800x1280, 832x960, 832x1024, 832x1088, 832x1152, 832x1216, 832x1248, 864x1152, 896x960, 896x1024, 896x1088, 896x1120, 896x1152, 960x832, 960x896, 960x1024, 960x1088, 1024x832, 1024x896, 1024x960, 1024x1024, 1088x768, 1088x832, 1088x896, 1088x960, 1120x896, 1152x704, 1152x832, 1152x864, 1152x896...\nmagic_prompt_option: COMBO | Determine if MagicPrompt should be used in generation | default=AUTO | options=AUTO, ON, OFF\nseed: INT | default=0 | min=0 | max=2147483647 | step=1\nnum_images: INT | default=1 | min=1 | max=8 | step=1\nrendering_speed: COMBO | Controls the trade-off between generation speed and quality | default=DEFAULT | options=DEFAULT, TURBO, QUALITY\ncharacter_image: IMAGE | Image to use as character reference.\ncharacter_mask: MASK | Optional mask for character reference image.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "ImageAddNoise": "[required inputs]\nimage: IMAGE\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615\nstrength: FLOAT | default=0.5 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nIMAGE: IMAGE",
  "ImageBatch": "Batch Images\n\n[required inputs]\nimage1: IMAGE\nimage2: IMAGE\n\n[outputs]\nIMAGE: IMAGE",
  "ImageBlend": "[required inputs]\nimage1: IMAGE\nimage2: IMAGE\nblend_factor: FLOAT | default=0.5 | min=0.0 | max=1.0 | step=0.01\nblend_mode: COMBO | options=normal, multiply, screen, overlay, soft_light, difference\n\n[outputs]\nIMAGE: IMAGE",
  "ImageBlur": "[required inputs]\nimage: IMAGE\nblur_radius: INT | default=1 | min=1 | max=31 | step=1\nsigma: FLOAT | default=1.0 | min=0.1 | max=10.0 | step=0.1\n\n[outputs]\nIMAGE: IMAGE",
  "ImageColorToMask": "[required inputs]\nimage: IMAGE\ncolor: INT | default=0 | min=0 | max=16777215 | step=1\n\n[outputs]\nMASK: MASK",
  "ImageCompositeMasked": "[required inputs]\ndestination: IMAGE\nsource: IMAGE\nx: INT | default=0 | min=0 | max=16384 | step=1\ny: INT | default=0 | min=0 | max=16384 | step=1\nresize_source: BOOLEAN | default=False\n\n[optional inputs]\nmask: MASK\n\n[outputs]\nIMAGE: IMAGE",
  "ImageCrop": "Image Crop\n\n[required inputs]\nimage: IMAGE\nwidth: INT | default=512 | min=1 | max=16384 | step=1\nheight: INT | default=512 | min=1 | max=16384 | step=1\nx: INT | default=0 | min=0 | max=16384 | step=1\ny: INT | default=0 | min=0 | max=16384 | step=1\n\n[outputs]\nIMAGE: IMAGE",
  "ImageDeduplication": "Image Deduplication\n\n[required inputs]\nimages: IMAGE | List of images to process.\nsimilarity_threshold: FLOAT | Similarity threshold (0-1). Higher means more similar. Images above this threshold are considered duplicates. | default=0.95 | min=0.0 | max=1.0\n\n[outputs]\nimages: IMAGE | Processed images",
  "ImageFlip": "[required inputs]\nimage: IMAGE\nflip_method: COMBO | options=x-axis: vertically, y-axis: horizontally\n\n[outputs]\nIMAGE: IMAGE",
  "ImageFromBatch": "[required inputs]\nimage: IMAGE\nbatch_index: INT | default=0 | min=0 | max=4095\nlength: INT | default=1 | min=1 | max=4096\n\n[outputs]\nIMAGE: IMAGE",
  "ImageGrid": "Image Grid\n\n[required inputs]\nimages: IMAGE | List of images to process.\ncolumns: INT | Number of columns in the grid. | default=4 | min=1 | max=20\ncell_width: INT | Width of each cell in the grid. | default=256 | min=32 | max=2048\ncell_height: INT | Height of each cell in the grid. | default=256 | min=32 | max=2048\npadding: INT | Padding between images. | default=4 | min=0 | max=50\n\n[outputs]\nimages: IMAGE | Processed images",
  "ImageInvert": "Invert Image\n\n[required inputs]\nimage: IMAGE\n\n[outputs]\nIMAGE: IMAGE",
  "ImageOnlyCheckpointLoader": "Image Only Checkpoint Loader (img2vid model)\n\n[required inputs]\nckpt_name: COMBO\n\n[outputs]\nMODEL: MODEL\nCLIP_VISION: CLIP_VISION\nVAE: VAE",
  "ImageOnlyCheckpointSave": "[required inputs]\nmodel: MODEL\nclip_vision: CLIP_VISION\nvae: VAE\nfilename_prefix: STRING | default=checkpoints/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "ImagePadForOutpaint": "Pad Image for Outpainting\n\n[required inputs]\nimage: IMAGE\nleft: INT | default=0 | min=0 | max=16384 | step=8\ntop: INT | default=0 | min=0 | max=16384 | step=8\nright: INT | default=0 | min=0 | max=16384 | step=8\nbottom: INT | default=0 | min=0 | max=16384 | step=8\nfeathering: INT | default=40 | min=0 | max=16384 | step=1\n\n[outputs]\nIMAGE: IMAGE\nMASK: MASK",
  "ImageQuantize": "[required inputs]\nimage: IMAGE\ncolors: INT | default=256 | min=1 | max=256 | step=1\ndither: COMBO | options=none, floyd-steinberg, bayer-2, bayer-4, bayer-8, bayer-16\n\n[outputs]\nIMAGE: IMAGE",
  "ImageRGBToYUV": "[required inputs]\nimage: IMAGE\n\n[outputs]\nY: IMAGE\nU: IMAGE\nV: IMAGE",
  "ImageRotate": "[required inputs]\nimage: IMAGE\nrotation: COMBO | options=none, 90 degrees, 180 degrees, 270 degrees\n\n[outputs]\nIMAGE: IMAGE",
  "ImageScale": "Upscale Image\n\n[required inputs]\nimage: IMAGE\nupscale_method: COMBO | options=nearest-exact, bilinear, area, bicubic, lanczos\nwidth: INT | default=512 | min=0 | max=16384 | step=1\nheight: INT | default=512 | min=0 | max=16384 | step=1\ncrop: COMBO | options=disabled, center\n\n[outputs]\nIMAGE: IMAGE",
  "ImageScaleBy": "Upscale Image By\n\n[required inputs]\nimage: IMAGE\nupscale_method: COMBO | options=nearest-exact, bilinear, area, bicubic, lanczos\nscale_by: FLOAT | default=1.0 | min=0.01 | max=8.0 | step=0.01\n\n[outputs]\nIMAGE: IMAGE",
  "ImageScaleToMaxDimension": "[required inputs]\nimage: IMAGE\nupscale_method: COMBO | options=area, lanczos, bilinear, nearest-exact, bilinear, bicubic\nlargest_size: INT | default=512 | min=0 | max=16384 | step=1\n\n[outputs]\nIMAGE: IMAGE",
  "ImageScaleToTotalPixels": "[required inputs]\nimage: IMAGE\nupscale_method: COMBO | options=nearest-exact, bilinear, area, bicubic, lanczos\nmegapixels: FLOAT | default=1.0 | min=0.01 | max=16.0 | step=0.01\n\n[outputs]\nIMAGE: IMAGE",
  "ImageSharpen": "[required inputs]\nimage: IMAGE\nsharpen_radius: INT | default=1 | min=1 | max=31 | step=1\nsigma: FLOAT | default=1.0 | min=0.1 | max=10.0 | step=0.01\nalpha: FLOAT | default=1.0 | min=0.0 | max=5.0 | step=0.01\n\n[outputs]\nIMAGE: IMAGE",
  "ImageStitch": "Image Stitch\nStitches image2 to image1 in the specified direction.\nIf image2 is not provided, returns image1 unchanged.\nOptional spacing can be added between images.\n\n[required inputs]\nimage1: IMAGE\ndirection: COMBO | default=right | options=right, down, left, up\nmatch_image_size: BOOLEAN | default=True\nspacing_width: INT | default=0 | min=0 | max=1024 | step=2\nspacing_color: COMBO | default=white | options=white, black, red, green, blue\n\n[optional inputs]\nimage2: IMAGE\n\n[outputs]\nIMAGE: IMAGE",
  "ImageToMask": "Convert Image to Mask\n\n[required inputs]\nimage: IMAGE\nchannel: COMBO | options=red, green, blue, alpha\n\n[outputs]\nMASK: MASK",
  "ImageUpscaleWithModel": "Upscale Image (using Model)\n\n[required inputs]\nupscale_model: UPSCALE_MODEL\nimage: IMAGE\n\n[outputs]\nIMAGE: IMAGE",
  "ImageYUVToRGB": "[required inputs]\nY: IMAGE\nU: IMAGE\nV: IMAGE\n\n[outputs]\nIMAGE: IMAGE",
  "InpaintModelConditioning": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\npixels: IMAGE\nmask: MASK\nnoise_mask: BOOLEAN | Add a noise mask to the latent so sampling will only happen within the mask. Might improve results or completely break things depending on the model. | default=True\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "InstructPixToPixConditioning": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\npixels: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "InvertMask": "[required inputs]\nmask: MASK\n\n[outputs]\nMASK: MASK",
  "JoinImageWithAlpha": "Join Image with Alpha\n\n[required inputs]\nimage: IMAGE\nalpha: MASK\n\n[outputs]\nIMAGE: IMAGE",
  "KSampler": "Uses the provided model, positive and negative conditioning to denoise the latent image.\n\n[required inputs]\nmodel: MODEL | The model used for denoising the input latent.\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615\nsteps: INT | The number of steps used in the denoising process. | default=20 | min=1 | max=10000\ncfg: FLOAT | The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality. | default=8.0 | min=0.0 | max=100.0 | step=0.1 | round=0.01\nsampler_name: COMBO | The algorithm used when sampling, this can affect the quality, speed, and style of the generated output. | options=euler, euler_cfg_pp, euler_ancestral, euler_ancestral_cfg_pp, heun, heunpp2, dpm_2, dpm_2_ancestral, lms, dpm_fast, dpm_adaptive, dpmpp_2s_ancestral, dpmpp_2s_ancestral_cfg_pp, dpmpp_sde, dpmpp_sde_gpu, dpmpp_2m, dpmpp_2m_cfg_pp, dpmpp_2m_sde, dpmpp_2m_sde_gpu, dpmpp_2m_sde_heun, dpmpp_2m_sde_heun_gpu, dpmpp_3m_sde, dpmpp_3m_sde_gpu, ddpm, lcm, ipndm, ipndm_v, deis, res_multistep, res_multistep_cfg_pp, res_multistep_ancestral, res_multistep_ancestral_cfg_pp, gradient_estimation, gradient_estimation_cfg_pp, er_sde, seeds_2, seeds_3, sa_solver, sa_solver_pece, ddim, uni_pc, uni_pc_bh2\nscheduler: COMBO | The scheduler controls how noise is gradually removed to form the image. | options=simple, sgm_uniform, karras, exponential, ddim_uniform, beta, normal, linear_quadratic, kl_optimal\npositive: CONDITIONING | The conditioning describing the attributes you want to include in the image.\nnegative: CONDITIONING | The conditioning describing the attributes you want to exclude from the image.\nlatent_image: LATENT | The latent image to denoise.\ndenoise: FLOAT | The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling. | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nLATENT: LATENT | The denoised latent.",
  "KSamplerAdvanced": "KSampler (Advanced)\n\n[required inputs]\nmodel: MODEL\nadd_noise: COMBO | options=enable, disable\nnoise_seed: INT | default=0 | min=0 | max=18446744073709551615\nsteps: INT | default=20 | min=1 | max=10000\ncfg: FLOAT | default=8.0 | min=0.0 | max=100.0 | step=0.1 | round=0.01\nsampler_name: COMBO | options=euler, euler_cfg_pp, euler_ancestral, euler_ancestral_cfg_pp, heun, heunpp2, dpm_2, dpm_2_ancestral, lms, dpm_fast, dpm_adaptive, dpmpp_2s_ancestral, dpmpp_2s_ancestral_cfg_pp, dpmpp_sde, dpmpp_sde_gpu, dpmpp_2m, dpmpp_2m_cfg_pp, dpmpp_2m_sde, dpmpp_2m_sde_gpu, dpmpp_2m_sde_heun, dpmpp_2m_sde_heun_gpu, dpmpp_3m_sde, dpmpp_3m_sde_gpu, ddpm, lcm, ipndm, ipndm_v, deis, res_multistep, res_multistep_cfg_pp, res_multistep_ancestral, res_multistep_ancestral_cfg_pp, gradient_estimation, gradient_estimation_cfg_pp, er_sde, seeds_2, seeds_3, sa_solver, sa_solver_pece, ddim, uni_pc, uni_pc_bh2\nscheduler: COMBO | options=simple, sgm_uniform, karras, exponential, ddim_uniform, beta, normal, linear_quadratic, kl_optimal\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent_image: LATENT\nstart_at_step: INT | default=0 | min=0 | max=10000\nend_at_step: INT | default=10000 | min=0 | max=10000\nreturn_with_leftover_noise: COMBO | options=disable, enable\n\n[outputs]\nLATENT: LATENT",
  "KSamplerSelect": "[required inputs]\nsampler_name: COMBO | options=euler, euler_cfg_pp, euler_ancestral, euler_ancestral_cfg_pp, heun, heunpp2, dpm_2, dpm_2_ancestral, lms, dpm_fast, dpm_adaptive, dpmpp_2s_ancestral, dpmpp_2s_ancestral_cfg_pp, dpmpp_sde, dpmpp_sde_gpu, dpmpp_2m, dpmpp_2m_cfg_pp, dpmpp_2m_sde, dpmpp_2m_sde_gpu, dpmpp_2m_sde_heun, dpmpp_2m_sde_heun_gpu, dpmpp_3m_sde, dpmpp_3m_sde_gpu, ddpm, lcm, ipndm, ipndm_v, deis, res_multistep, res_multistep_cfg_pp, res_multistep_ancestral, res_multistep_ancestral_cfg_pp, gradient_estimation, gradient_estimation_cfg_pp, er_sde, seeds_2, seeds_3, sa_solver, sa_solver_pece, ddim, uni_pc, uni_pc_bh2\n\n[outputs]\nSAMPLER: SAMPLER",
  "KarrasScheduler": "[required inputs]\nsteps: INT | default=20 | min=1 | max=10000\nsigma_max: FLOAT | default=14.614642 | min=0.0 | max=5000.0 | step=0.01 | round=False\nsigma_min: FLOAT | default=0.0291675 | min=0.0 | max=5000.0 | step=0.01 | round=False\nrho: FLOAT | default=7.0 | min=0.0 | max=100.0 | step=0.01 | round=False\n\n[outputs]\nSIGMAS: SIGMAS",
  "KlingCameraControlI2VNode": "Kling Image to Video (Camera Control)\nTransform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image.\n\n[required inputs]\nstart_frame: IMAGE | Reference Image - URL or Base64 encoded string, cannot exceed 10MB, resolution not less than 300*300px, aspect ratio between 1:2.5 ~ 2.5:1. Base64 should not include data:image prefix.\nprompt: STRING | Positive text prompt\nnegative_prompt: STRING | Negative text prompt\ncfg_scale: FLOAT | default=0.75 | min=0.0 | max=1.0\naspect_ratio: COMBO | default=16:9 | options=16:9, 9:16, 1:1\ncamera_control: CAMERA_CONTROL | Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingCameraControlT2VNode": "Kling Text to Video (Camera Control)\nTransform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text.\n\n[required inputs]\nprompt: STRING | Positive text prompt\nnegative_prompt: STRING | Negative text prompt\ncfg_scale: FLOAT | default=0.75 | min=0.0 | max=1.0\naspect_ratio: COMBO | default=16:9 | options=16:9, 9:16, 1:1\ncamera_control: CAMERA_CONTROL | Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingCameraControls": "Kling Camera Controls\nAllows specifying configuration options for Kling Camera Controls and motion control effects.\n\n[required inputs]\ncamera_control_type: COMBO | options=simple, down_back, forward_up, right_turn_forward, left_turn_forward\nhorizontal_movement: FLOAT | Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right | default=0.0 | min=-10.0 | max=10.0 | step=0.25\nvertical_movement: FLOAT | Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward. | default=0.0 | min=-10.0 | max=10.0 | step=0.25\npan: FLOAT | Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation. | default=0.5 | min=-10.0 | max=10.0 | step=0.25\ntilt: FLOAT | Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation. | default=0.0 | min=-10.0 | max=10.0 | step=0.25\nroll: FLOAT | Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise. | default=0.0 | min=-10.0 | max=10.0 | step=0.25\nzoom: FLOAT | Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view. | default=0.0 | min=-10.0 | max=10.0 | step=0.25\n\n[outputs]\ncamera_control: CAMERA_CONTROL",
  "KlingDualCharacterVideoEffectNode": "Kling Dual Character Video Effects\nAchieve different special effects when generating a video based on the effect_scene. First image will be positioned on left side, second on right side of the composite.\n\n[required inputs]\nimage_left: IMAGE | Left side image\nimage_right: IMAGE | Right side image\neffect_scene: COMBO | options=hug, kiss, heart_gesture\nmodel_name: COMBO | default=kling-v1 | options=kling-v1, kling-v1-5, kling-v1-6\nmode: COMBO | default=std | options=std, pro\nduration: COMBO | options=5, 10\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nduration: STRING",
  "KlingImage2VideoNode": "Kling Image to Video\nKling Image to Video Node\n\n[required inputs]\nstart_frame: IMAGE | The reference image used to generate the video.\nprompt: STRING | Positive text prompt\nnegative_prompt: STRING | Negative text prompt\nmodel_name: COMBO | default=kling-v2-master | options=kling-v1, kling-v1-5, kling-v1-6, kling-v2-master, kling-v2-1, kling-v2-1-master, kling-v2-5-turbo\ncfg_scale: FLOAT | default=0.8 | min=0.0 | max=1.0\nmode: COMBO | default=std | options=std, pro\naspect_ratio: COMBO | default=16:9 | options=16:9, 9:16, 1:1\nduration: COMBO | default=5 | options=5, 10\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingImageGenerationNode": "Kling Image Generation\nKling Image Generation Node. Generate an image from a text prompt with an optional reference image.\n\n[required inputs]\nprompt: STRING | Positive text prompt\nnegative_prompt: STRING | Negative text prompt\nimage_type: COMBO | options=subject, face\nimage_fidelity: FLOAT | Reference intensity for user-uploaded images | default=0.5 | min=0.0 | max=1.0 | step=0.01\nhuman_fidelity: FLOAT | Subject reference similarity | default=0.45 | min=0.0 | max=1.0 | step=0.01\nmodel_name: COMBO | default=kling-v1 | options=kling-v1, kling-v1-5, kling-v2\naspect_ratio: COMBO | default=16:9 | options=16:9, 9:16, 1:1, 4:3, 3:4, 3:2, 2:3, 21:9\nn: INT | Number of generated images | default=1 | min=1 | max=9\n\n[optional inputs]\nimage: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "KlingLipSyncAudioToVideoNode": "Kling Lip Sync Video with Audio\nKling Lip Sync Audio to Video Node. Syncs mouth movements in a video file to the audio content of an audio file. When using, ensure that the audio contains clearly distinguishable vocals and that the video contains a distinct face. The audio file should not be larger than 5MB. The video file should not be larger than 100MB, should have height/width between 720px and 1920px, and should be between 2s and 10s in length.\n\n[required inputs]\nvideo: VIDEO\naudio: AUDIO\nvoice_language: COMBO | default=en | options=zh, en\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingLipSyncTextToVideoNode": "Kling Lip Sync Video with Text\nKling Lip Sync Text to Video Node. Syncs mouth movements in a video file to a text prompt. The video file should not be larger than 100MB, should have height/width between 720px and 1920px, and should be between 2s and 10s in length.\n\n[required inputs]\nvideo: VIDEO\ntext: STRING | Text Content for Lip-Sync Video Generation. Required when mode is text2video. Maximum length is 120 characters.\nvoice: COMBO | default=Melody | options=Melody, Sunny, Sage, Ace, Blossom, Peppy, Dove, Shine, Anchor, Lyric, Tender, Siren, Zippy, Bud, Sprite, Candy, Beacon, Rock, Titan, Grace, Helen, Lore, Crag, Prattle, Hearth, The Reader, Commercial Lady, \u9633\u5149\u5c11\u5e74, \u61c2\u4e8b\u5c0f\u5f1f, \u8fd0\u52a8\u5c11\u5e74, \u9752\u6625\u5c11\u5973, \u6e29\u67d4\u5c0f\u59b9, \u5143\u6c14\u5c11\u5973, \u9633\u5149\u7537\u751f, \u5e7d\u9ed8\u5c0f\u54e5, \u6587\u827a\u5c0f\u54e5, \u751c\u7f8e\u90bb\u5bb6, \u6e29\u67d4\u59d0\u59d0, \u804c\u573a\u5973\u9752, \u6d3b\u6cfc\u7537\u7ae5, \u4fcf\u76ae\u5973\u7ae5, \u7a33\u91cd\u8001\u7238, \u6e29\u67d4\u5988\u5988, \u4e25\u8083\u4e0a\u53f8, \u4f18\u96c5\u8d35\u5987, \u6148\u7965\u7237\u7237, \u5520\u53e8\u7237\u7237, \u5520\u53e8\u5976\u5976, \u548c\u853c\u5976\u5976, \u4e1c\u5317\u8001\u94c1...\nvoice_speed: FLOAT | Speech Rate. Valid range: 0.8~2.0, accurate to one decimal place. | default=1 | min=0.8 | max=2.0\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingSingleImageVideoEffectNode": "Kling Video Effects\nAchieve different special effects when generating a video based on the effect_scene.\n\n[required inputs]\nimage: IMAGE | Reference Image. URL or Base64 encoded string (without data:image prefix). File size cannot exceed 10MB, resolution not less than 300*300px, aspect ratio between 1:2.5 ~ 2.5:1\neffect_scene: COMBO | options=bloombloom, dizzydizzy, fuzzyfuzzy, squish, expansion\nmodel_name: COMBO | options=kling-v1-6\nduration: COMBO | options=5, 10\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingStartEndFrameNode": "Kling Start-End Frame to Video\nGenerate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last.\n\n[required inputs]\nstart_frame: IMAGE | Reference Image - URL or Base64 encoded string, cannot exceed 10MB, resolution not less than 300*300px, aspect ratio between 1:2.5 ~ 2.5:1. Base64 should not include data:image prefix.\nend_frame: IMAGE | Reference Image - End frame control. URL or Base64 encoded string, cannot exceed 10MB, resolution not less than 300*300px. Base64 should not include data:image prefix.\nprompt: STRING | Positive text prompt\nnegative_prompt: STRING | Negative text prompt\ncfg_scale: FLOAT | default=0.5 | min=0.0 | max=1.0\naspect_ratio: COMBO | options=16:9, 9:16, 1:1\nmode: COMBO | The configuration to use for the video generation following the format: mode / duration / model_name. | default=pro mode / 5s duration / kling-v2-5-turbo | options=standard mode / 5s duration / kling-v1, pro mode / 5s duration / kling-v1, pro mode / 5s duration / kling-v1-5, pro mode / 10s duration / kling-v1-5, pro mode / 5s duration / kling-v1-6, pro mode / 10s duration / kling-v1-6, pro mode / 5s duration / kling-v2-1, pro mode / 10s duration / kling-v2-1, pro mode / 5s duration / kling-v2-5-turbo, pro mode / 10s duration / kling-v2-5-turbo\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingTextToVideoNode": "Kling Text to Video\nKling Text to Video Node\n\n[required inputs]\nprompt: STRING | Positive text prompt\nnegative_prompt: STRING | Negative text prompt\ncfg_scale: FLOAT | default=1.0 | min=0.0 | max=1.0\naspect_ratio: COMBO | default=16:9 | options=16:9, 9:16, 1:1\nmode: COMBO | The configuration to use for the video generation following the format: mode / duration / model_name. | default=standard mode / 5s duration / kling-v1-6 | options=standard mode / 5s duration / kling-v1, standard mode / 10s duration / kling-v1, pro mode / 5s duration / kling-v1, pro mode / 10s duration / kling-v1, standard mode / 5s duration / kling-v1-6, standard mode / 10s duration / kling-v1-6, pro mode / 5s duration / kling-v2-master, pro mode / 10s duration / kling-v2-master, standard mode / 5s duration / kling-v2-master, standard mode / 10s duration / kling-v2-master, pro mode / 5s duration / kling-v2-1-master, pro mode / 10s duration / kling-v2-1-master, pro mode / 5s duration / kling-v2-5-turbo, pro mode / 10s duration / kling-v2-5-turbo\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingVideoExtendNode": "Kling Video Extend\nKling Video Extend Node. Extend videos made by other Kling nodes. The video_id is created by using other Kling Nodes.\n\n[required inputs]\nprompt: STRING | Positive text prompt for guiding the video extension\nnegative_prompt: STRING | Negative text prompt for elements to avoid in the extended video\ncfg_scale: FLOAT | default=0.5 | min=0.0 | max=1.0\nvideo_id: STRING | The ID of the video to be extended. Supports videos generated by text-to-video, image-to-video, and previous video extension operations. Cannot exceed 3 minutes total duration after extension.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO\nvideo_id: STRING\nduration: STRING",
  "KlingVirtualTryOnNode": "Kling Virtual Try On\nKling Virtual Try On Node. Input a human image and a cloth image to try on the cloth on the human. You can merge multiple clothing item pictures into one image with a white background.\n\n[required inputs]\nhuman_image: IMAGE\ncloth_image: IMAGE\nmodel_name: COMBO | default=kolors-virtual-try-on-v1 | options=kolors-virtual-try-on-v1, kolors-virtual-try-on-v1-5\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "LTXVAddGuide": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nlatent: LATENT\nimage: IMAGE | Image or video to condition the latent video on. Must be 8*n + 1 frames. If the video is not 8*n + 1 frames, it will be cropped to the nearest 8*n + 1 frames.\nframe_idx: INT | Frame index to start the conditioning at. For single-frame images or videos with 1-8 frames, any frame_idx value is acceptable. For videos with 9+ frames, frame_idx must be divisible by 8, otherwise it will be rounded down to the nearest multiple of 8. Negative values are counted from the end of the video. | default=0 | min=-9999 | max=9999\nstrength: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "LTXVConditioning": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nframe_rate: FLOAT | default=25.0 | min=0.0 | max=1000.0 | step=0.01\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "LTXVCropGuides": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "LTXVImgToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nimage: IMAGE\nwidth: INT | default=768 | min=64 | max=16384 | step=32\nheight: INT | default=512 | min=64 | max=16384 | step=32\nlength: INT | default=97 | min=9 | max=16384 | step=8\nbatch_size: INT | default=1 | min=1 | max=4096\nstrength: FLOAT | default=1.0 | min=0.0 | max=1.0\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "LTXVPreprocess": "[required inputs]\nimage: IMAGE\nimg_compression: INT | Amount of compression to apply on image. | default=35 | min=0 | max=100\n\n[outputs]\noutput_image: IMAGE",
  "LTXVScheduler": "[required inputs]\nsteps: INT | default=20 | min=1 | max=10000\nmax_shift: FLOAT | default=2.05 | min=0.0 | max=100.0 | step=0.01\nbase_shift: FLOAT | default=0.95 | min=0.0 | max=100.0 | step=0.01\nstretch: BOOLEAN | Stretch the sigmas to be in the range [terminal, 1]. | default=True\nterminal: FLOAT | The terminal value of the sigmas after stretching. | default=0.1 | min=0.0 | max=0.99 | step=0.01\n\n[optional inputs]\nlatent: LATENT\n\n[outputs]\nSIGMAS: SIGMAS",
  "LaplaceScheduler": "[required inputs]\nsteps: INT | default=20 | min=1 | max=10000\nsigma_max: FLOAT | default=14.614642 | min=0.0 | max=5000.0 | step=0.01 | round=False\nsigma_min: FLOAT | default=0.0291675 | min=0.0 | max=5000.0 | step=0.01 | round=False\nmu: FLOAT | default=0.0 | min=-10.0 | max=10.0 | step=0.1 | round=False\nbeta: FLOAT | default=0.5 | min=0.0 | max=10.0 | step=0.1 | round=False\n\n[outputs]\nSIGMAS: SIGMAS",
  "LatentAdd": "[required inputs]\nsamples1: LATENT\nsamples2: LATENT\n\n[outputs]\nLATENT: LATENT",
  "LatentApplyOperation": "[required inputs]\nsamples: LATENT\noperation: LATENT_OPERATION\n\n[outputs]\nLATENT: LATENT",
  "LatentApplyOperationCFG": "[required inputs]\nmodel: MODEL\noperation: LATENT_OPERATION\n\n[outputs]\nMODEL: MODEL",
  "LatentBatch": "[required inputs]\nsamples1: LATENT\nsamples2: LATENT\n\n[outputs]\nLATENT: LATENT",
  "LatentBatchSeedBehavior": "[required inputs]\nsamples: LATENT\nseed_behavior: COMBO | default=fixed | options=random, fixed\n\n[outputs]\nLATENT: LATENT",
  "LatentBlend": "Latent Blend\n\n[required inputs]\nsamples1: LATENT\nsamples2: LATENT\nblend_factor: FLOAT | default=0.5 | min=0 | max=1 | step=0.01\n\n[outputs]\nLATENT: LATENT",
  "LatentComposite": "Latent Composite\n\n[required inputs]\nsamples_to: LATENT\nsamples_from: LATENT\nx: INT | default=0 | min=0 | max=16384 | step=8\ny: INT | default=0 | min=0 | max=16384 | step=8\nfeather: INT | default=0 | min=0 | max=16384 | step=8\n\n[outputs]\nLATENT: LATENT",
  "LatentCompositeMasked": "[required inputs]\ndestination: LATENT\nsource: LATENT\nx: INT | default=0 | min=0 | max=16384 | step=8\ny: INT | default=0 | min=0 | max=16384 | step=8\nresize_source: BOOLEAN | default=False\n\n[optional inputs]\nmask: MASK\n\n[outputs]\nLATENT: LATENT",
  "LatentConcat": "[required inputs]\nsamples1: LATENT\nsamples2: LATENT\ndim: COMBO | options=x, -x, y, -y, t, -t\n\n[outputs]\nLATENT: LATENT",
  "LatentCrop": "Crop Latent\n\n[required inputs]\nsamples: LATENT\nwidth: INT | default=512 | min=64 | max=16384 | step=8\nheight: INT | default=512 | min=64 | max=16384 | step=8\nx: INT | default=0 | min=0 | max=16384 | step=8\ny: INT | default=0 | min=0 | max=16384 | step=8\n\n[outputs]\nLATENT: LATENT",
  "LatentCut": "[required inputs]\nsamples: LATENT\ndim: COMBO | options=x, y, t\nindex: INT | default=0 | min=-16384 | max=16384 | step=1\namount: INT | default=1 | min=1 | max=16384 | step=1\n\n[outputs]\nLATENT: LATENT",
  "LatentFlip": "Flip Latent\n\n[required inputs]\nsamples: LATENT\nflip_method: COMBO | options=x-axis: vertically, y-axis: horizontally\n\n[outputs]\nLATENT: LATENT",
  "LatentFromBatch": "Latent From Batch\n\n[required inputs]\nsamples: LATENT\nbatch_index: INT | default=0 | min=0 | max=63\nlength: INT | default=1 | min=1 | max=64\n\n[outputs]\nLATENT: LATENT",
  "LatentInterpolate": "[required inputs]\nsamples1: LATENT\nsamples2: LATENT\nratio: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nLATENT: LATENT",
  "LatentMultiply": "[required inputs]\nsamples: LATENT\nmultiplier: FLOAT | default=1.0 | min=-10.0 | max=10.0 | step=0.01\n\n[outputs]\nLATENT: LATENT",
  "LatentOperationSharpen": "[required inputs]\nsharpen_radius: INT | default=9 | min=1 | max=31 | step=1\nsigma: FLOAT | default=1.0 | min=0.1 | max=10.0 | step=0.1\nalpha: FLOAT | default=0.1 | min=0.0 | max=5.0 | step=0.01\n\n[outputs]\nLATENT_OPERATION: LATENT_OPERATION",
  "LatentOperationTonemapReinhard": "[required inputs]\nmultiplier: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\nLATENT_OPERATION: LATENT_OPERATION",
  "LatentRotate": "Rotate Latent\n\n[required inputs]\nsamples: LATENT\nrotation: COMBO | options=none, 90 degrees, 180 degrees, 270 degrees\n\n[outputs]\nLATENT: LATENT",
  "LatentSubtract": "[required inputs]\nsamples1: LATENT\nsamples2: LATENT\n\n[outputs]\nLATENT: LATENT",
  "LatentUpscale": "Upscale Latent\n\n[required inputs]\nsamples: LATENT\nupscale_method: COMBO | options=nearest-exact, bilinear, area, bicubic, bislerp\nwidth: INT | default=512 | min=0 | max=16384 | step=8\nheight: INT | default=512 | min=0 | max=16384 | step=8\ncrop: COMBO | options=disabled, center\n\n[outputs]\nLATENT: LATENT",
  "LatentUpscaleBy": "Upscale Latent By\n\n[required inputs]\nsamples: LATENT\nupscale_method: COMBO | options=nearest-exact, bilinear, area, bicubic, bislerp\nscale_by: FLOAT | default=1.5 | min=0.01 | max=8.0 | step=0.01\n\n[outputs]\nLATENT: LATENT",
  "LatentUpscaleModelLoader": "Load Latent Upscale Model\n\n[required inputs]\nmodel_name: COMBO\n\n[outputs]\nLATENT_UPSCALE_MODEL: LATENT_UPSCALE_MODEL",
  "LazyCache": "A homebrew version of EasyCache - even 'easier' version of EasyCache to implement. Overall works worse than EasyCache, but better in some rare cases AND universal compatibility with everything in ComfyUI.\n\n[required inputs]\nmodel: MODEL | The model to add LazyCache to.\nreuse_threshold: FLOAT | The threshold for reusing cached steps. | default=0.2 | min=0.0 | max=3.0 | step=0.01\nstart_percent: FLOAT | The relative sampling step to begin use of LazyCache. | default=0.15 | min=0.0 | max=1.0 | step=0.01\nend_percent: FLOAT | The relative sampling step to end use of LazyCache. | default=0.95 | min=0.0 | max=1.0 | step=0.01\nverbose: BOOLEAN | Whether to log verbose information. | default=False\n\n[outputs]\nMODEL: MODEL | The model with LazyCache.",
  "Load3D": "Load 3D & Animation\n\n[required inputs]\nmodel_file: COMBO\nimage: LOAD_3D\nwidth: INT | default=1024 | min=1 | max=4096 | step=1\nheight: INT | default=1024 | min=1 | max=4096 | step=1\n\n[outputs]\nimage: IMAGE\nmask: MASK\nmesh_path: STRING\nnormal: IMAGE\ncamera_info: LOAD3D_CAMERA\nrecording_video: VIDEO",
  "LoadAudio": "Load Audio\n\n[required inputs]\naudio: COMBO\n\n[outputs]\nAUDIO: AUDIO",
  "LoadImage": "Load Image\n\n[required inputs]\nimage: COMBO | options=example.png\n\n[outputs]\nIMAGE: IMAGE\nMASK: MASK",
  "LoadImageDataSetFromFolder": "Load Image Dataset from Folder\n\n[required inputs]\nfolder: COMBO | The folder to load images from. | options=3d\n\n[outputs]\nimages: IMAGE | List of loaded images",
  "LoadImageMask": "Load Image (as Mask)\n\n[required inputs]\nimage: COMBO | options=example.png\nchannel: COMBO | options=alpha, red, green, blue\n\n[outputs]\nMASK: MASK",
  "LoadImageOutput": "Load Image (from Outputs)\nLoad an image from the output folder. When the refresh button is clicked, the node will update the image list and automatically select the first image, allowing for easy iteration.\n\n[required inputs]\nimage: COMBO\n\n[outputs]\nIMAGE: IMAGE\nMASK: MASK",
  "LoadImageTextDataSetFromFolder": "Load Image and Text Dataset from Folder\n\n[required inputs]\nfolder: COMBO | The folder to load images from. | options=3d\n\n[outputs]\nimages: IMAGE | List of loaded images\ntexts: STRING | List of text captions",
  "LoadLatent": "[required inputs]\nlatent: COMBO\n\n[outputs]\nLATENT: LATENT",
  "LoadTrainingDataset": "Load Training Dataset\n\n[required inputs]\nfolder_name: STRING | Name of folder containing the saved dataset (inside output directory). | default=training_dataset\n\n[outputs]\nlatents: LATENT | List of latent dicts\nconditioning: CONDITIONING | List of conditioning lists",
  "LoadVideo": "Load Video\n\n[required inputs]\nfile: COMBO\n\n[outputs]\nVIDEO: VIDEO",
  "LoraLoader": "Load LoRA\nLoRAs are used to modify diffusion and CLIP models, altering the way in which latents are denoised such as applying styles. Multiple LoRA nodes can be linked together.\n\n[required inputs]\nmodel: MODEL | The diffusion model the LoRA will be applied to.\nclip: CLIP | The CLIP model the LoRA will be applied to.\nlora_name: COMBO | The name of the LoRA.\nstrength_model: FLOAT | How strongly to modify the diffusion model. This value can be negative. | default=1.0 | min=-100.0 | max=100.0 | step=0.01\nstrength_clip: FLOAT | How strongly to modify the CLIP model. This value can be negative. | default=1.0 | min=-100.0 | max=100.0 | step=0.01\n\n[outputs]\nMODEL: MODEL | The modified diffusion model.\nCLIP: CLIP | The modified CLIP model.",
  "LoraLoaderModelOnly": "LoRAs are used to modify diffusion and CLIP models, altering the way in which latents are denoised such as applying styles. Multiple LoRA nodes can be linked together.\n\n[required inputs]\nmodel: MODEL\nlora_name: COMBO\nstrength_model: FLOAT | default=1.0 | min=-100.0 | max=100.0 | step=0.01\n\n[outputs]\nMODEL: MODEL | The modified diffusion model.",
  "LoraModelLoader": "Load LoRA Model\n\n[required inputs]\nmodel: MODEL | The diffusion model the LoRA will be applied to.\nlora: LORA_MODEL | The LoRA model to apply to the diffusion model.\nstrength_model: FLOAT | How strongly to modify the diffusion model. This value can be negative. | default=1.0 | min=-100.0 | max=100.0\n\n[outputs]\nmodel: MODEL | The modified diffusion model.",
  "LoraSave": "Extract and Save Lora\n\n[required inputs]\nfilename_prefix: STRING | default=loras/ComfyUI_extracted_lora\nrank: INT | default=8 | min=1 | max=4096 | step=1\nlora_type: COMBO | options=standard, full_diff\nbias_diff: BOOLEAN | default=True\n\n[optional inputs]\nmodel_diff: MODEL | The ModelSubtract output to be converted to a lora.\ntext_encoder_diff: CLIP | The CLIPSubtract output to be converted to a lora.\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "LossGraphNode": "Plot Loss Graph\n\n[required inputs]\nloss: LOSS_MAP | Loss map from training node.\nfilename_prefix: STRING | Prefix for the saved loss graph image. | default=loss_graph\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "LotusConditioning": "[outputs]\nconditioning: CONDITIONING",
  "LtxvApiImageToVideo": "LTXV Image To Video\nProfessional-quality videos with customizable duration and resolution based on start image.\n\n[required inputs]\nimage: IMAGE | First frame to be used for the video.\nmodel: COMBO | options=LTX-2 (Pro), LTX-2 (Fast)\nprompt: STRING | default=\nduration: COMBO | default=8 | options=6, 8, 10, 12, 14, 16, 18, 20\nresolution: COMBO | options=1920x1080, 2560x1440, 3840x2160\nfps: COMBO | default=25 | options=25, 50\n\n[optional inputs]\ngenerate_audio: BOOLEAN | When true, the generated video will include AI-generated audio matching the scene. | default=False\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "LtxvApiTextToVideo": "LTXV Text To Video\nProfessional-quality videos with customizable duration and resolution.\n\n[required inputs]\nmodel: COMBO | options=LTX-2 (Pro), LTX-2 (Fast)\nprompt: STRING | default=\nduration: COMBO | default=8 | options=6, 8, 10, 12, 14, 16, 18, 20\nresolution: COMBO | options=1920x1080, 2560x1440, 3840x2160\nfps: COMBO | default=25 | options=25, 50\n\n[optional inputs]\ngenerate_audio: BOOLEAN | When true, the generated video will include AI-generated audio matching the scene. | default=False\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "LumaConceptsNode": "Luma Concepts\nCamera Concepts for use with Luma Text to Video and Luma Image to Video nodes.\n\n[required inputs]\nconcept1: COMBO | options=None, truck_left, pan_right, pedestal_down, low_angle, pedestal_up, selfie, pan_left, roll_right, zoom_in, over_the_shoulder, orbit_right, orbit_left, static, tiny_planet, high_angle, bolt_cam, dolly_zoom, overhead, zoom_out, handheld, roll_left, pov, aerial_drone, push_in, crane_down, truck_right, tilt_down, elevator_doors, tilt_up, ground_level, pull_out, aerial, crane_up, eye_level\nconcept2: COMBO | options=None, truck_left, pan_right, pedestal_down, low_angle, pedestal_up, selfie, pan_left, roll_right, zoom_in, over_the_shoulder, orbit_right, orbit_left, static, tiny_planet, high_angle, bolt_cam, dolly_zoom, overhead, zoom_out, handheld, roll_left, pov, aerial_drone, push_in, crane_down, truck_right, tilt_down, elevator_doors, tilt_up, ground_level, pull_out, aerial, crane_up, eye_level\nconcept3: COMBO | options=None, truck_left, pan_right, pedestal_down, low_angle, pedestal_up, selfie, pan_left, roll_right, zoom_in, over_the_shoulder, orbit_right, orbit_left, static, tiny_planet, high_angle, bolt_cam, dolly_zoom, overhead, zoom_out, handheld, roll_left, pov, aerial_drone, push_in, crane_down, truck_right, tilt_down, elevator_doors, tilt_up, ground_level, pull_out, aerial, crane_up, eye_level\nconcept4: COMBO | options=None, truck_left, pan_right, pedestal_down, low_angle, pedestal_up, selfie, pan_left, roll_right, zoom_in, over_the_shoulder, orbit_right, orbit_left, static, tiny_planet, high_angle, bolt_cam, dolly_zoom, overhead, zoom_out, handheld, roll_left, pov, aerial_drone, push_in, crane_down, truck_right, tilt_down, elevator_doors, tilt_up, ground_level, pull_out, aerial, crane_up, eye_level\n\n[optional inputs]\nluma_concepts: LUMA_CONCEPTS | Optional Camera Concepts to add to the ones chosen here.\n\n[outputs]\nluma_concepts: LUMA_CONCEPTS",
  "LumaImageModifyNode": "Luma Image to Image\nModifies images synchronously based on prompt and aspect ratio.\n\n[required inputs]\nimage: IMAGE\nprompt: STRING | Prompt for the image generation | default=\nimage_weight: FLOAT | Weight of the image; the closer to 1.0, the less the image will be modified. | default=0.1 | min=0.0 | max=0.98 | step=0.01\nmodel: COMBO | options=photon-1, photon-flash-1\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "LumaImageNode": "Luma Text to Image\nGenerates images synchronously based on prompt and aspect ratio.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation | default=\nmodel: COMBO | options=photon-1, photon-flash-1\naspect_ratio: COMBO | default=16:9 | options=1:1, 16:9, 9:16, 4:3, 3:4, 21:9, 9:21\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\nstyle_image_weight: FLOAT | Weight of style image. Ignored if no style_image provided. | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[optional inputs]\nimage_luma_ref: LUMA_REF | Luma Reference node connection to influence generation with input images; up to 4 images can be considered.\nstyle_image: IMAGE | Style reference image; only 1 image will be used.\ncharacter_image: IMAGE | Character reference images; can be a batch of multiple, up to 4 images can be considered.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "LumaImageToVideoNode": "Luma Image to Video\nGenerates videos synchronously based on prompt, input images, and output_size.\n\n[required inputs]\nprompt: STRING | Prompt for the video generation | default=\nmodel: COMBO | options=ray-2, ray-flash-2, ray-1-6\nresolution: COMBO | default=540p | options=540p, 720p, 1080p, 4k\nduration: COMBO | options=5s, 9s\nloop: BOOLEAN | default=False\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[optional inputs]\nfirst_image: IMAGE | First frame of generated video.\nlast_image: IMAGE | Last frame of generated video.\nluma_concepts: LUMA_CONCEPTS | Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "LumaReferenceNode": "Luma Reference\nHolds an image and weight for use with Luma Generate Image node.\n\n[required inputs]\nimage: IMAGE | Image to use as reference.\nweight: FLOAT | Weight of image reference. | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[optional inputs]\nluma_ref: LUMA_REF\n\n[outputs]\nluma_ref: LUMA_REF",
  "LumaVideoNode": "Luma Text to Video\nGenerates videos synchronously based on prompt and output_size.\n\n[required inputs]\nprompt: STRING | Prompt for the video generation | default=\nmodel: COMBO | options=ray-2, ray-flash-2, ray-1-6\naspect_ratio: COMBO | default=16:9 | options=1:1, 16:9, 9:16, 4:3, 3:4, 21:9, 9:21\nresolution: COMBO | default=540p | options=540p, 720p, 1080p, 4k\nduration: COMBO | options=5s, 9s\nloop: BOOLEAN | default=False\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[optional inputs]\nluma_concepts: LUMA_CONCEPTS | Optional Camera Concepts to dictate camera motion via the Luma Concepts node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "Mahiro": "Mahiro is so cute that she deserves a better guidance function!! (\u3002\u30fb\u03c9\u30fb\u3002)\nModify the guidance to scale more on the 'direction' of the positive prompt rather than the difference between the negative prompt.\n\n[required inputs]\nmodel: MODEL\n\n[outputs]\npatched_model: MODEL",
  "MakeTrainingDataset": "Make Training Dataset\n\n[required inputs]\nimages: IMAGE | List of images to encode.\nvae: VAE | VAE model for encoding images to latents.\nclip: CLIP | CLIP model for encoding text to conditioning.\n\n[optional inputs]\ntexts: STRING | List of text captions. Can be length n (matching images), 1 (repeated for all), or omitted (uses empty string).\n\n[outputs]\nlatents: LATENT | List of latent dicts\nconditioning: CONDITIONING | List of conditioning lists",
  "MaskComposite": "[required inputs]\ndestination: MASK\nsource: MASK\nx: INT | default=0 | min=0 | max=16384 | step=1\ny: INT | default=0 | min=0 | max=16384 | step=1\noperation: COMBO | options=multiply, add, subtract, and, or, xor\n\n[outputs]\nMASK: MASK",
  "MaskPreview": "Saves the input images to your ComfyUI output directory.\n\n[required inputs]\nmask: MASK\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "MaskToImage": "Convert Mask to Image\n\n[required inputs]\nmask: MASK\n\n[outputs]\nIMAGE: IMAGE",
  "MergeImageLists": "Merge Image Lists\n\n[required inputs]\nimages: IMAGE | List of images to process.\n\n[outputs]\nimages: IMAGE | Processed images",
  "MergeTextLists": "Merge Text Lists\n\n[required inputs]\ntexts: STRING | List of texts to process.\n\n[outputs]\ntexts: STRING | Processed texts",
  "MinimaxHailuoVideoNode": "MiniMax Hailuo Video\nGenerates videos from prompt, with optional start frame using the new MiniMax Hailuo-02 model.\n\n[required inputs]\nprompt_text: STRING | Text prompt to guide the video generation. | default=\n\n[optional inputs]\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615 | step=1\nfirst_frame_image: IMAGE | Optional image to use as the first frame to generate a video.\nprompt_optimizer: BOOLEAN | Optimize prompt to improve generation quality when needed. | default=True\nduration: COMBO | The length of the output video in seconds. | default=6 | options=6, 10\nresolution: COMBO | The dimensions of the video display. 1080p is 1920x1080, 768p is 1366x768. | default=768P | options=768P, 1080P\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "MinimaxImageToVideoNode": "MiniMax Image to Video\nGenerates videos synchronously based on an image and prompt, and optional parameters.\n\n[required inputs]\nimage: IMAGE | Image to use as first frame of video generation\nprompt_text: STRING | Text prompt to guide the video generation | default=\nmodel: COMBO | Model to use for video generation | default=I2V-01 | options=I2V-01-Director, I2V-01, I2V-01-live\n\n[optional inputs]\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "MinimaxTextToVideoNode": "MiniMax Text to Video\nGenerates videos synchronously based on a prompt, and optional parameters.\n\n[required inputs]\nprompt_text: STRING | Text prompt to guide the video generation | default=\nmodel: COMBO | Model to use for video generation | default=T2V-01 | options=T2V-01, T2V-01-Director\n\n[optional inputs]\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=18446744073709551615 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "ModelComputeDtype": "[required inputs]\nmodel: MODEL\ndtype: COMBO | options=default, fp32, fp16, bf16\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeAdd": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeAuraflow": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\ninit_x_linear.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\npositional_encoding: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ncond_seq_linear.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nregister_tokens: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_layers.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_layers.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_layers.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_layers.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_layers.31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmodF.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_linear.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeBlocks": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\ninput: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nout: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeCosmos14B": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nextra_pos_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nx_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\naffline_norm.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block32.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block33.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block34.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block35.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeCosmos7B": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nextra_pos_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nx_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\naffline_norm.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.block27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeCosmosPredict2_14B": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nx_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedding_norm.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.32.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.33.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.34.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.35.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeCosmosPredict2_2B": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nx_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedding_norm.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeFlux1": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\nimg_in.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntime_in.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nguidance_in: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nvector_in.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntxt_in.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ndouble_blocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.32.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.33.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.34.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.35.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.36.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nsingle_blocks.37.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeLTXV": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npatchify_proj.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nadaln_single.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ncaption_projection.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nscale_shift_table: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nproj_out.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeMochiPreview": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_frequencies.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt5_y_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt5_yproj.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.32.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.33.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.34.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.35.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.36.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.37.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.38.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.39.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.40.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.41.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.42.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.43.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.44.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.45.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.46.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.47.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeQwenImage": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_embeds.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nimg_in.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntxt_norm.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntxt_in.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntime_text_embed.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.32.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.33.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.34.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.35.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.36.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.37.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.38.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.39.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.40.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.41.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.42.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.43.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.44.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.45.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.46.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.47.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.48.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.49.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.50.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.51.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.52.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.53.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.54.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.55.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.56.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.57.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.58.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntransformer_blocks.59.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nproj_out.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeSD1": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\ntime_embed.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nlabel_emb.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nout.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeSD2": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\ntime_embed.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nlabel_emb.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nout.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeSD35_Large": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_embed.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nx_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ncontext_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ny_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.32.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.33.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.34.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.35.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.36.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.37.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeSD3_2B": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npos_embed.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nx_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ncontext_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ny_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nt_embedder.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\njoint_blocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nfinal_layer.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeSDXL": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\ntime_embed.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nlabel_emb.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.0: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.1: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.2: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.3: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.4: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.5: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.6: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.7: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ninput_blocks.8: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.0: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.1: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nmiddle_block.2: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.0: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.1: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.2: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.3: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.4: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.5: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.6: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.7: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\noutput_blocks.8: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nout.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeSimple": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\nratio: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeSubtract": "[required inputs]\nmodel1: MODEL\nmodel2: MODEL\nmultiplier: FLOAT | default=1.0 | min=-10.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelMergeWAN2_1": "1.3B model has 30 blocks, 14B model has 40 blocks. Image to video model has the extra img_emb.\n\n[required inputs]\nmodel1: MODEL\nmodel2: MODEL\npatch_embedding.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntime_embedding.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntime_projection.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\ntext_embedding.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nimg_emb.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.0.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.1.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.2.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.3.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.4.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.5.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.6.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.7.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.8.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.9.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.10.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.11.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.12.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.13.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.14.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.15.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.16.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.17.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.18.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.19.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.20.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.21.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.22.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.23.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.24.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.25.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.26.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.27.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.28.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.29.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.30.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.31.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.32.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.33.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.34.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.35.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.36.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.37.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.38.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nblocks.39.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nhead.: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelPatchLoader": "[required inputs]\nname: COMBO\n\n[outputs]\nMODEL_PATCH: MODEL_PATCH",
  "ModelSamplingAuraFlow": "[required inputs]\nmodel: MODEL\nshift: FLOAT | default=1.73 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelSamplingContinuousEDM": "[required inputs]\nmodel: MODEL\nsampling: COMBO | options=v_prediction, edm, edm_playground_v2.5, eps, cosmos_rflow\nsigma_max: FLOAT | default=120.0 | min=0.0 | max=1000.0 | step=0.001 | round=False\nsigma_min: FLOAT | default=0.002 | min=0.0 | max=1000.0 | step=0.001 | round=False\n\n[outputs]\nMODEL: MODEL",
  "ModelSamplingContinuousEdm": "",
  "ModelSamplingContinuousV": "[required inputs]\nmodel: MODEL\nsampling: COMBO | options=v_prediction\nsigma_max: FLOAT | default=500.0 | min=0.0 | max=1000.0 | step=0.001 | round=False\nsigma_min: FLOAT | default=0.03 | min=0.0 | max=1000.0 | step=0.001 | round=False\n\n[outputs]\nMODEL: MODEL",
  "ModelSamplingDiscrete": "[required inputs]\nmodel: MODEL\nsampling: COMBO | options=eps, v_prediction, lcm, x0, img_to_img\nzsnr: BOOLEAN | default=False\n\n[outputs]\nMODEL: MODEL",
  "ModelSamplingFlux": "[required inputs]\nmodel: MODEL\nmax_shift: FLOAT | default=1.15 | min=0.0 | max=100.0 | step=0.01\nbase_shift: FLOAT | default=0.5 | min=0.0 | max=100.0 | step=0.01\nwidth: INT | default=1024 | min=16 | max=16384 | step=8\nheight: INT | default=1024 | min=16 | max=16384 | step=8\n\n[outputs]\nMODEL: MODEL",
  "ModelSamplingLTXV": "[required inputs]\nmodel: MODEL\nmax_shift: FLOAT | default=2.05 | min=0.0 | max=100.0 | step=0.01\nbase_shift: FLOAT | default=0.95 | min=0.0 | max=100.0 | step=0.01\n\n[optional inputs]\nlatent: LATENT\n\n[outputs]\nMODEL: MODEL",
  "ModelSamplingSD3": "[required inputs]\nmodel: MODEL\nshift: FLOAT | default=3.0 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelSamplingStableCascade": "[required inputs]\nmodel: MODEL\nshift: FLOAT | default=2.0 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "ModelSave": "[required inputs]\nmodel: MODEL\nfilename_prefix: STRING | default=diffusion_models/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "MoonvalleyImg2VideoNode": "Moonvalley Marey Image to Video\nMoonvalley Marey Image to Video Node\n\n[required inputs]\nimage: IMAGE | The reference image used to generate the video\nprompt: STRING\nnegative_prompt: STRING | Negative prompt text | default=<synthetic> <scene cut> gopro, bright, contrast, static, overexposed, vignette, artifacts, still, noise, texture, scanlines, videogame, 360 camera, VR, transition, flare, saturation, distorted, warped, wide angle, saturated, vibrant, glowing, cross dissolve, cheesy, ugly hands, mutated hands, mutant, disfigured, extra fingers, blown out, horrible, blurry, worst quality, bad, dissolve, melt, fade in, fade out, wobbly, weird, low quality, plastic, stock footage, video camera, boring\nresolution: COMBO | Resolution of the output video | default=16:9 (1920 x 1080) | options=16:9 (1920 x 1080), 9:16 (1080 x 1920), 1:1 (1152 x 1152), 4:3 (1536 x 1152), 3:4 (1152 x 1536)\nprompt_adherence: FLOAT | Guidance scale for generation control | default=4.5 | min=1.0 | max=20.0 | step=1.0\nseed: INT | Random seed value | default=9 | min=0 | max=4294967295 | step=1\nsteps: INT | Number of denoising steps | default=33 | min=1 | max=100 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "MoonvalleyTxt2VideoNode": "Moonvalley Marey Text to Video\n\n[required inputs]\nprompt: STRING\nnegative_prompt: STRING | Negative prompt text | default=<synthetic> <scene cut> gopro, bright, contrast, static, overexposed, vignette, artifacts, still, noise, texture, scanlines, videogame, 360 camera, VR, transition, flare, saturation, distorted, warped, wide angle, saturated, vibrant, glowing, cross dissolve, cheesy, ugly hands, mutated hands, mutant, disfigured, extra fingers, blown out, horrible, blurry, worst quality, bad, dissolve, melt, fade in, fade out, wobbly, weird, low quality, plastic, stock footage, video camera, boring\nresolution: COMBO | Resolution of the output video | default=16:9 (1920 x 1080) | options=16:9 (1920 x 1080), 9:16 (1080 x 1920), 1:1 (1152 x 1152), 4:3 (1536 x 1152), 3:4 (1152 x 1536), 21:9 (2560 x 1080)\nprompt_adherence: FLOAT | Guidance scale for generation control | default=4.0 | min=1.0 | max=20.0 | step=1.0\nseed: INT | Random seed value | default=9 | min=0 | max=4294967295 | step=1\nsteps: INT | Inference steps | default=33 | min=1 | max=100 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "MoonvalleyVideo2VideoNode": "Moonvalley Marey Video to Video\n\n[required inputs]\nprompt: STRING | Describes the video to generate\nnegative_prompt: STRING | Negative prompt text | default=<synthetic> <scene cut> gopro, bright, contrast, static, overexposed, vignette, artifacts, still, noise, texture, scanlines, videogame, 360 camera, VR, transition, flare, saturation, distorted, warped, wide angle, saturated, vibrant, glowing, cross dissolve, cheesy, ugly hands, mutated hands, mutant, disfigured, extra fingers, blown out, horrible, blurry, worst quality, bad, dissolve, melt, fade in, fade out, wobbly, weird, low quality, plastic, stock footage, video camera, boring\nseed: INT | Random seed value | default=9 | min=0 | max=4294967295 | step=1\nvideo: VIDEO | The reference video used to generate the output video. Must be at least 5 seconds long. Videos longer than 5s will be automatically trimmed. Only MP4 format supported.\nsteps: INT | Number of inference steps | default=33 | min=1 | max=100 | step=1\n\n[optional inputs]\ncontrol_type: COMBO | default=Motion Transfer | options=Motion Transfer, Pose Transfer\nmotion_intensity: INT | Only used if control_type is 'Motion Transfer' | default=100 | min=0 | max=100 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "Morphology": "ImageMorphology\n\n[required inputs]\nimage: IMAGE\noperation: COMBO | options=erode, dilate, open, close, gradient, bottom_hat, top_hat\nkernel_size: INT | default=3 | min=3 | max=999 | step=1\n\n[outputs]\nIMAGE: IMAGE",
  "NormalizeImages": "Normalize Images\n\n[required inputs]\nimages: IMAGE | Image to process.\nmean: FLOAT | Mean value for normalization. | default=0.5 | min=0.0 | max=1.0\nstd: FLOAT | Standard deviation for normalization. | default=0.5 | min=0.001 | max=1.0\n\n[outputs]\nimages: IMAGE | Processed images",
  "Note": "",
  "OpenAIChatConfig": "OpenAI ChatGPT Advanced Options\nAllows specifying advanced configuration options for the OpenAI Chat Nodes.\n\n[required inputs]\ntruncation: COMBO | The truncation strategy to use for the model response. auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.disabled: If a model response will exceed the context window size for a model, the request will fail with a 400 error | default=auto | options=auto, disabled\n\n[optional inputs]\nmax_output_tokens: INT | An upper bound for the number of tokens that can be generated for a response, including visible output tokens | default=4096 | min=16 | max=16384\ninstructions: STRING | Instructions for the model on how to generate the response\n\n[outputs]\nOPENAI_CHAT_CONFIG: OPENAI_CHAT_CONFIG",
  "OpenAIChatNode": "OpenAI ChatGPT\nGenerate text responses from an OpenAI model.\n\n[required inputs]\nprompt: STRING | Text inputs to the model, used to generate a response. | default=\npersist_context: BOOLEAN | This parameter is deprecated and has no effect. | default=False\nmodel: COMBO | The model used to generate the response | options=o4-mini, o1, o3, o1-pro, gpt-4o, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, gpt-5, gpt-5-mini, gpt-5-nano\n\n[optional inputs]\nimages: IMAGE | Optional image(s) to use as context for the model. To include multiple images, you can use the Batch Images node.\nfiles: OPENAI_INPUT_FILES | Optional file(s) to use as context for the model. Accepts inputs from the OpenAI Chat Input Files node.\nadvanced_options: OPENAI_CHAT_CONFIG | Optional configuration for the model. Accepts inputs from the OpenAI Chat Advanced Options node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nSTRING: STRING",
  "OpenAIDalle2": "OpenAI DALL\u00b7E 2\nGenerates images synchronously via OpenAI's DALL\u00b7E 2 endpoint.\n\n[required inputs]\nprompt: STRING | Text prompt for DALL\u00b7E | default=\n\n[optional inputs]\nseed: INT | not implemented yet in backend | default=0 | min=0 | max=2147483647 | step=1\nsize: COMBO | Image size | default=1024x1024 | options=256x256, 512x512, 1024x1024\nn: INT | How many images to generate | default=1 | min=1 | max=8 | step=1\nimage: IMAGE | Optional reference image for image editing.\nmask: MASK | Optional mask for inpainting (white areas will be replaced)\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "OpenAIDalle3": "OpenAI DALL\u00b7E 3\nGenerates images synchronously via OpenAI's DALL\u00b7E 3 endpoint.\n\n[required inputs]\nprompt: STRING | Text prompt for DALL\u00b7E | default=\n\n[optional inputs]\nseed: INT | not implemented yet in backend | default=0 | min=0 | max=2147483647 | step=1\nquality: COMBO | Image quality | default=standard | options=standard, hd\nstyle: COMBO | Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. | default=natural | options=natural, vivid\nsize: COMBO | Image size | default=1024x1024 | options=1024x1024, 1024x1792, 1792x1024\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "OpenAIGPTImage1": "OpenAI GPT Image 1\nGenerates images synchronously via OpenAI's GPT Image 1 endpoint.\n\n[required inputs]\nprompt: STRING | Text prompt for GPT Image 1 | default=\n\n[optional inputs]\nseed: INT | not implemented yet in backend | default=0 | min=0 | max=2147483647 | step=1\nquality: COMBO | Image quality, affects cost and generation time. | default=low | options=low, medium, high\nbackground: COMBO | Return image with or without background | default=opaque | options=opaque, transparent\nsize: COMBO | Image size | default=auto | options=auto, 1024x1024, 1024x1536, 1536x1024\nn: INT | How many images to generate | default=1 | min=1 | max=8 | step=1\nimage: IMAGE | Optional reference image for image editing.\nmask: MASK | Optional mask for inpainting (white areas will be replaced)\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "OpenAIInputFiles": "OpenAI ChatGPT Input Files\nLoads and prepares input files (text, pdf, etc.) to include as inputs for the OpenAI Chat Node. The files will be read by the OpenAI model when generating a response. \ud83d\udec8 TIP: Can be chained together with other OpenAI Input File nodes.\n\n[required inputs]\nfile: COMBO | Input files to include as context for the model. Only accepts text (.txt) and PDF (.pdf) files for now.\n\n[optional inputs]\nOPENAI_INPUT_FILES: OPENAI_INPUT_FILES | An optional additional file(s) to batch together with the file loaded from this node. Allows chaining of input files so that a single message can include multiple input files.\n\n[outputs]\nOPENAI_INPUT_FILES: OPENAI_INPUT_FILES",
  "OpenAIVideoSora2": "OpenAI Sora - Video\nOpenAI video and audio generation.\n\n[required inputs]\nmodel: COMBO | default=sora-2 | options=sora-2, sora-2-pro\nprompt: STRING | Guiding text; may be empty if an input image is present. | default=\nsize: COMBO | default=1280x720 | options=720x1280, 1280x720, 1024x1792, 1792x1024\nduration: COMBO | default=8 | options=4, 8, 12\n\n[optional inputs]\nimage: IMAGE\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=2147483647 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "OptimalStepsScheduler": "[required inputs]\nmodel_type: COMBO | options=FLUX, Wan, Chroma\nsteps: INT | default=20 | min=3 | max=1000\ndenoise: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nSIGMAS: SIGMAS",
  "PairConditioningCombine": "Cond Pair Combine\n\n[required inputs]\npositive_A: CONDITIONING\nnegative_A: CONDITIONING\npositive_B: CONDITIONING\nnegative_B: CONDITIONING\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "PairConditioningSetDefaultCombine": "Cond Pair Set Default Combine\n\n[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\npositive_DEFAULT: CONDITIONING\nnegative_DEFAULT: CONDITIONING\n\n[optional inputs]\nhooks: HOOKS\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "PairConditioningSetProperties": "Cond Pair Set Props\n\n[required inputs]\npositive_NEW: CONDITIONING\nnegative_NEW: CONDITIONING\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nset_cond_area: COMBO | options=default, mask bounds\n\n[optional inputs]\nmask: MASK\nhooks: HOOKS\ntimesteps: TIMESTEPS_RANGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "PairConditioningSetPropertiesAndCombine": "Cond Pair Set Props Combine\n\n[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\npositive_NEW: CONDITIONING\nnegative_NEW: CONDITIONING\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nset_cond_area: COMBO | options=default, mask bounds\n\n[optional inputs]\nmask: MASK\nhooks: HOOKS\ntimesteps: TIMESTEPS_RANGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING",
  "PatchModelAddDownscale": "PatchModelAddDownscale (Kohya Deep Shrink)\n\n[required inputs]\nmodel: MODEL\nblock_number: INT | default=3 | min=1 | max=32 | step=1\ndownscale_factor: FLOAT | default=2.0 | min=0.1 | max=9.0 | step=0.001\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=0.35 | min=0.0 | max=1.0 | step=0.001\ndownscale_after_skip: BOOLEAN | default=True\ndownscale_method: COMBO | options=bicubic, nearest-exact, bilinear, area, bislerp\nupscale_method: COMBO | options=bicubic, nearest-exact, bilinear, area, bislerp\n\n[outputs]\nMODEL: MODEL",
  "PerpNeg": "Perp-Neg (DEPRECATED by PerpNegGuider)\n\n[required inputs]\nmodel: MODEL\nempty_conditioning: CONDITIONING\nneg_scale: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "PerpNegGuider": "[required inputs]\nmodel: MODEL\npositive: CONDITIONING\nnegative: CONDITIONING\nempty_conditioning: CONDITIONING\ncfg: FLOAT | default=8.0 | min=0.0 | max=100.0 | step=0.1 | round=0.01\nneg_scale: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\nGUIDER: GUIDER",
  "PerturbedAttentionGuidance": "[required inputs]\nmodel: MODEL\nscale: FLOAT | default=3.0 | min=0.0 | max=100.0 | step=0.01 | round=0.01\n\n[outputs]\nMODEL: MODEL",
  "PhotoMakerEncode": "[required inputs]\nphotomaker: PHOTOMAKER\nimage: IMAGE\nclip: CLIP\ntext: STRING | default=photograph of photomaker\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "PhotoMakerLoader": "[required inputs]\nphotomaker_model_name: COMBO\n\n[outputs]\nPHOTOMAKER: PHOTOMAKER",
  "PikaImageToVideoNode2_2": "Pika Image to Video\nSends an image and prompt to the Pika API v2.2 to generate a video.\n\n[required inputs]\nimage: IMAGE | The image to convert to video\nprompt_text: STRING\nnegative_prompt: STRING\nseed: INT | min=0 | max=4294967295\nresolution: COMBO | default=1080p | options=1080p, 720p\nduration: COMBO | default=5 | options=5, 10\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "PikaScenesV2_2": "Pika Scenes (Video Image Composition)\nCombine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them.\n\n[required inputs]\nprompt_text: STRING\nnegative_prompt: STRING\nseed: INT | min=0 | max=4294967295\nresolution: COMBO | default=1080p | options=1080p, 720p\nduration: COMBO | default=5 | options=5, 10\ningredients_mode: COMBO | default=creative | options=creative, precise\naspect_ratio: FLOAT | Aspect ratio (width / height) | default=1.7777777777777777 | min=0.4 | max=2.5 | step=0.001\n\n[optional inputs]\nimage_ingredient_1: IMAGE | Image that will be used as ingredient to create a video.\nimage_ingredient_2: IMAGE | Image that will be used as ingredient to create a video.\nimage_ingredient_3: IMAGE | Image that will be used as ingredient to create a video.\nimage_ingredient_4: IMAGE | Image that will be used as ingredient to create a video.\nimage_ingredient_5: IMAGE | Image that will be used as ingredient to create a video.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "PikaStartEndFrameNode2_2": "Pika Start and End Frame to Video\nGenerate a video by combining your first and last frame. Upload two images to define the start and end points, and let the AI create a smooth transition between them.\n\n[required inputs]\nimage_start: IMAGE | The first image to combine.\nimage_end: IMAGE | The last image to combine.\nprompt_text: STRING\nnegative_prompt: STRING\nseed: INT | min=0 | max=4294967295\nresolution: COMBO | default=1080p | options=1080p, 720p\nduration: COMBO | default=5 | options=5, 10\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "PikaTextToVideoNode2_2": "Pika Text to Video\nSends a text prompt to the Pika API v2.2 to generate a video.\n\n[required inputs]\nprompt_text: STRING\nnegative_prompt: STRING\nseed: INT | min=0 | max=4294967295\nresolution: COMBO | default=1080p | options=1080p, 720p\nduration: COMBO | default=5 | options=5, 10\naspect_ratio: FLOAT | Aspect ratio (width / height) | default=1.7777777777777777 | min=0.4 | max=2.5 | step=0.001\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "Pikadditions": "Pikadditions (Video Object Insertion)\nAdd any object or image into your video. Upload a video and specify what you'd like to add to create a seamlessly integrated result.\n\n[required inputs]\nvideo: VIDEO | The video to add an image to.\nimage: IMAGE | The image to add to the video.\nprompt_text: STRING\nnegative_prompt: STRING\nseed: INT | min=0 | max=4294967295\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "Pikaffects": "Pikaffects (Video Effects)\nGenerate a video with a specific Pikaffect. Supported Pikaffects: Cake-ify, Crumble, Crush, Decapitate, Deflate, Dissolve, Explode, Eye-pop, Inflate, Levitate, Melt, Peel, Poke, Squish, Ta-da, Tear\n\n[required inputs]\nimage: IMAGE | The reference image to apply the Pikaffect to.\npikaffect: COMBO | default=Cake-ify | options=Cake-ify, Crumble, Crush, Decapitate, Deflate, Dissolve, Explode, Eye-pop, Inflate, Levitate, Melt, Peel, Poke, Squish, Ta-da, Tear\nprompt_text: STRING\nnegative_prompt: STRING\nseed: INT | min=0 | max=4294967295\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "Pikaswaps": "Pika Swaps (Video Object Replacement)\nSwap out any object or region of your video with a new image or object. Define areas to replace either with a mask or coordinates.\n\n[required inputs]\nvideo: VIDEO | The video to swap an object in.\n\n[optional inputs]\nimage: IMAGE | The image used to replace the masked object in the video.\nmask: MASK | Use the mask to define areas in the video to replace.\nprompt_text: STRING\nnegative_prompt: STRING\nseed: INT | min=0 | max=4294967295\nregion_to_modify: STRING | Plaintext description of the object / region to modify.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "PixverseImageToVideoNode": "PixVerse Image to Video\nGenerates videos based on prompt and output_size.\n\n[required inputs]\nimage: IMAGE\nprompt: STRING | Prompt for the video generation | default=\nquality: COMBO | default=540p | options=360p, 540p, 720p, 1080p\nduration_seconds: COMBO | options=5, 8\nmotion_mode: COMBO | options=normal, fast\nseed: INT | Seed for video generation. | default=0 | min=0 | max=2147483647\n\n[optional inputs]\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\npixverse_template: PIXVERSE_TEMPLATE | An optional template to influence style of generation, created by the PixVerse Template node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "PixverseTemplateNode": "PixVerse Template\n\n[required inputs]\ntemplate: COMBO | options=Microwave, Suit Swagger, Anything, Robot, Subject 3 Fever, kiss kiss\n\n[outputs]\npixverse_template: PIXVERSE_TEMPLATE",
  "PixverseTextToVideoNode": "PixVerse Text to Video\nGenerates videos based on prompt and output_size.\n\n[required inputs]\nprompt: STRING | Prompt for the video generation | default=\naspect_ratio: COMBO | options=16:9, 4:3, 1:1, 3:4, 9:16\nquality: COMBO | default=540p | options=360p, 540p, 720p, 1080p\nduration_seconds: COMBO | options=5, 8\nmotion_mode: COMBO | options=normal, fast\nseed: INT | Seed for video generation. | default=0 | min=0 | max=2147483647\n\n[optional inputs]\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\npixverse_template: PIXVERSE_TEMPLATE | An optional template to influence style of generation, created by the PixVerse Template node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "PixverseTransitionVideoNode": "PixVerse Transition Video\nGenerates videos based on prompt and output_size.\n\n[required inputs]\nfirst_frame: IMAGE\nlast_frame: IMAGE\nprompt: STRING | Prompt for the video generation | default=\nquality: COMBO | default=540p | options=360p, 540p, 720p, 1080p\nduration_seconds: COMBO | options=5, 8\nmotion_mode: COMBO | options=normal, fast\nseed: INT | Seed for video generation. | default=0 | min=0 | max=2147483647\n\n[optional inputs]\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "PolyexponentialScheduler": "[required inputs]\nsteps: INT | default=20 | min=1 | max=10000\nsigma_max: FLOAT | default=14.614642 | min=0.0 | max=5000.0 | step=0.01 | round=False\nsigma_min: FLOAT | default=0.0291675 | min=0.0 | max=5000.0 | step=0.01 | round=False\nrho: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\n\n[outputs]\nSIGMAS: SIGMAS",
  "PorterDuffImageComposite": "Porter-Duff Image Composite\n\n[required inputs]\nsource: IMAGE\nsource_alpha: MASK\ndestination: IMAGE\ndestination_alpha: MASK\nmode: COMBO | default=DST | options=ADD, CLEAR, DARKEN, DST, DST_ATOP, DST_IN, DST_OUT, DST_OVER, LIGHTEN, MULTIPLY, OVERLAY, SCREEN, SRC, SRC_ATOP, SRC_IN, SRC_OUT, SRC_OVER, XOR\n\n[outputs]\nIMAGE: IMAGE\nMASK: MASK",
  "Preview3D": "Preview 3D & Animation\n\n[required inputs]\nmodel_file: STRING | default=\n\n[optional inputs]\ncamera_info: LOAD3D_CAMERA\nbg_image: IMAGE",
  "PreviewAny": "Preview as Text\n\n[required inputs]\nsource: *",
  "PreviewAudio": "Preview Audio\n\n[required inputs]\naudio: AUDIO\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "PreviewImage": "Preview Image\nSaves the input images to your ComfyUI output directory.\n\n[required inputs]\nimages: IMAGE\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "Primitive": "",
  "PrimitiveBoolean": "Boolean\n\n[required inputs]\nvalue: BOOLEAN\n\n[outputs]\nBOOLEAN: BOOLEAN",
  "PrimitiveFloat": "Float\n\n[required inputs]\nvalue: FLOAT | min=-9223372036854775807 | max=9223372036854775807\n\n[outputs]\nFLOAT: FLOAT",
  "PrimitiveInt": "Int\n\n[required inputs]\nvalue: INT | min=-9223372036854775807 | max=9223372036854775807\n\n[outputs]\nINT: INT",
  "PrimitiveString": "String\n\n[required inputs]\nvalue: STRING\n\n[outputs]\nSTRING: STRING",
  "PrimitiveStringMultiline": "String (Multiline)\n\n[required inputs]\nvalue: STRING\n\n[outputs]\nSTRING: STRING",
  "QuadrupleCLIPLoader": "[Recipes]\n\nhidream: long clip-l, long clip-g, t5xxl, llama_8b_3.1_instruct\n\n[required inputs]\nclip_name1: COMBO\nclip_name2: COMBO\nclip_name3: COMBO\nclip_name4: COMBO\n\n[outputs]\nCLIP: CLIP",
  "QuadrupleClipLoader": "",
  "QwenImageDiffsynthControlnet": "[required inputs]\nmodel: MODEL\nmodel_patch: MODEL_PATCH\nvae: VAE\nimage: IMAGE\nstrength: FLOAT | default=1.0 | min=-10.0 | max=10.0 | step=0.01\n\n[optional inputs]\nmask: MASK\n\n[outputs]\nMODEL: MODEL",
  "RandomCropImages": "Random Crop Images\n\n[required inputs]\nimages: IMAGE | Image to process.\nwidth: INT | Crop width. | default=512 | min=1 | max=8192\nheight: INT | Crop height. | default=512 | min=1 | max=8192\nseed: INT | Random seed. | default=0 | min=0 | max=18446744073709551615\n\n[outputs]\nimages: IMAGE | Processed images",
  "RandomNoise": "[required inputs]\nnoise_seed: INT | default=0 | min=0 | max=18446744073709551615\n\n[outputs]\nNOISE: NOISE",
  "RebatchImages": "Rebatch Images\n\n[required inputs]\nimages: IMAGE\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nIMAGE: IMAGE",
  "RebatchLatents": "Rebatch Latents\n\n[required inputs]\nlatents: LATENT\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nLATENT: LATENT",
  "RecordAudio": "Record Audio\n\n[required inputs]\naudio: AUDIO_RECORD\n\n[outputs]\nAUDIO: AUDIO",
  "RecraftColorRGB": "Recraft Color RGB\nCreate Recraft Color by choosing specific RGB values.\n\n[required inputs]\nr: INT | Red value of color. | default=0 | min=0 | max=255\ng: INT | Green value of color. | default=0 | min=0 | max=255\nb: INT | Blue value of color. | default=0 | min=0 | max=255\n\n[optional inputs]\nrecraft_color: RECRAFT_COLOR\n\n[outputs]\nrecraft_color: RECRAFT_COLOR",
  "RecraftControls": "Recraft Controls\nCreate Recraft Controls for customizing Recraft generation.\n\n[optional inputs]\ncolors: RECRAFT_COLOR\nbackground_color: RECRAFT_COLOR\n\n[outputs]\nrecraft_controls: RECRAFT_CONTROLS",
  "RecraftCreativeUpscaleNode": "Recraft Creative Upscale Image\nUpscale image synchronously.\nEnhances a given raster image using \u2018creative upscale\u2019 tool, boosting resolution with a focus on refining small details and faces.\n\n[required inputs]\nimage: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "RecraftCrispUpscaleNode": "Recraft Crisp Upscale Image\nUpscale image synchronously.\nEnhances a given raster image using \u2018crisp upscale\u2019 tool, increasing image resolution, making the image sharper and cleaner.\n\n[required inputs]\nimage: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "RecraftImageInpaintingNode": "Recraft Image Inpainting\nModify image based on prompt and mask.\n\n[required inputs]\nimage: IMAGE\nmask: MASK\nprompt: STRING | Prompt for the image generation. | default=\nn: INT | The number of images to generate. | default=1 | min=1 | max=6\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[optional inputs]\nrecraft_style: RECRAFT_V3_STYLE\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "RecraftImageToImageNode": "Recraft Image to Image\nModify image based on prompt and strength.\n\n[required inputs]\nimage: IMAGE\nprompt: STRING | Prompt for the image generation. | default=\nn: INT | The number of images to generate. | default=1 | min=1 | max=6\nstrength: FLOAT | Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity. | default=0.5 | min=0.0 | max=1.0 | step=0.01\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[optional inputs]\nrecraft_style: RECRAFT_V3_STYLE\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\nrecraft_controls: RECRAFT_CONTROLS | Optional additional controls over the generation via the Recraft Controls node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "RecraftRemoveBackgroundNode": "Recraft Remove Background\nRemove background from image, and return processed image and mask.\n\n[required inputs]\nimage: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE\nMASK: MASK",
  "RecraftReplaceBackgroundNode": "Recraft Replace Background\nReplace background on image, based on provided prompt.\n\n[required inputs]\nimage: IMAGE\nprompt: STRING | Prompt for the image generation. | default=\nn: INT | The number of images to generate. | default=1 | min=1 | max=6\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[optional inputs]\nrecraft_style: RECRAFT_V3_STYLE\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "RecraftStyleV3DigitalIllustration": "Recraft Style - Digital Illustration\nSelect realistic_image style and optional substyle.\n\n[required inputs]\nsubstyle: COMBO | options=None, 2d_art_poster, 2d_art_poster_2, antiquarian, bold_fantasy, child_book, child_books, cover, crosshatch, digital_engraving, engraving_color, expressionism, freehand_details, grain, grain_20, graphic_intensity, hand_drawn, hand_drawn_outline, handmade_3d, hard_comics, infantile_sketch, long_shadow, modern_folk, multicolor, neon_calm, noir, nostalgic_pastel, outline_details, pastel_gradient, pastel_sketch, pixel_art, plastic, pop_art, pop_renaissance, seamless, street_art, tablet_sketch, urban_glow, urban_sketching, vanilla_dreams, young_adult_book, young_adult_book_2\n\n[outputs]\nrecraft_style: RECRAFT_V3_STYLE",
  "RecraftStyleV3InfiniteStyleLibrary": "Recraft Style - Infinite Style Library\nSelect style based on preexisting UUID from Recraft's Infinite Style Library.\n\n[required inputs]\nstyle_id: STRING | UUID of style from Infinite Style Library. | default=\n\n[outputs]\nrecraft_style: RECRAFT_V3_STYLE",
  "RecraftStyleV3LogoRaster": "Recraft Style - Logo Raster\nSelect realistic_image style and optional substyle.\n\n[required inputs]\nsubstyle: COMBO | options=emblem_graffiti, emblem_pop_art, emblem_punk, emblem_stamp, emblem_vintage\n\n[outputs]\nrecraft_style: RECRAFT_V3_STYLE",
  "RecraftStyleV3RealisticImage": "Recraft Style - Realistic Image\nSelect realistic_image style and optional substyle.\n\n[required inputs]\nsubstyle: COMBO | options=None, b_and_w, enterprise, evening_light, faded_nostalgia, forest_life, hard_flash, hdr, motion_blur, mystic_naturalism, natural_light, natural_tones, organic_calm, real_life_glow, retro_realism, retro_snapshot, studio_portrait, urban_drama, village_realism, warm_folk\n\n[outputs]\nrecraft_style: RECRAFT_V3_STYLE",
  "RecraftTextToImageNode": "Recraft Text to Image\nGenerates images synchronously based on prompt and resolution.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation. | default=\nsize: COMBO | The size of the generated image. | default=1024x1024 | options=1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024\nn: INT | The number of images to generate. | default=1 | min=1 | max=6\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[optional inputs]\nrecraft_style: RECRAFT_V3_STYLE\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\nrecraft_controls: RECRAFT_CONTROLS | Optional additional controls over the generation via the Recraft Controls node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "RecraftTextToVectorNode": "Recraft Text to Vector\nGenerates SVG synchronously based on prompt and resolution.\n\n[required inputs]\nprompt: STRING | Prompt for the image generation. | default=\nsubstyle: COMBO | options=None, bold_stroke, chemistry, colored_stencil, contour_pop_art, cosmics, cutout, depressive, editorial, emotional_flat, engraving, infographical, line_art, line_circuit, linocut, marker_outline, mosaic, naivector, roundish_flat, seamless, segmented_colors, sharp_contrast, thin, vector_photo, vivid_shapes\nsize: COMBO | The size of the generated image. | default=1024x1024 | options=1024x1024, 1365x1024, 1024x1365, 1536x1024, 1024x1536, 1820x1024, 1024x1820, 1024x2048, 2048x1024, 1434x1024, 1024x1434, 1024x1280, 1280x1024, 1024x1707, 1707x1024\nn: INT | The number of images to generate. | default=1 | min=1 | max=6\nseed: INT | Seed to determine if node should re-run; actual results are nondeterministic regardless of seed. | default=0 | min=0 | max=18446744073709551615\n\n[optional inputs]\nnegative_prompt: STRING | An optional text description of undesired elements on an image. | default=\nrecraft_controls: RECRAFT_CONTROLS | Optional additional controls over the generation via the Recraft Controls node.\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nSVG: SVG",
  "RecraftVectorizeImageNode": "Recraft Vectorize Image\nGenerates SVG synchronously from an input image.\n\n[required inputs]\nimage: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nSVG: SVG",
  "ReferenceLatent": "This node sets the guiding latent for an edit model. If the model supports it you can chain multiple to set multiple reference images.\n\n[required inputs]\nconditioning: CONDITIONING\n\n[optional inputs]\nlatent: LATENT\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "RegexExtract": "Regex Extract\n\n[required inputs]\nstring: STRING\nregex_pattern: STRING\nmode: COMBO | options=First Match, All Matches, First Group, All Groups\ncase_insensitive: BOOLEAN | default=True\nmultiline: BOOLEAN | default=False\ndotall: BOOLEAN | default=False\ngroup_index: INT | default=1 | min=0 | max=100\n\n[outputs]\nSTRING: STRING",
  "RegexMatch": "Regex Match\n\n[required inputs]\nstring: STRING\nregex_pattern: STRING\ncase_insensitive: BOOLEAN | default=True\nmultiline: BOOLEAN | default=False\ndotall: BOOLEAN | default=False\n\n[outputs]\nmatches: BOOLEAN",
  "RegexReplace": "Regex Replace\nFind and replace text using regex patterns.\n\n[required inputs]\nstring: STRING\nregex_pattern: STRING\nreplace: STRING\n\n[optional inputs]\ncase_insensitive: BOOLEAN | default=True\nmultiline: BOOLEAN | default=False\ndotall: BOOLEAN | When enabled, the dot (.) character will match any character including newline characters. When disabled, dots won't match newlines. | default=False\ncount: INT | Maximum number of replacements to make. Set to 0 to replace all occurrences (default). Set to 1 to replace only the first match, 2 for the first two matches, etc. | default=0 | min=0 | max=100\n\n[outputs]\nSTRING: STRING",
  "RenormCFG": "[required inputs]\nmodel: MODEL\ncfg_trunc: FLOAT | default=100 | min=0.0 | max=100.0 | step=0.01\nrenorm_cfg: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "RepeatImageBatch": "[required inputs]\nimage: IMAGE\namount: INT | default=1 | min=1 | max=4096\n\n[outputs]\nIMAGE: IMAGE",
  "RepeatLatentBatch": "Repeat Latent Batch\n\n[required inputs]\nsamples: LATENT\namount: INT | default=1 | min=1 | max=64\n\n[outputs]\nLATENT: LATENT",
  "ReplaceText": "Replace Text\n\n[required inputs]\ntexts: STRING | Text to process.\nfind: STRING | Text to find. | default=\nreplace: STRING | Text to replace with. | default=\n\n[outputs]\ntexts: STRING | Processed texts",
  "Reroute": "",
  "RescaleCFG": "[required inputs]\nmodel: MODEL\nmultiplier: FLOAT | default=0.7 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "RescaleCfg": "",
  "ResizeAndPadImage": "[required inputs]\nimage: IMAGE\ntarget_width: INT | default=512 | min=1 | max=16384 | step=1\ntarget_height: INT | default=512 | min=1 | max=16384 | step=1\npadding_color: COMBO | options=white, black\ninterpolation: COMBO | options=area, bicubic, nearest-exact, bilinear, lanczos\n\n[outputs]\nIMAGE: IMAGE",
  "ResizeImagesByLongerEdge": "Resize Images by Longer Edge\n\n[required inputs]\nimages: IMAGE | Image to process.\nlonger_edge: INT | Target length for the longer edge. | default=1024 | min=1 | max=8192\n\n[outputs]\nimages: IMAGE | Processed images",
  "ResizeImagesByShorterEdge": "Resize Images by Shorter Edge\n\n[required inputs]\nimages: IMAGE | Image to process.\nshorter_edge: INT | Target length for the shorter edge. | default=512 | min=1 | max=8192\n\n[outputs]\nimages: IMAGE | Processed images",
  "Rodin3D_Detail": "Rodin 3D Generate - Detail Generate\nGenerate 3D Assets using Rodin API\n\n[required inputs]\nImages: IMAGE\n\n[optional inputs]\nSeed: INT | default=0 | min=0 | max=65535\nMaterial_Type: COMBO | default=PBR | options=PBR, Shaded\nPolygon_count: COMBO | default=18K-Quad | options=4K-Quad, 8K-Quad, 18K-Quad, 50K-Quad, 200K-Triangle\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\n3D Model Path: STRING",
  "Rodin3D_Gen2": "Rodin 3D Generate - Gen-2 Generate\nGenerate 3D Assets using Rodin API\n\n[required inputs]\nImages: IMAGE\nTAPose: BOOLEAN | default=False\n\n[optional inputs]\nSeed: INT | default=0 | min=0 | max=65535\nMaterial_Type: COMBO | default=PBR | options=PBR, Shaded\nPolygon_count: COMBO | default=500K-Triangle | options=4K-Quad, 8K-Quad, 18K-Quad, 50K-Quad, 2K-Triangle, 20K-Triangle, 150K-Triangle, 500K-Triangle\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\n3D Model Path: STRING",
  "Rodin3D_Regular": "Rodin 3D Generate - Regular Generate\nGenerate 3D Assets using Rodin API\n\n[required inputs]\nImages: IMAGE\n\n[optional inputs]\nSeed: INT | default=0 | min=0 | max=65535\nMaterial_Type: COMBO | default=PBR | options=PBR, Shaded\nPolygon_count: COMBO | default=18K-Quad | options=4K-Quad, 8K-Quad, 18K-Quad, 50K-Quad, 200K-Triangle\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\n3D Model Path: STRING",
  "Rodin3D_Sketch": "Rodin 3D Generate - Sketch Generate\nGenerate 3D Assets using Rodin API\n\n[required inputs]\nImages: IMAGE\n\n[optional inputs]\nSeed: INT | default=0 | min=0 | max=65535\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\n3D Model Path: STRING",
  "Rodin3D_Smooth": "Rodin 3D Generate - Smooth Generate\nGenerate 3D Assets using Rodin API\n\n[required inputs]\nImages: IMAGE\n\n[optional inputs]\nSeed: INT | default=0 | min=0 | max=65535\nMaterial_Type: COMBO | default=PBR | options=PBR, Shaded\nPolygon_count: COMBO | default=18K-Quad | options=4K-Quad, 8K-Quad, 18K-Quad, 50K-Quad, 200K-Triangle\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\n3D Model Path: STRING",
  "RunwayFirstLastFrameNode": "Runway First-Last-Frame to Video\nUpload first and last keyframes, draft a prompt, and generate a video. More complex transitions, such as cases where the Last frame is completely different from the First frame, may benefit from the longer 10s duration. This would give the generation more time to smoothly transition between the two inputs. Before diving in, review these best practices to ensure that your input selections will set your generation up for success: https://help.runwayml.com/hc/en-us/articles/34170748696595-Creating-with-Keyframes-on-Gen-3.\n\n[required inputs]\nprompt: STRING | Text prompt for the generation | default=\nstart_frame: IMAGE | Start frame to be used for the video\nend_frame: IMAGE | End frame to be used for the video. Supported for gen3a_turbo only.\nduration: COMBO | options=5, 10\nratio: COMBO | options=768:1280, 1280:768\nseed: INT | Random seed for generation | default=0 | min=0 | max=4294967295 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "RunwayImageToVideoNodeGen3a": "Runway Image to Video (Gen3a Turbo)\nGenerate a video from a single starting frame using Gen3a Turbo model. Before diving in, review these best practices to ensure that your input selections will set your generation up for success: https://help.runwayml.com/hc/en-us/articles/33927968552339-Creating-with-Act-One-on-Gen-3-Alpha-and-Turbo.\n\n[required inputs]\nprompt: STRING | Text prompt for the generation | default=\nstart_frame: IMAGE | Start frame to be used for the video\nduration: COMBO | options=5, 10\nratio: COMBO | options=768:1280, 1280:768\nseed: INT | Random seed for generation | default=0 | min=0 | max=4294967295 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "RunwayImageToVideoNodeGen4": "Runway Image to Video (Gen4 Turbo)\nGenerate a video from a single starting frame using Gen4 Turbo model. Before diving in, review these best practices to ensure that your input selections will set your generation up for success: https://help.runwayml.com/hc/en-us/articles/37327109429011-Creating-with-Gen-4-Video.\n\n[required inputs]\nprompt: STRING | Text prompt for the generation | default=\nstart_frame: IMAGE | Start frame to be used for the video\nduration: COMBO | options=5, 10\nratio: COMBO | options=1280:720, 720:1280, 1104:832, 832:1104, 960:960, 1584:672\nseed: INT | Random seed for generation | default=0 | min=0 | max=4294967295 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "RunwayTextToImageNode": "Runway Text to Image\nGenerate an image from a text prompt using Runway's Gen 4 model. You can also include reference image to guide the generation.\n\n[required inputs]\nprompt: STRING | Text prompt for the generation | default=\nratio: COMBO | options=1920:1080, 1080:1920, 1024:1024, 1360:768, 1080:1080, 1168:880, 1440:1080, 1080:1440, 1808:768, 2112:912\n\n[optional inputs]\nreference_image: IMAGE | Optional reference image to guide the generation\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "SDTurboScheduler": "[required inputs]\nmodel: MODEL\nsteps: INT | default=1 | min=1 | max=10\ndenoise: FLOAT | default=1.0 | min=0 | max=1.0 | step=0.01\n\n[outputs]\nSIGMAS: SIGMAS",
  "SD_4XUpscale_Conditioning": "[required inputs]\nimages: IMAGE\npositive: CONDITIONING\nnegative: CONDITIONING\nscale_ratio: FLOAT | default=4.0 | min=0.0 | max=10.0 | step=0.01\nnoise_augmentation: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "SV3D_Conditioning": "[required inputs]\nclip_vision: CLIP_VISION\ninit_image: IMAGE\nvae: VAE\nwidth: INT | default=576 | min=16 | max=16384 | step=8\nheight: INT | default=576 | min=16 | max=16384 | step=8\nvideo_frames: INT | default=21 | min=1 | max=4096\nelevation: FLOAT | default=0.0 | min=-90.0 | max=90.0 | step=0.1 | round=False\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "SVD_img2vid_Conditioning": "[required inputs]\nclip_vision: CLIP_VISION\ninit_image: IMAGE\nvae: VAE\nwidth: INT | default=1024 | min=16 | max=16384 | step=8\nheight: INT | default=576 | min=16 | max=16384 | step=8\nvideo_frames: INT | default=14 | min=1 | max=4096\nmotion_bucket_id: INT | default=127 | min=1 | max=1023\nfps: INT | default=6 | min=1 | max=1024\naugmentation_level: FLOAT | default=0.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "SamplerCustom": "[required inputs]\nmodel: MODEL\nadd_noise: BOOLEAN | default=True\nnoise_seed: INT | default=0 | min=0 | max=18446744073709551615\ncfg: FLOAT | default=8.0 | min=0.0 | max=100.0 | step=0.1 | round=0.01\npositive: CONDITIONING\nnegative: CONDITIONING\nsampler: SAMPLER\nsigmas: SIGMAS\nlatent_image: LATENT\n\n[outputs]\noutput: LATENT\ndenoised_output: LATENT",
  "SamplerCustomAdvanced": "[required inputs]\nnoise: NOISE\nguider: GUIDER\nsampler: SAMPLER\nsigmas: SIGMAS\nlatent_image: LATENT\n\n[outputs]\noutput: LATENT\ndenoised_output: LATENT",
  "SamplerDPMAdaptative": "[required inputs]\norder: INT | default=3 | min=2 | max=3\nrtol: FLOAT | default=0.05 | min=0.0 | max=100.0 | step=0.01 | round=False\natol: FLOAT | default=0.0078 | min=0.0 | max=100.0 | step=0.01 | round=False\nh_init: FLOAT | default=0.05 | min=0.0 | max=100.0 | step=0.01 | round=False\npcoeff: FLOAT | default=0.0 | min=0.0 | max=100.0 | step=0.01 | round=False\nicoeff: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ndcoeff: FLOAT | default=0.0 | min=0.0 | max=100.0 | step=0.01 | round=False\naccept_safety: FLOAT | default=0.81 | min=0.0 | max=100.0 | step=0.01 | round=False\neta: FLOAT | default=0.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerDPMPP_2M_SDE": "[required inputs]\nsolver_type: COMBO | options=midpoint, heun\neta: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\nnoise_device: COMBO | options=gpu, cpu\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerDPMPP_2S_Ancestral": "[required inputs]\neta: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerDPMPP_3M_SDE": "[required inputs]\neta: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\nnoise_device: COMBO | options=gpu, cpu\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerDPMPP_SDE": "[required inputs]\neta: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\nr: FLOAT | default=0.5 | min=0.0 | max=100.0 | step=0.01 | round=False\nnoise_device: COMBO | options=gpu, cpu\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerDpmpp2mSde": "",
  "SamplerDpmppSde": "",
  "SamplerER_SDE": "[required inputs]\nsolver_type: COMBO | options=ER-SDE, Reverse-time SDE, ODE\nmax_stage: INT | default=3 | min=1 | max=3\neta: FLOAT | Stochastic strength of reverse-time SDE.\nWhen eta=0, it reduces to deterministic ODE. This setting doesn't apply to ER-SDE solver type. | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerEulerAncestral": "[required inputs]\neta: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerEulerAncestralCFGPP": "SamplerEulerAncestralCFG++\n\n[required inputs]\neta: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01 | round=False\ns_noise: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01 | round=False\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerEulerCFGpp": "SamplerEulerCFG++\n\n[required inputs]\nversion: COMBO | options=regular, alternative\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerLCMUpscale": "[required inputs]\nscale_ratio: FLOAT | default=1.0 | min=0.1 | max=20.0 | step=0.01\nscale_steps: INT | default=-1 | min=-1 | max=1000 | step=1\nupscale_method: COMBO | options=bislerp, nearest-exact, bilinear, area, bicubic\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerLMS": "[required inputs]\norder: INT | default=4 | min=1 | max=100\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplerSASolver": "[required inputs]\nmodel: MODEL\neta: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01 | round=False\nsde_start_percent: FLOAT | default=0.2 | min=0.0 | max=1.0 | step=0.001\nsde_end_percent: FLOAT | default=0.8 | min=0.0 | max=1.0 | step=0.001\ns_noise: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.01 | round=False\npredictor_order: INT | default=3 | min=1 | max=6\ncorrector_order: INT | default=4 | min=0 | max=6\nuse_pece: BOOLEAN\nsimple_order_2: BOOLEAN\n\n[outputs]\nSAMPLER: SAMPLER",
  "SamplingPercentToSigma": "[required inputs]\nmodel: MODEL\nsampling_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.0001\nreturn_actual_sigma: BOOLEAN | Return the actual sigma value instead of the value used for interval checks.\nThis only affects results at 0.0 and 1.0. | default=False\n\n[outputs]\nsigma_value: FLOAT",
  "SaveAnimatedPNG": "[required inputs]\nimages: IMAGE\nfilename_prefix: STRING | default=ComfyUI\nfps: FLOAT | default=6.0 | min=0.01 | max=1000.0 | step=0.01\ncompress_level: INT | default=4 | min=0 | max=9\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveAnimatedPng": "",
  "SaveAnimatedWEBP": "[required inputs]\nimages: IMAGE\nfilename_prefix: STRING | default=ComfyUI\nfps: FLOAT | default=6.0 | min=0.01 | max=1000.0 | step=0.01\nlossless: BOOLEAN | default=True\nquality: INT | default=80 | min=0 | max=100\nmethod: COMBO | options=default, fastest, slowest\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveAnimatedWebp": "",
  "SaveAudio": "Save Audio (FLAC)\n\n[required inputs]\naudio: AUDIO\nfilename_prefix: STRING | default=audio/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveAudioMP3": "Save Audio (MP3)\n\n[required inputs]\naudio: AUDIO\nfilename_prefix: STRING | default=audio/ComfyUI\nquality: COMBO | default=V0 | options=V0, 128k, 320k\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveAudioOpus": "Save Audio (Opus)\n\n[required inputs]\naudio: AUDIO\nfilename_prefix: STRING | default=audio/ComfyUI\nquality: COMBO | default=128k | options=64k, 96k, 128k, 192k, 320k\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveGLB": "[required inputs]\nmesh: MESH\nfilename_prefix: STRING | default=mesh/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveImage": "Save Image\nSaves the input images to your ComfyUI output directory.\n\n[required inputs]\nimages: IMAGE | The images to save.\nfilename_prefix: STRING | The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes. | default=ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveImageDataSetToFolder": "Save Image Dataset to Folder\n\n[required inputs]\nimages: IMAGE | List of images to save.\nfolder_name: STRING | Name of the folder to save images to (inside output directory). | default=dataset\nfilename_prefix: STRING | Prefix for saved image filenames. | default=image\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveImageTextDataSetToFolder": "Save Image and Text Dataset to Folder\n\n[required inputs]\nimages: IMAGE | List of images to save.\ntexts: STRING | List of text captions to save.\nfolder_name: STRING | Name of the folder to save images to (inside output directory). | default=dataset\nfilename_prefix: STRING | Prefix for saved image filenames. | default=image\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveImageWebsocket": "[required inputs]\nimages: IMAGE",
  "SaveLatent": "[required inputs]\nsamples: LATENT\nfilename_prefix: STRING | default=latents/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveLoRA": "Save LoRA Weights\n\n[required inputs]\nlora: LORA_MODEL | The LoRA model to save. Do not use the model with LoRA layers.\nprefix: STRING | The prefix to use for the saved LoRA file. | default=loras/ComfyUI_trained_lora\n\n[optional inputs]\nsteps: INT | Optional: The number of steps to LoRA has been trained for, used to name the saved file.\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveSVGNode": "Save SVG files on disk.\n\n[required inputs]\nsvg: SVG\nfilename_prefix: STRING | The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes. | default=svg/ComfyUI\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveTrainingDataset": "Save Training Dataset\n\n[required inputs]\nlatents: LATENT | List of latent dicts from MakeTrainingDataset.\nconditioning: CONDITIONING | List of conditioning lists from MakeTrainingDataset.\nfolder_name: STRING | Name of folder to save dataset (inside output directory). | default=training_dataset\nshard_size: INT | Number of samples per shard file. | default=1000 | min=1 | max=100000\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveVideo": "Save Video\nSaves the input images to your ComfyUI output directory.\n\n[required inputs]\nvideo: VIDEO | The video to save.\nfilename_prefix: STRING | The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes. | default=video/ComfyUI\nformat: COMBO | The format to save the video as. | default=auto | options=auto, mp4\ncodec: COMBO | The codec to use for the video. | default=auto | options=auto, h264\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "SaveWEBM": "[required inputs]\nimages: IMAGE\nfilename_prefix: STRING | default=ComfyUI\ncodec: COMBO | options=vp9, av1\nfps: FLOAT | default=24.0 | min=0.01 | max=1000.0 | step=0.01\ncrf: FLOAT | Higher crf means lower quality with a smaller file size, lower crf means higher quality higher filesize. | default=32.0 | min=0 | max=63.0 | step=1\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "ScaleROPE": "Scale and shift the ROPE of the model.\n\n[required inputs]\nmodel: MODEL\nscale_x: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.1\nshift_x: FLOAT | default=0.0 | min=-256.0 | max=256.0 | step=0.1\nscale_y: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.1\nshift_y: FLOAT | default=0.0 | min=-256.0 | max=256.0 | step=0.1\nscale_t: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.1\nshift_t: FLOAT | default=0.0 | min=-256.0 | max=256.0 | step=0.1\n\n[outputs]\nMODEL: MODEL",
  "Sd4xupscaleConditioning": "",
  "SdTurboScheduler": "",
  "SelfAttentionGuidance": "Self-Attention Guidance\n\n[required inputs]\nmodel: MODEL\nscale: FLOAT | default=0.5 | min=-2.0 | max=5.0 | step=0.01\nblur_sigma: FLOAT | default=2.0 | min=0.0 | max=10.0 | step=0.1\n\n[outputs]\nMODEL: MODEL",
  "SetClipHooks": "Set CLIP Hooks\n\n[required inputs]\nclip: CLIP\napply_to_conds: BOOLEAN | default=True\nschedule_clip: BOOLEAN | default=False\n\n[optional inputs]\nhooks: HOOKS\n\n[outputs]\nCLIP: CLIP",
  "SetFirstSigma": "[required inputs]\nsigmas: SIGMAS\nsigma: FLOAT | default=136.0 | min=0.0 | max=20000.0 | step=0.001 | round=False\n\n[outputs]\nSIGMAS: SIGMAS",
  "SetHookKeyframes": "Set Hook Keyframes\n\n[required inputs]\nhooks: HOOKS\n\n[optional inputs]\nhook_kf: HOOK_KEYFRAMES\n\n[outputs]\nHOOKS: HOOKS",
  "SetLatentNoiseMask": "Set Latent Noise Mask\n\n[required inputs]\nsamples: LATENT\nmask: MASK\n\n[outputs]\nLATENT: LATENT",
  "SetUnionControlNetType": "[required inputs]\ncontrol_net: CONTROL_NET\ntype: COMBO | options=auto, openpose, depth, hed/pidi/scribble/ted, canny/lineart/anime_lineart/mlsd, normal, segment, tile, repaint\n\n[outputs]\nCONTROL_NET: CONTROL_NET",
  "ShuffleDataset": "Shuffle Image Dataset\n\n[required inputs]\nimages: IMAGE | List of images to process.\nseed: INT | Random seed. | default=0 | min=0 | max=18446744073709551615\n\n[outputs]\nimages: IMAGE | Processed images",
  "ShuffleImageTextDataset": "Shuffle Image-Text Dataset\n\n[required inputs]\nimages: IMAGE | List of images to shuffle.\ntexts: STRING | List of texts to shuffle.\nseed: INT | Random seed. | default=0 | min=0 | max=18446744073709551615\n\n[outputs]\nimages: IMAGE | Shuffled images\ntexts: STRING | Shuffled texts",
  "SkipLayerGuidanceDiT": "Generic version of SkipLayerGuidance node that can be used on every DiT model.\n\n[required inputs]\nmodel: MODEL\ndouble_layers: STRING | default=7, 8, 9\nsingle_layers: STRING | default=7, 8, 9\nscale: FLOAT | default=3.0 | min=0.0 | max=10.0 | step=0.1\nstart_percent: FLOAT | default=0.01 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=0.15 | min=0.0 | max=1.0 | step=0.001\nrescaling_scale: FLOAT | default=0.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "SkipLayerGuidanceDiTSimple": "Simple version of the SkipLayerGuidanceDiT node that only modifies the uncond pass.\n\n[required inputs]\nmodel: MODEL\ndouble_layers: STRING | default=7, 8, 9\nsingle_layers: STRING | default=7, 8, 9\nstart_percent: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.001\n\n[outputs]\nMODEL: MODEL",
  "SkipLayerGuidanceSD3": "Generic version of SkipLayerGuidance node that can be used on every DiT model.\n\n[required inputs]\nmodel: MODEL\nlayers: STRING | default=7, 8, 9\nscale: FLOAT | default=3.0 | min=0.0 | max=10.0 | step=0.1\nstart_percent: FLOAT | default=0.01 | min=0.0 | max=1.0 | step=0.001\nend_percent: FLOAT | default=0.15 | min=0.0 | max=1.0 | step=0.001\n\n[outputs]\nMODEL: MODEL",
  "SolidMask": "[required inputs]\nvalue: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\nwidth: INT | default=512 | min=1 | max=16384 | step=1\nheight: INT | default=512 | min=1 | max=16384 | step=1\n\n[outputs]\nMASK: MASK",
  "SplitAudioChannels": "Split Audio Channels\nSeparates the audio into left and right channels.\n\n[required inputs]\naudio: AUDIO\n\n[outputs]\nleft: AUDIO\nright: AUDIO",
  "SplitImageWithAlpha": "Split Image with Alpha\n\n[required inputs]\nimage: IMAGE\n\n[outputs]\nIMAGE: IMAGE\nMASK: MASK",
  "SplitSigmas": "[required inputs]\nsigmas: SIGMAS\nstep: INT | default=0 | min=0 | max=10000\n\n[outputs]\nhigh_sigmas: SIGMAS\nlow_sigmas: SIGMAS",
  "SplitSigmasDenoise": "[required inputs]\nsigmas: SIGMAS\ndenoise: FLOAT | default=1.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nhigh_sigmas: SIGMAS\nlow_sigmas: SIGMAS",
  "StabilityAudioInpaint": "Stability AI Audio Inpaint\nTransforms part of existing audio sample using text instructions.\n\n[required inputs]\nmodel: COMBO | options=stable-audio-2.5\nprompt: STRING | default=\naudio: AUDIO | Audio must be between 6 and 190 seconds long.\n\n[optional inputs]\nduration: INT | Controls the duration in seconds of the generated audio. | default=190 | min=1 | max=190 | step=1\nseed: INT | The random seed used for generation. | default=0 | min=0 | max=4294967294 | step=1\nsteps: INT | Controls the number of sampling steps. | default=8 | min=4 | max=8 | step=1\nmask_start: INT | default=30 | min=0 | max=190 | step=1\nmask_end: INT | default=190 | min=0 | max=190 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nAUDIO: AUDIO",
  "StabilityAudioToAudio": "Stability AI Audio To Audio\nTransforms existing audio samples into new high-quality compositions using text instructions.\n\n[required inputs]\nmodel: COMBO | options=stable-audio-2.5\nprompt: STRING | default=\naudio: AUDIO | Audio must be between 6 and 190 seconds long.\n\n[optional inputs]\nduration: INT | Controls the duration in seconds of the generated audio. | default=190 | min=1 | max=190 | step=1\nseed: INT | The random seed used for generation. | default=0 | min=0 | max=4294967294 | step=1\nsteps: INT | Controls the number of sampling steps. | default=8 | min=4 | max=8 | step=1\nstrength: FLOAT | Parameter controls how much influence the audio parameter has on the generated audio. | default=1 | min=0.01 | max=1.0 | step=0.01\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nAUDIO: AUDIO",
  "StabilityStableImageSD_3_5Node": "Stability AI Stable Diffusion 3.5 Image\nGenerates images synchronously based on prompt and resolution.\n\n[required inputs]\nprompt: STRING | What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results. | default=\nmodel: COMBO | options=sd3.5-large, sd3.5-medium\naspect_ratio: COMBO | Aspect ratio of generated image. | default=1:1 | options=1:1, 16:9, 9:16, 3:2, 2:3, 5:4, 4:5, 21:9, 9:21\nstyle_preset: COMBO | Optional desired style of generated image. | options=None, 3d-model, analog-film, anime, cinematic, comic-book, digital-art, enhance, fantasy-art, isometric, line-art, low-poly, modeling-compound, neon-punk, origami, photographic, pixel-art, tile-texture\ncfg_scale: FLOAT | How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt) | default=4.0 | min=1.0 | max=10.0 | step=0.1\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=4294967294 | step=1\n\n[optional inputs]\nimage: IMAGE\nnegative_prompt: STRING | Keywords of what you do not wish to see in the output image. This is an advanced feature. | default=\nimage_denoise: FLOAT | Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all. | default=0.5 | min=0.0 | max=1.0 | step=0.01\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "StabilityStableImageUltraNode": "Stability AI Stable Image Ultra\nGenerates images synchronously based on prompt and resolution.\n\n[required inputs]\nprompt: STRING | What you wish to see in the output image. A strong, descriptive prompt that clearly defineselements, colors, and subjects will lead to better results. To control the weight of a given word use the format `(word:weight)`,where `word` is the word you'd like to control the weight of and `weight`is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`would convey a sky that was blue and green, but more green than blue. | default=\naspect_ratio: COMBO | Aspect ratio of generated image. | default=1:1 | options=1:1, 16:9, 9:16, 3:2, 2:3, 5:4, 4:5, 21:9, 9:21\nstyle_preset: COMBO | Optional desired style of generated image. | options=None, 3d-model, analog-film, anime, cinematic, comic-book, digital-art, enhance, fantasy-art, isometric, line-art, low-poly, modeling-compound, neon-punk, origami, photographic, pixel-art, tile-texture\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=4294967294 | step=1\n\n[optional inputs]\nimage: IMAGE\nnegative_prompt: STRING | A blurb of text describing what you do not wish to see in the output image. This is an advanced feature. | default=\nimage_denoise: FLOAT | Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all. | default=0.5 | min=0.0 | max=1.0 | step=0.01\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "StabilityTextToAudio": "Stability AI Text To Audio\nGenerates high-quality music and sound effects from text descriptions.\n\n[required inputs]\nmodel: COMBO | options=stable-audio-2.5\nprompt: STRING | default=\n\n[optional inputs]\nduration: INT | Controls the duration in seconds of the generated audio. | default=190 | min=1 | max=190 | step=1\nseed: INT | The random seed used for generation. | default=0 | min=0 | max=4294967294 | step=1\nsteps: INT | Controls the number of sampling steps. | default=8 | min=4 | max=8 | step=1\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nAUDIO: AUDIO",
  "StabilityUpscaleConservativeNode": "Stability AI Upscale Conservative\nUpscale image with minimal alterations to 4K resolution.\n\n[required inputs]\nimage: IMAGE\nprompt: STRING | What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results. | default=\ncreativity: FLOAT | Controls the likelihood of creating additional details not heavily conditioned by the init image. | default=0.35 | min=0.2 | max=0.5 | step=0.01\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=4294967294 | step=1\n\n[optional inputs]\nnegative_prompt: STRING | Keywords of what you do not wish to see in the output image. This is an advanced feature. | default=\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "StabilityUpscaleCreativeNode": "Stability AI Upscale Creative\nUpscale image with minimal alterations to 4K resolution.\n\n[required inputs]\nimage: IMAGE\nprompt: STRING | What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results. | default=\ncreativity: FLOAT | Controls the likelihood of creating additional details not heavily conditioned by the init image. | default=0.3 | min=0.1 | max=0.5 | step=0.01\nstyle_preset: COMBO | Optional desired style of generated image. | options=None, 3d-model, analog-film, anime, cinematic, comic-book, digital-art, enhance, fantasy-art, isometric, line-art, low-poly, modeling-compound, neon-punk, origami, photographic, pixel-art, tile-texture\nseed: INT | The random seed used for creating the noise. | default=0 | min=0 | max=4294967294 | step=1\n\n[optional inputs]\nnegative_prompt: STRING | Keywords of what you do not wish to see in the output image. This is an advanced feature. | default=\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "StabilityUpscaleFastNode": "Stability AI Upscale Fast\nQuickly upscales an image via Stability API call to 4x its original size; intended for upscaling low-quality/compressed images.\n\n[required inputs]\nimage: IMAGE\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "StableCascade_EmptyLatentImage": "[required inputs]\nwidth: INT | default=1024 | min=256 | max=16384 | step=8\nheight: INT | default=1024 | min=256 | max=16384 | step=8\ncompression: INT | default=42 | min=4 | max=128 | step=1\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[outputs]\nstage_c: LATENT\nstage_b: LATENT",
  "StableCascade_StageB_Conditioning": "[required inputs]\nconditioning: CONDITIONING\nstage_c: LATENT\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "StableCascade_StageC_VAEEncode": "[required inputs]\nimage: IMAGE\nvae: VAE\ncompression: INT | default=42 | min=4 | max=128 | step=1\n\n[outputs]\nstage_c: LATENT\nstage_b: LATENT",
  "StableCascade_SuperResolutionControlnet": "[required inputs]\nimage: IMAGE\nvae: VAE\n\n[outputs]\ncontrolnet_input: IMAGE\nstage_c: LATENT\nstage_b: LATENT",
  "StableZero123_Conditioning": "[required inputs]\nclip_vision: CLIP_VISION\ninit_image: IMAGE\nvae: VAE\nwidth: INT | default=256 | min=16 | max=16384 | step=8\nheight: INT | default=256 | min=16 | max=16384 | step=8\nbatch_size: INT | default=1 | min=1 | max=4096\nelevation: FLOAT | default=0.0 | min=-180.0 | max=180.0 | step=0.1 | round=False\nazimuth: FLOAT | default=0.0 | min=-180.0 | max=180.0 | step=0.1 | round=False\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "StableZero123_Conditioning_Batched": "[required inputs]\nclip_vision: CLIP_VISION\ninit_image: IMAGE\nvae: VAE\nwidth: INT | default=256 | min=16 | max=16384 | step=8\nheight: INT | default=256 | min=16 | max=16384 | step=8\nbatch_size: INT | default=1 | min=1 | max=4096\nelevation: FLOAT | default=0.0 | min=-180.0 | max=180.0 | step=0.1 | round=False\nazimuth: FLOAT | default=0.0 | min=-180.0 | max=180.0 | step=0.1 | round=False\nelevation_batch_increment: FLOAT | default=0.0 | min=-180.0 | max=180.0 | step=0.1 | round=False\nazimuth_batch_increment: FLOAT | default=0.0 | min=-180.0 | max=180.0 | step=0.1 | round=False\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "Stablezero123Conditioning": "",
  "Stablezero123ConditioningBatched": "",
  "StringCompare": "Compare\n\n[required inputs]\nstring_a: STRING\nstring_b: STRING\nmode: COMBO | options=Starts With, Ends With, Equal\ncase_sensitive: BOOLEAN | default=True\n\n[outputs]\nBOOLEAN: BOOLEAN",
  "StringConcatenate": "Concatenate\n\n[required inputs]\nstring_a: STRING\nstring_b: STRING\ndelimiter: STRING | default=\n\n[outputs]\nSTRING: STRING",
  "StringContains": "Contains\n\n[required inputs]\nstring: STRING\nsubstring: STRING\ncase_sensitive: BOOLEAN | default=True\n\n[outputs]\ncontains: BOOLEAN",
  "StringLength": "Length\n\n[required inputs]\nstring: STRING\n\n[outputs]\nlength: INT",
  "StringReplace": "Replace\n\n[required inputs]\nstring: STRING\nfind: STRING\nreplace: STRING\n\n[outputs]\nSTRING: STRING",
  "StringSubstring": "Substring\n\n[required inputs]\nstring: STRING\nstart: INT\nend: INT\n\n[outputs]\nSTRING: STRING",
  "StringTrim": "Trim\n\n[required inputs]\nstring: STRING\nmode: COMBO | options=Both, Left, Right\n\n[outputs]\nSTRING: STRING",
  "StripWhitespace": "Strip Whitespace\n\n[required inputs]\ntexts: STRING | Text to process.\n\n[outputs]\ntexts: STRING | Processed texts",
  "StyleModelApply": "Apply Style Model\n\n[required inputs]\nconditioning: CONDITIONING\nstyle_model: STYLE_MODEL\nclip_vision_output: CLIP_VISION_OUTPUT\nstrength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.001\nstrength_type: COMBO | options=multiply, attn_bias\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "StyleModelLoader": "Load Style Model\n\n[required inputs]\nstyle_model_name: COMBO\n\n[outputs]\nSTYLE_MODEL: STYLE_MODEL",
  "SvdImg2vidConditioning": "",
  "T5TokenizerOptions": "[required inputs]\nclip: CLIP\nmin_padding: INT | default=0 | min=0 | max=10000 | step=1\nmin_length: INT | default=0 | min=0 | max=10000 | step=1\n\n[outputs]\nCLIP: CLIP",
  "TCFG": "Tangential Damping CFG\nTCFG \u2013 Tangential Damping CFG (2503.18137)\n\nRefine the uncond (negative) to align with the cond (positive) for improving quality.\n\n[required inputs]\nmodel: MODEL\n\n[outputs]\npatched_model: MODEL",
  "TemporalScoreRescaling": "TSR - Temporal Score Rescaling\n[Post-CFG Function]\nTSR - Temporal Score Rescaling (2510.01184)\n\nRescaling the model's score or noise to steer the sampling diversity.\n\n[required inputs]\nmodel: MODEL\ntsr_k: FLOAT | Controls the rescaling strength.\nLower k produces more detailed results; higher k produces smoother results in image generation. Setting k = 1 disables rescaling. | default=0.95 | min=0.01 | max=100.0 | step=0.001\ntsr_sigma: FLOAT | Controls how early rescaling takes effect.\nLarger values take effect earlier. | default=1.0 | min=0.01 | max=100.0 | step=0.001\n\n[outputs]\npatched_model: MODEL",
  "TerminalLog": "",
  "TextEncodeAceStepAudio": "[required inputs]\nclip: CLIP\ntags: STRING\nlyrics: STRING\nlyrics_strength: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "TextEncodeHunyuanVideo_ImageToVideo": "[required inputs]\nclip: CLIP\nclip_vision_output: CLIP_VISION_OUTPUT\nprompt: STRING\nimage_interleave: INT | How much the image influences things vs the text prompt. Higher number means more influence from the text prompt. | default=2 | min=1 | max=512\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "TextEncodeQwenImageEdit": "[required inputs]\nclip: CLIP\nprompt: STRING\n\n[optional inputs]\nvae: VAE\nimage: IMAGE\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "TextEncodeQwenImageEditPlus": "[required inputs]\nclip: CLIP\nprompt: STRING\n\n[optional inputs]\nvae: VAE\nimage1: IMAGE\nimage2: IMAGE\nimage3: IMAGE\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "TextToLowercase": "Text to Lowercase\n\n[required inputs]\ntexts: STRING | Text to process.\n\n[outputs]\ntexts: STRING | Processed texts",
  "TextToUppercase": "Text to Uppercase\n\n[required inputs]\ntexts: STRING | Text to process.\n\n[outputs]\ntexts: STRING | Processed texts",
  "ThresholdMask": "[required inputs]\nmask: MASK\nvalue: FLOAT | default=0.5 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMASK: MASK",
  "TomePatchModel": "[required inputs]\nmodel: MODEL\nratio: FLOAT | default=0.3 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "TopazImageEnhance": "Topaz Image Enhance\nIndustry-standard upscaling and image enhancement.\n\n[required inputs]\nmodel: COMBO | options=Reimagine\nimage: IMAGE\n\n[optional inputs]\nprompt: STRING | Optional text prompt for creative upscaling guidance. | default=\nsubject_detection: COMBO | options=All, Foreground, Background\nface_enhancement: BOOLEAN | Enhance faces (if present) during processing. | default=True\nface_enhancement_creativity: FLOAT | Set the creativity level for face enhancement. | default=0.0 | min=0.0 | max=1.0 | step=0.01\nface_enhancement_strength: FLOAT | Controls how sharp enhanced faces are relative to the background. | default=1.0 | min=0.0 | max=1.0 | step=0.01\ncrop_to_fill: BOOLEAN | By default, the image is letterboxed when the output aspect ratio differs. Enable to crop the image to fill the output dimensions. | default=False\noutput_width: INT | Zero value means to calculate automatically (usually it will be original size or output_height if specified). | default=0 | min=0 | max=32000 | step=1\noutput_height: INT | Zero value means to output in the same height as original or output width. | default=0 | min=0 | max=32000 | step=1\ncreativity: INT | default=3 | min=1 | max=9 | step=1\nface_preservation: BOOLEAN | Preserve subjects' facial identity. | default=True\ncolor_preservation: BOOLEAN | Preserve the original colors. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "TopazVideoEnhance": "Topaz Video Enhance\nBreathe new life into video with powerful upscaling and recovery technology.\n\n[required inputs]\nvideo: VIDEO\nupscaler_enabled: BOOLEAN | default=True\nupscaler_model: COMBO | options=Starlight (Astra) Fast, Starlight (Astra) Creative\nupscaler_resolution: COMBO | options=FullHD (1080p), 4K (2160p)\n\n[optional inputs]\nupscaler_creativity: COMBO | Creativity level (applies only to Starlight (Astra) Creative). | default=low | options=low, middle, high\ninterpolation_enabled: BOOLEAN | default=False\ninterpolation_model: COMBO | default=apo-8 | options=apo-8\ninterpolation_slowmo: INT | Slow-motion factor applied to the input video. For example, 2 makes the output twice as slow and doubles the duration. | default=1 | min=1 | max=16\ninterpolation_frame_rate: INT | Output frame rate. | default=60 | min=15 | max=240\ninterpolation_duplicate: BOOLEAN | Analyze the input for duplicate frames and remove them. | default=False\ninterpolation_duplicate_threshold: FLOAT | Detection sensitivity for duplicate frames. | default=0.01 | min=0.001 | max=0.1 | step=0.001\ndynamic_compression_level: COMBO | CQP level. | default=Low | options=Low, Mid, High\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "TorchCompileModel": "[required inputs]\nmodel: MODEL\nbackend: COMBO | options=inductor, cudagraphs\n\n[outputs]\nMODEL: MODEL",
  "TrainLoraNode": "Train LoRA\n\n[required inputs]\nmodel: MODEL | The model to train the LoRA on.\nlatents: LATENT | The Latents to use for training, serve as dataset/input of the model.\npositive: CONDITIONING | The positive conditioning to use for training.\nbatch_size: INT | The batch size to use for training. | default=1 | min=1 | max=10000\ngrad_accumulation_steps: INT | The number of gradient accumulation steps to use for training. | default=1 | min=1 | max=1024\nsteps: INT | The number of steps to train the LoRA for. | default=16 | min=1 | max=100000\nlearning_rate: FLOAT | The learning rate to use for training. | default=0.0005 | min=1e-07 | max=1.0 | step=1e-07\nrank: INT | The rank of the LoRA layers. | default=8 | min=1 | max=128\noptimizer: COMBO | The optimizer to use for training. | default=AdamW | options=AdamW, Adam, SGD, RMSprop\nloss_function: COMBO | The loss function to use for training. | default=MSE | options=MSE, L1, Huber, SmoothL1\nseed: INT | The seed to use for training (used in generator for LoRA weight initialization and noise sampling) | default=0 | min=0 | max=18446744073709551615\ntraining_dtype: COMBO | The dtype to use for training. | default=bf16 | options=bf16, fp32\nlora_dtype: COMBO | The dtype to use for lora. | default=bf16 | options=bf16, fp32\nalgorithm: COMBO | The algorithm to use for training. | default=LoRA | options=LoRA, LoHa, LoKr, OFT\ngradient_checkpointing: BOOLEAN | Use gradient checkpointing for training. | default=True\nexisting_lora: COMBO | The existing LoRA to append to. Set to None for new LoRA. | default=[None] | options=[None]\n\n[outputs]\nmodel: MODEL | Model with LoRA applied\nlora: LORA_MODEL | LoRA weights\nloss_map: LOSS_MAP | Loss history\nsteps: INT | Total training steps",
  "TrimAudioDuration": "Trim Audio Duration\nTrim audio tensor into chosen time range.\n\n[required inputs]\naudio: AUDIO\nstart_index: FLOAT | Start time in seconds, can be negative to count from the end (supports sub-seconds). | default=0.0 | min=-18446744073709551615 | max=18446744073709551615 | step=0.01\nduration: FLOAT | Duration in seconds | default=60.0 | min=0.0 | step=0.01\n\n[outputs]\nAUDIO: AUDIO",
  "TrimVideoLatent": "[required inputs]\nsamples: LATENT\ntrim_amount: INT | default=0 | min=0 | max=99999\n\n[outputs]\nLATENT: LATENT",
  "TripleCLIPLoader": "[Recipes]\n\nsd3: clip-l, clip-g, t5\n\n[required inputs]\nclip_name1: COMBO\nclip_name2: COMBO\nclip_name3: COMBO\n\n[outputs]\nCLIP: CLIP",
  "TripoConversionNode": "Tripo: Convert model\n\n[required inputs]\noriginal_model_task_id: MODEL_TASK_ID,RIG_TASK_ID,RETARGET_TASK_ID\nformat: COMBO | options=GLTF, USDZ, FBX, OBJ, STL, 3MF\n\n[optional inputs]\nquad: BOOLEAN | default=False\nface_limit: INT | default=-1 | min=-1 | max=500000\ntexture_size: INT | default=4096 | min=128 | max=4096\ntexture_format: COMBO | default=JPEG | options=BMP, DPX, HDR, JPEG, OPEN_EXR, PNG, TARGA, TIFF, WEBP\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "TripoImageToModelNode": "Tripo: Image to Model\n\n[required inputs]\nimage: IMAGE\n\n[optional inputs]\nmodel_version: COMBO | The model version to use for generation | options=v2.5-20250123, v2.0-20240919, v1.4-20240625\nstyle: COMBO | default=None | options=person:person2cartoon, animal:venom, object:clay, object:steampunk, object:christmas, object:barbie, gold, ancient_bronze, None\ntexture: BOOLEAN | default=True\npbr: BOOLEAN | default=True\nmodel_seed: INT | default=42\norientation: COMBO | default=default | options=align_image, default\ntexture_seed: INT | default=42\ntexture_quality: COMBO | default=standard | options=standard, detailed\ntexture_alignment: COMBO | default=original_image | options=original_image, geometry\nface_limit: INT | default=-1 | min=-1 | max=500000\nquad: BOOLEAN | default=False\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO\n\n[outputs]\nmodel_file: STRING\nmodel task_id: MODEL_TASK_ID",
  "TripoMultiviewToModelNode": "Tripo: Multiview to Model\n\n[required inputs]\nimage: IMAGE\n\n[optional inputs]\nimage_left: IMAGE\nimage_back: IMAGE\nimage_right: IMAGE\nmodel_version: COMBO | The model version to use for generation | options=v2.5-20250123, v2.0-20240919, v1.4-20240625\norientation: COMBO | default=default | options=align_image, default\ntexture: BOOLEAN | default=True\npbr: BOOLEAN | default=True\nmodel_seed: INT | default=42\ntexture_seed: INT | default=42\ntexture_quality: COMBO | default=standard | options=standard, detailed\ntexture_alignment: COMBO | default=original_image | options=original_image, geometry\nface_limit: INT | default=-1 | min=-1 | max=500000\nquad: BOOLEAN | default=False\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO\n\n[outputs]\nmodel_file: STRING\nmodel task_id: MODEL_TASK_ID",
  "TripoRefineNode": "Tripo: Refine Draft model\nRefine a draft model created by v1.4 Tripo models only.\n\n[required inputs]\nmodel_task_id: MODEL_TASK_ID | Must be a v1.4 Tripo model\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO\n\n[outputs]\nmodel_file: STRING\nmodel task_id: MODEL_TASK_ID",
  "TripoRetargetNode": "Tripo: Retarget rigged model\n\n[required inputs]\noriginal_model_task_id: RIG_TASK_ID\nanimation: COMBO | options=preset:idle, preset:walk, preset:climb, preset:jump, preset:slash, preset:shoot, preset:hurt, preset:fall, preset:turn\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO\n\n[outputs]\nmodel_file: STRING\nretarget task_id: RETARGET_TASK_ID",
  "TripoRigNode": "Tripo: Rig model\n\n[required inputs]\noriginal_model_task_id: MODEL_TASK_ID\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO\n\n[outputs]\nmodel_file: STRING\nrig task_id: RIG_TASK_ID",
  "TripoTextToModelNode": "Tripo: Text to Model\n\n[required inputs]\nprompt: STRING\n\n[optional inputs]\nnegative_prompt: STRING\nmodel_version: COMBO | default=v2.5-20250123 | options=v2.5-20250123, v2.0-20240919, v1.4-20240625\nstyle: COMBO | default=None | options=person:person2cartoon, animal:venom, object:clay, object:steampunk, object:christmas, object:barbie, gold, ancient_bronze, None\ntexture: BOOLEAN | default=True\npbr: BOOLEAN | default=True\nimage_seed: INT | default=42\nmodel_seed: INT | default=42\ntexture_seed: INT | default=42\ntexture_quality: COMBO | default=standard | options=standard, detailed\nface_limit: INT | default=-1 | min=-1 | max=500000\nquad: BOOLEAN | default=False\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO\n\n[outputs]\nmodel_file: STRING\nmodel task_id: MODEL_TASK_ID",
  "TripoTextureNode": "Tripo: Texture model\n\n[required inputs]\nmodel_task_id: MODEL_TASK_ID\n\n[optional inputs]\ntexture: BOOLEAN | default=True\npbr: BOOLEAN | default=True\ntexture_seed: INT | default=42\ntexture_quality: COMBO | default=standard | options=standard, detailed\ntexture_alignment: COMBO | default=original_image | options=original_image, geometry\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO\n\n[outputs]\nmodel_file: STRING\nmodel task_id: MODEL_TASK_ID",
  "TruncateText": "Truncate Text\n\n[required inputs]\ntexts: STRING | Text to process.\nmax_length: INT | Maximum text length. | default=77 | min=1 | max=10000\n\n[outputs]\ntexts: STRING | Processed texts",
  "UNETLoader": "Load Diffusion Model\n\n[required inputs]\nunet_name: COMBO\nweight_dtype: COMBO | options=default, fp8_e4m3fn, fp8_e4m3fn_fast, fp8_e5m2\n\n[outputs]\nMODEL: MODEL",
  "UNetCrossAttentionMultiply": "[required inputs]\nmodel: MODEL\nq: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nk: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nv: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nout: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "UNetSelfAttentionMultiply": "[required inputs]\nmodel: MODEL\nq: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nk: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nv: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nout: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "UNetTemporalAttentionMultiply": "[required inputs]\nmodel: MODEL\nself_structural: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\nself_temporal: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\ncross_structural: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\ncross_temporal: FLOAT | default=1.0 | min=0.0 | max=10.0 | step=0.01\n\n[outputs]\nMODEL: MODEL",
  "USOStyleReference": "[required inputs]\nmodel: MODEL\nmodel_patch: MODEL_PATCH\nclip_vision_output: CLIP_VISION_OUTPUT\n\n[outputs]\nMODEL: MODEL",
  "UnclipCheckpointLoader": "",
  "UnclipConditioning": "",
  "UnetLoader": "",
  "UpscaleModelLoader": "Load Upscale Model\n\n[required inputs]\nmodel_name: COMBO\n\n[outputs]\nUPSCALE_MODEL: UPSCALE_MODEL",
  "VAEDecode": "VAE Decode\nDecodes latent images back into pixel space images.\n\n[required inputs]\nsamples: LATENT | The latent to be decoded.\nvae: VAE | The VAE model used for decoding the latent.\n\n[outputs]\nIMAGE: IMAGE | The decoded image.",
  "VAEDecodeAudio": "VAE Decode Audio\n\n[required inputs]\nsamples: LATENT\nvae: VAE\n\n[outputs]\nAUDIO: AUDIO",
  "VAEDecodeHunyuan3D": "[required inputs]\nsamples: LATENT\nvae: VAE\nnum_chunks: INT | default=8000 | min=1000 | max=500000\noctree_resolution: INT | default=256 | min=16 | max=512\n\n[outputs]\nVOXEL: VOXEL",
  "VAEDecodeTiled": "VAE Decode (Tiled)\n\n[required inputs]\nsamples: LATENT\nvae: VAE\ntile_size: INT | default=512 | min=64 | max=4096 | step=32\noverlap: INT | default=64 | min=0 | max=4096 | step=32\ntemporal_size: INT | Only used for video VAEs: Amount of frames to decode at a time. | default=64 | min=8 | max=4096 | step=4\ntemporal_overlap: INT | Only used for video VAEs: Amount of frames to overlap. | default=8 | min=4 | max=4096 | step=4\n\n[outputs]\nIMAGE: IMAGE",
  "VAEEncode": "VAE Encode\n\n[required inputs]\npixels: IMAGE\nvae: VAE\n\n[outputs]\nLATENT: LATENT",
  "VAEEncodeAudio": "VAE Encode Audio\n\n[required inputs]\naudio: AUDIO\nvae: VAE\n\n[outputs]\nLATENT: LATENT",
  "VAEEncodeForInpaint": "VAE Encode (for Inpainting)\n\n[required inputs]\npixels: IMAGE\nvae: VAE\nmask: MASK\ngrow_mask_by: INT | default=6 | min=0 | max=64 | step=1\n\n[outputs]\nLATENT: LATENT",
  "VAEEncodeTiled": "VAE Encode (Tiled)\n\n[required inputs]\npixels: IMAGE\nvae: VAE\ntile_size: INT | default=512 | min=64 | max=4096 | step=64\noverlap: INT | default=64 | min=0 | max=4096 | step=32\ntemporal_size: INT | Only used for video VAEs: Amount of frames to encode at a time. | default=64 | min=8 | max=4096 | step=4\ntemporal_overlap: INT | Only used for video VAEs: Amount of frames to overlap. | default=8 | min=4 | max=4096 | step=4\n\n[outputs]\nLATENT: LATENT",
  "VAELoader": "Load VAE\n\n[required inputs]\nvae_name: COMBO | options=pixel_space\n\n[outputs]\nVAE: VAE",
  "VAESave": "[required inputs]\nvae: VAE\nfilename_prefix: STRING | default=vae/ComfyUI_vae\n\n[hidden inputs]\nprompt: PROMPT\nextra_pnginfo: EXTRA_PNGINFO",
  "VPScheduler": "[required inputs]\nsteps: INT | default=20 | min=1 | max=10000\nbeta_d: FLOAT | default=19.9 | min=0.0 | max=5000.0 | step=0.01 | round=False\nbeta_min: FLOAT | default=0.1 | min=0.0 | max=5000.0 | step=0.01 | round=False\neps_s: FLOAT | default=0.001 | min=0.0 | max=1.0 | step=0.0001 | round=False\n\n[outputs]\nSIGMAS: SIGMAS",
  "VaeDecode": "",
  "VaeEncode": "",
  "VaeEncodeForInpaint": "",
  "VaeLoader": "",
  "VaeSave": "",
  "Veo3FirstLastFrameNode": "Google Veo 3 First-Last-Frame to Video\nGenerate video using prompt and first and last frames.\n\n[required inputs]\nprompt: STRING | Text description of the video | default=\nnegative_prompt: STRING | Negative text prompt to guide what to avoid in the video | default=\nresolution: COMBO | options=720p, 1080p\naspect_ratio: COMBO | Aspect ratio of the output video | default=16:9 | options=16:9, 9:16\nduration: INT | Duration of the output video in seconds | default=8 | min=4 | max=8 | step=2\nseed: INT | Seed for video generation | default=0 | min=0 | max=4294967295 | step=1\nfirst_frame: IMAGE | Start frame\nlast_frame: IMAGE | End frame\nmodel: COMBO | default=veo-3.1-fast-generate | options=veo-3.1-generate, veo-3.1-fast-generate\ngenerate_audio: BOOLEAN | Generate audio for the video. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "Veo3VideoGenerationNode": "Google Veo 3 Video Generation\nGenerates videos from text prompts using Google's Veo 3 API\n\n[required inputs]\nprompt: STRING | Text description of the video | default=\naspect_ratio: COMBO | Aspect ratio of the output video | default=16:9 | options=16:9, 9:16\n\n[optional inputs]\nnegative_prompt: STRING | Negative text prompt to guide what to avoid in the video | default=\nduration_seconds: INT | Duration of the output video in seconds (Veo 3 only supports 8 seconds) | default=8 | min=8 | max=8 | step=1\nenhance_prompt: BOOLEAN | Whether to enhance the prompt with AI assistance | default=True\nperson_generation: COMBO | Whether to allow generating people in the video | default=ALLOW | options=ALLOW, BLOCK\nseed: INT | Seed for video generation (0 for random) | default=0 | min=0 | max=4294967295 | step=1\nimage: IMAGE | Optional reference image to guide video generation\nmodel: COMBO | Veo 3 model to use for video generation | default=veo-3.0-generate-001 | options=veo-3.1-generate, veo-3.1-fast-generate, veo-3.0-generate-001, veo-3.0-fast-generate-001\ngenerate_audio: BOOLEAN | Generate audio for the video. Supported by all Veo 3 models. | default=False\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "VeoVideoGenerationNode": "Google Veo 2 Video Generation\nGenerates videos from text prompts using Google's Veo 2 API\n\n[required inputs]\nprompt: STRING | Text description of the video | default=\naspect_ratio: COMBO | Aspect ratio of the output video | default=16:9 | options=16:9, 9:16\n\n[optional inputs]\nnegative_prompt: STRING | Negative text prompt to guide what to avoid in the video | default=\nduration_seconds: INT | Duration of the output video in seconds | default=5 | min=5 | max=8 | step=1\nenhance_prompt: BOOLEAN | Whether to enhance the prompt with AI assistance | default=True\nperson_generation: COMBO | Whether to allow generating people in the video | default=ALLOW | options=ALLOW, BLOCK\nseed: INT | Seed for video generation (0 for random) | default=0 | min=0 | max=4294967295 | step=1\nimage: IMAGE | Optional reference image to guide video generation\nmodel: COMBO | Veo 2 model to use for video generation | default=veo-2.0-generate-001 | options=veo-2.0-generate-001\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "VideoLinearCFGGuidance": "[required inputs]\nmodel: MODEL\nmin_cfg: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.5 | round=0.01\n\n[outputs]\nMODEL: MODEL",
  "VideoLinearCfgGuidance": "",
  "VideoTriangleCFGGuidance": "[required inputs]\nmodel: MODEL\nmin_cfg: FLOAT | default=1.0 | min=0.0 | max=100.0 | step=0.5 | round=0.01\n\n[outputs]\nMODEL: MODEL",
  "ViduImageToVideoNode": "Vidu Image To Video Generation\nGenerate video from image and optional prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=viduq1 | options=viduq1\nimage: IMAGE | An image to be used as the start frame of the generated video\n\n[optional inputs]\nprompt: STRING | A textual description for video generation | default=\nduration: INT | Duration of the output video in seconds | default=5 | min=5 | max=5 | step=1\nseed: INT | Seed for video generation (0 for random) | default=0 | min=0 | max=2147483647 | step=1\nresolution: COMBO | Supported values may vary by model & duration | default=1080p | options=1080p\nmovement_amplitude: COMBO | The movement amplitude of objects in the frame | default=auto | options=auto, small, medium, large\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "ViduReferenceVideoNode": "Vidu Reference To Video Generation\nGenerate video from multiple images and prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=viduq1 | options=viduq1\nimages: IMAGE | Images to use as references to generate a video with consistent subjects (max 7 images).\nprompt: STRING | A textual description for video generation\n\n[optional inputs]\nduration: INT | Duration of the output video in seconds | default=5 | min=5 | max=5 | step=1\nseed: INT | Seed for video generation (0 for random) | default=0 | min=0 | max=2147483647 | step=1\naspect_ratio: COMBO | The aspect ratio of the output video | default=16:9 | options=16:9, 9:16, 1:1\nresolution: COMBO | Supported values may vary by model & duration | default=1080p | options=1080p\nmovement_amplitude: COMBO | The movement amplitude of objects in the frame | default=auto | options=auto, small, medium, large\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "ViduStartEndToVideoNode": "Vidu Start End To Video Generation\nGenerate a video from start and end frames and a prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=viduq1 | options=viduq1\nfirst_frame: IMAGE | Start frame\nend_frame: IMAGE | End frame\n\n[optional inputs]\nprompt: STRING | A textual description for video generation\nduration: INT | Duration of the output video in seconds | default=5 | min=5 | max=5 | step=1\nseed: INT | Seed for video generation (0 for random) | default=0 | min=0 | max=2147483647 | step=1\nresolution: COMBO | Supported values may vary by model & duration | default=1080p | options=1080p\nmovement_amplitude: COMBO | The movement amplitude of objects in the frame | default=auto | options=auto, small, medium, large\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "ViduTextToVideoNode": "Vidu Text To Video Generation\nGenerate video from text prompt\n\n[required inputs]\nmodel: COMBO | Model name | default=viduq1 | options=viduq1\nprompt: STRING | A textual description for video generation\n\n[optional inputs]\nduration: INT | Duration of the output video in seconds | default=5 | min=5 | max=5 | step=1\nseed: INT | Seed for video generation (0 for random) | default=0 | min=0 | max=2147483647 | step=1\naspect_ratio: COMBO | The aspect ratio of the output video | default=16:9 | options=16:9, 9:16, 1:1\nresolution: COMBO | Supported values may vary by model & duration | default=1080p | options=1080p\nmovement_amplitude: COMBO | The movement amplitude of objects in the frame | default=auto | options=auto, small, medium, large\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "VoxelToMesh": "[required inputs]\nvoxel: VOXEL\nalgorithm: COMBO | options=surface net, basic\nthreshold: FLOAT | default=0.6 | min=-1.0 | max=1.0 | step=0.01\n\n[outputs]\nMESH: MESH",
  "VoxelToMeshBasic": "[required inputs]\nvoxel: VOXEL\nthreshold: FLOAT | default=0.6 | min=-1.0 | max=1.0 | step=0.01\n\n[outputs]\nMESH: MESH",
  "VpScheduler": "",
  "Wan22FunControlToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nref_image: IMAGE\ncontrol_video: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "Wan22ImageToVideoLatent": "[required inputs]\nvae: VAE\nwidth: INT | default=1280 | min=32 | max=16384 | step=32\nheight: INT | default=704 | min=32 | max=16384 | step=32\nlength: INT | default=49 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nstart_image: IMAGE\n\n[outputs]\nLATENT: LATENT",
  "WanAnimateToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=77 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\ncontinue_motion_max_frames: INT | default=5 | min=1 | max=16384 | step=4\nvideo_frame_offset: INT | The amount of frames to seek in all the input videos. Used for generating longer videos by chunk. Connect to the video_frame_offset output of the previous node for extending a video. | default=0 | min=0 | max=16384 | step=1\n\n[optional inputs]\nclip_vision_output: CLIP_VISION_OUTPUT\nreference_image: IMAGE\nface_video: IMAGE\npose_video: IMAGE\nbackground_video: IMAGE\ncharacter_mask: MASK\ncontinue_motion: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT\ntrim_latent: INT\ntrim_image: INT\nvideo_frame_offset: INT",
  "WanCameraEmbedding": "[required inputs]\ncamera_pose: COMBO | default=Static | options=Static, Pan Up, Pan Down, Pan Left, Pan Right, Zoom In, Zoom Out, Anti Clockwise (ACW), ClockWise (CW)\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\n\n[optional inputs]\nspeed: FLOAT | default=1.0 | min=0 | max=10.0 | step=0.1\nfx: FLOAT | default=0.5 | min=0 | max=1 | step=1e-09\nfy: FLOAT | default=0.5 | min=0 | max=1 | step=1e-09\ncx: FLOAT | default=0.5 | min=0 | max=1 | step=0.01\ncy: FLOAT | default=0.5 | min=0 | max=1 | step=0.01\n\n[outputs]\ncamera_embedding: WAN_CAMERA_EMBEDDING\nwidth: INT\nheight: INT\nlength: INT",
  "WanCameraImageToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nclip_vision_output: CLIP_VISION_OUTPUT\nstart_image: IMAGE\ncamera_conditions: WAN_CAMERA_EMBEDDING\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanContextWindowsManual": "WAN Context Windows (Manual)\nManually set context windows for WAN-like models (dim=2).\n\n[required inputs]\nmodel: MODEL | The model to apply context windows to during sampling.\ncontext_length: INT | The length of the context window. | default=81 | min=1 | max=16384 | step=4\ncontext_overlap: INT | The overlap of the context window. | default=30 | min=0\ncontext_schedule: COMBO | The stride of the context window. | options=standard_static, standard_uniform, looped_uniform, batched\ncontext_stride: INT | The stride of the context window; only applicable to uniform schedules. | default=1 | min=1\nclosed_loop: BOOLEAN | Whether to close the context window loop; only applicable to looped schedules. | default=False\nfuse_method: COMBO | The method to use to fuse the context windows. | default=pyramid | options=pyramid, relative, flat, overlap-linear\n\n[outputs]\nMODEL: MODEL | The model with context windows applied during sampling.",
  "WanFirstLastFrameToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nclip_vision_start_image: CLIP_VISION_OUTPUT\nclip_vision_end_image: CLIP_VISION_OUTPUT\nstart_image: IMAGE\nend_image: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanFunControlToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nclip_vision_output: CLIP_VISION_OUTPUT\nstart_image: IMAGE\ncontrol_video: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanFunInpaintToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nclip_vision_output: CLIP_VISION_OUTPUT\nstart_image: IMAGE\nend_image: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanHuMoImageToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=97 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\naudio_encoder_output: AUDIO_ENCODER_OUTPUT\nref_image: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanImageToImageApi": "Wan Image to Image\nGenerates an image from one or two input images and a text prompt. The output image is currently fixed at 1.6 MP; its aspect ratio matches the input image(s).\n\n[required inputs]\nmodel: COMBO | Model to use. | default=wan2.5-i2i-preview | options=wan2.5-i2i-preview\nimage: IMAGE | Single-image editing or multi-image fusion, maximum 2 images.\nprompt: STRING | Prompt used to describe the elements and visual features, supports English/Chinese. | default=\n\n[optional inputs]\nnegative_prompt: STRING | Negative text prompt to guide what to avoid. | default=\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the result. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "WanImageToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nclip_vision_output: CLIP_VISION_OUTPUT\nstart_image: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanImageToVideoApi": "Wan Image to Video\nGenerates video based on the first frame and text prompt.\n\n[required inputs]\nmodel: COMBO | Model to use. | default=wan2.5-i2v-preview | options=wan2.5-i2v-preview\nimage: IMAGE\nprompt: STRING | Prompt used to describe the elements and visual features, supports English/Chinese. | default=\n\n[optional inputs]\nnegative_prompt: STRING | Negative text prompt to guide what to avoid. | default=\nresolution: COMBO | default=480P | options=480P, 720P, 1080P\nduration: INT | Available durations: 5 and 10 seconds | default=5 | min=5 | max=10 | step=5\naudio: AUDIO | Audio must contain a clear, loud voice, without extraneous noise, background music.\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\ngenerate_audio: BOOLEAN | If there is no audio input, generate audio automatically. | default=False\nprompt_extend: BOOLEAN | Whether to enhance the prompt with AI assistance. | default=True\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the result. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "WanPhantomSubjectToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\nimages: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative_text: CONDITIONING\nnegative_img_text: CONDITIONING\nlatent: LATENT",
  "WanSoundImageToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=77 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\n\n[optional inputs]\naudio_encoder_output: AUDIO_ENCODER_OUTPUT\nref_image: IMAGE\ncontrol_video: IMAGE\nref_motion: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanSoundImageToVideoExtend": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nlength: INT | default=77 | min=1 | max=16384 | step=4\nvideo_latent: LATENT\n\n[optional inputs]\naudio_encoder_output: AUDIO_ENCODER_OUTPUT\nref_image: IMAGE\ncontrol_video: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanTextToImageApi": "Wan Text to Image\nGenerates image based on text prompt.\n\n[required inputs]\nmodel: COMBO | Model to use. | default=wan2.5-t2i-preview | options=wan2.5-t2i-preview\nprompt: STRING | Prompt used to describe the elements and visual features, supports English/Chinese. | default=\n\n[optional inputs]\nnegative_prompt: STRING | Negative text prompt to guide what to avoid. | default=\nwidth: INT | default=1024 | min=768 | max=1440 | step=32\nheight: INT | default=1024 | min=768 | max=1440 | step=32\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\nprompt_extend: BOOLEAN | Whether to enhance the prompt with AI assistance. | default=True\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the result. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nIMAGE: IMAGE",
  "WanTextToVideoApi": "Wan Text to Video\nGenerates video based on text prompt.\n\n[required inputs]\nmodel: COMBO | Model to use. | default=wan2.5-t2v-preview | options=wan2.5-t2v-preview\nprompt: STRING | Prompt used to describe the elements and visual features, supports English/Chinese. | default=\n\n[optional inputs]\nnegative_prompt: STRING | Negative text prompt to guide what to avoid. | default=\nsize: COMBO | default=480p: 1:1 (624x624) | options=480p: 1:1 (624x624), 480p: 16:9 (832x480), 480p: 9:16 (480x832), 720p: 1:1 (960x960), 720p: 16:9 (1280x720), 720p: 9:16 (720x1280), 720p: 4:3 (1088x832), 720p: 3:4 (832x1088), 1080p: 1:1 (1440x1440), 1080p: 16:9 (1920x1080), 1080p: 9:16 (1080x1920), 1080p: 4:3 (1632x1248), 1080p: 3:4 (1248x1632)\nduration: INT | Available durations: 5 and 10 seconds | default=5 | min=5 | max=10 | step=5\naudio: AUDIO | Audio must contain a clear, loud voice, without extraneous noise, background music.\nseed: INT | Seed to use for generation. | default=0 | min=0 | max=2147483647 | step=1\ngenerate_audio: BOOLEAN | If there is no audio input, generate audio automatically. | default=False\nprompt_extend: BOOLEAN | Whether to enhance the prompt with AI assistance. | default=True\nwatermark: BOOLEAN | Whether to add an \"AI generated\" watermark to the result. | default=True\n\n[hidden inputs]\nauth_token_comfy_org: AUTH_TOKEN_COMFY_ORG\napi_key_comfy_org: API_KEY_COMFY_ORG\nunique_id: UNIQUE_ID\n\n[outputs]\nVIDEO: VIDEO",
  "WanTrackToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\ntracks: STRING | default=[]\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\ntemperature: FLOAT | default=220.0 | min=1.0 | max=1000.0 | step=0.1\ntopk: INT | default=2 | min=1 | max=10\nstart_image: IMAGE\n\n[optional inputs]\nclip_vision_output: CLIP_VISION_OUTPUT\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT",
  "WanVaceToVideo": "[required inputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nvae: VAE\nwidth: INT | default=832 | min=16 | max=16384 | step=16\nheight: INT | default=480 | min=16 | max=16384 | step=16\nlength: INT | default=81 | min=1 | max=16384 | step=4\nbatch_size: INT | default=1 | min=1 | max=4096\nstrength: FLOAT | default=1.0 | min=0.0 | max=1000.0 | step=0.01\n\n[optional inputs]\ncontrol_video: IMAGE\ncontrol_masks: MASK\nreference_image: IMAGE\n\n[outputs]\npositive: CONDITIONING\nnegative: CONDITIONING\nlatent: LATENT\ntrim_latent: INT",
  "WebcamCapture": "Webcam Capture\n\n[required inputs]\nimage: WEBCAM\nwidth: INT | default=0 | min=0 | max=16384 | step=1\nheight: INT | default=0 | min=0 | max=16384 | step=1\ncapture_on_queue: BOOLEAN | default=True\n\n[outputs]\nIMAGE: IMAGE",
  "unCLIPCheckpointLoader": "[required inputs]\nckpt_name: COMBO\n\n[outputs]\nMODEL: MODEL\nCLIP: CLIP\nVAE: VAE\nCLIP_VISION: CLIP_VISION",
  "unCLIPConditioning": "[required inputs]\nconditioning: CONDITIONING\nclip_vision_output: CLIP_VISION_OUTPUT\nstrength: FLOAT | default=1.0 | min=-10.0 | max=10.0 | step=0.01\nnoise_augmentation: FLOAT | default=0.0 | min=0.0 | max=1.0 | step=0.01\n\n[outputs]\nCONDITIONING: CONDITIONING",
  "wanBlockSwap": "NOP\n\n[required inputs]\nmodel: MODEL\n\n[outputs]\nMODEL: MODEL"
}